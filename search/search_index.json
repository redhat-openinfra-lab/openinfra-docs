{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"OCPBaremetalPI/","title":"OCP Baremetal IPI Installation","text":""},{"location":"OCPBaremetalPI/#introduction","title":"Introduction","text":"<p>Goals:</p> <ul> <li>Enable field on OpenShift Baremetal 4.12    </li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features    </li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs\u00a0    </li> <li>Send valuable feedback to Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OpenShift Baremetal</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"OCPBaremetalPI/#lab-access","title":"Lab Access","text":""},{"location":"OCPBaremetalPI/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.</p> <p>We have limited resources available, but there should be enough room for about 10 virtual environments.  </p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snap-shotting, adding more cinder volumes to OCS nodes or even adding more networks via either OpenStack CLI, Horizon, or Ansible Tower.</p> Role vRAM vCPU vNIC Disk Bootstrap 20G 6 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 100GB Master 16GB 4 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 25 GB Worker 24GB 12 <ul><li>1x pxe</li><li>1x baremetal</li> 50GB 100GB OSD Custom (optional)"},{"location":"OCPBaremetalPI/#building-your-kni-lab","title":"Building Your KNI Lab:","text":""},{"location":"OCPBaremetalPI/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.</p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy OpenShift Baremetal Environment</p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.</p> <p></p> </li> <li> <p>The jobs can be monitored under Jobs in the left pane.\u00a0 Additional jobs will be initiated to create the project, network, instances, and bare metal bootstrap.\u00a0 Wait the deployment to finish which can take ~10-15 minutes.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to Red Hat VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>networks:  \n  - { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"1500\" }  \n\u00a0\u00a0- { name: \"baremetal0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"1500\" }  \n\u00a0\u00a0- { name: \"virtualipmi\", cidr: \"10.30.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"1500\" }  \n</code></pre> </li> <li> <p>Set user/project and password using the same project and password used previously when creating the project.  Submit the job.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 controllers 2 computes and 3 ceph nodes.  </p> <p>With OCS (xlarge workers):  </p> <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n</code></pre> <p>Without ODF (normal workers): <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n</code></pre></p> </li> <li> <p>Set user/project and password and submit the job using the same project and password used previously when creating the project.  Submit the job. </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to - HextupleO - set up KNI</p> </li> <li> <p>Follow the survey and submit the job</p> </li> <li> <p>At the end you will be getting a screen similar to this one:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user kni using the password you set in the playbook.</p>"},{"location":"OCPBaremetalPI/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"OCPBaremetalPI/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0</p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.</p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address. You will use that IP to access your undercloud node. This will match the above output from the Ansible job (even though it does not in this document).\u00a0 You can SSH to this IP as kni using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0   </p> </li> <li> <p>Go to Routers, select the existing router (projectName_router); click the Interfaces tab and then click the Add Interface icon on the right. \u00a0 Add the baremetal0 interface in the Subnet dropdown.\u00a0 Click Submit.  </p> <p></p> </li> </ol>"},{"location":"OCPBaremetalPI/#bootstrap","title":"Bootstrap","text":"<ol> <li> <p>Access the bootstrap server via ssh using the IP address obtained in step 3 of the Accessing Your Project\u2019s OpenStack Environment section and the password specified in Tower when deploying the KNI environment.</p> </li> <li> <p>You can now start deploying Openshift Baremetal (KNI) based on the standard instructions below or feel free to deploy using any other documented process.  </p> <p>INFO: Repos Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the rhel8.repo file from here:</p> <p>[kni@bootstrap ~]$ sudo curl http://172.20.129.10/hextupleo-repo/rhel8.repo -o /etc/yum.repos.d/rhel8.repo </p> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-vanilla-openshift-baremetal","title":"Deploying Vanilla Openshift Baremetal","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, we highly encourage everyone to get at least one vanilla deployment done and get familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation found here.\u00a0 Please also review the official documentation for accuracy and open any Bugzilla\u2019s against it.</p>"},{"location":"OCPBaremetalPI/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<ol> <li> <p>Log into Bootstrap VM.  </p> <pre><code>ssh kni@&lt;bootstrapInstance&gt;  / password specified in Tower\n</code></pre> </li> <li> <p>Update all packages on the system.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf update -y\n...\nComplete!\n[kni@bootstrap ~]$ sudo reboot\n</code></pre> </li> <li> <p>Install the KNI Packages.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf install -y libvirt qemu-kvm mkisofs python3-devel jq ipmitool\n</code></pre> </li> <li> <p>Modify the user to add the libvirt group to the newly created kni user.  </p> <pre><code>[kni@bootstrap ~]$ sudo usermod --append --groups libvirt kni\n</code></pre> </li> <li> <p>Start and enable libvirtd; verify the daemon started successfully.</p> <pre><code>[kni@bootstrap ~]$ sudo systemctl enable \u2013now libvirtd\n[kni@bootstrap ~]$ systemctl status libvirtd\n\u25cf libvirtd.service - Virtualization daemon\n   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; vendor preset: enabled)\n   Active: active (running) since Thu 2023-02-16 10:07:21 EST; 1s ago\n     Docs: man:libvirtd(8)\n           https://libvirt.org\n Main PID: 7506 (libvirtd)\n    Tasks: 21 (limit: 32768)\n   Memory: 18.1M\n   CGroup: /system.slice/libvirtd.service\n       \u251c\u25007407 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u251c\u25007408 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u2514\u25007506 /usr/sbin/libvirtd --timeout 120\n</code></pre> </li> <li> <p>Create the default storage pool and start it.</p> <pre><code>[kni@bootstrap ~]$ sudo virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images\nPool default defined\n\n[kni@bootstrap ~]$ sudo virsh pool-start default \nPool default started\n\n[kni@bootstrap ~]$ sudo virsh pool-autostart default\nPool default marked as autostarted\n</code></pre> </li> <li> <p>Set up networking using the reconfig-net.sh script in the ~/GoodieBag directory.  Once the connections are reconfigured, the script will display the final configuration.  The output should look similar to what is displayed below.  The script will also add the baremetal subnet to the external router if it was not completed in the Accessing Your Project\u2019s OpenStack Environment section in the Horizon GUI.  </p> <pre><code>[kni@bootstrap ~]$ cd GoodieBag\n[kni@bootstrap GoodieBag]$ ./reconfig-net.sh -h\nScript to reconfigure the eth1 and eth2 interfaces on bootstrap server in OpenStack for Openshift deployments.\nUsage: reconfig-net.sh [-a|--all|interfaceName]\nIf -a or --all is passed, eth1 and eth2 will both be reconfigured as provisioning and baremetal bridge interfaces.\nIf -r or --router is passed, interface reconfiguration is skipped and router configuration will complete.\n\n[kni@bootstrap ~]$ sudo /tmp/reconfig-net.sh -a\nConnection 'System eth1' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)\nConnection 'System eth2' (9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04) successfully deleted.\nConnection 'provisioning' (1b5a7497-4ff5-43ec-bb3e-df01e8f070e8) successfully added.\nConnection 'bridge-slave-eth1' (3a46f2d4-5a3e-4b8d-994b-0a055ade7a84) successfully added.\nConnection 'provisioning' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5) \nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/7) \nprovisioning configured successfully.\n\nConnection 'System eth2' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)\nConnection 'System eth2' (3a73717e-65ab-93e8-b518-24f5af32dc0d) successfully deleted.\nConnection 'baremetal' (9ba0964b-3667-42f5-ae81-a0a0936985cf) successfully added.\nConnection 'bridge-slave-eth2' (f2b52c9b-087f-4b7c-9186-0ed593897c73) successfully added.\nConnection 'baremetal' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/9)\nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/11)\nbaremetal configured successfully.\n\nCurrent network interface configuration:\nNAME               UUID                                  TYPE      DEVICE       \nSystem eth0        5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0         \nbaremetal          9ba0964b-3667-42f5-ae81-a0a0936985cf  bridge    baremetal    \nprovisioning       1b5a7497-4ff5-43ec-bb3e-df01e8f070e8  bridge    provisioning \nvirbr0             00fb2639-d510-4c0b-bdad-2a471e7c67c5  bridge    virbr0            \nbridge-slave-eth1  3a46f2d4-5a3e-4b8d-994b-0a055ade7a84  ethernet  eth1         \nbridge-slave-eth2  f2b52c9b-087f-4b7c-9186-0ed593897c73  ethernet  eth2         \n\nInstalling required packages to add the baremetal subnet to the external router.\nVerifying baremetal subnet has been added to external router.\nExternal router configuration correctly.\n[kni@bootstrap GoodieBag]$ \n</code></pre> </li> <li> <p>Create a pull-secret.txt file.  In a web browser, navigate to Install OpenShift on Bare Metal with user-provisioned infrastructure, in the Pull Secret section, click the Copy pull secret link.  </p> <p></p> </li> <li> <p>Create a pull-secret.txt file in the kni user\u2019s home directory by pasting the data just copied.  </p> <pre><code>[kni@bootstrap ~]$ vi pull-secret.txt\n{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3lc3NobGVhc2UtZGaWZ0LXJl9hY2NfNViOTE3NDA1NWIyMWU4ZWQxN2ExYjhmOTU6SDA3UETQxN2Q2NzKSK92JMD3Kkk20kl\n\u2026\nNLXRmeUxFcFVRZnVuRVGl2czNjckTJWdkNpVQkdaVypeHVX1jTVhBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}\n:wq\n[kni@bootstrap ~]$\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#openshift-installation","title":"OpenShift Installation","text":"<p>The installation is based on the latest-4.12 version.  This will need to be updated as new versions are released.</p> <ol> <li> <p>Retrieve the GA OpenShift Installer using the get-ocp-installer.sh script in the kni user\u2019s ~/GoodieBag directory.  This script will download the openshift-client installer, ensure the required packages are installed, create the install-config.yaml file, update the file with the appropriate IP addresses, ssh public keys, and pull-secret, and configure DHCP and DNS.  </p> <pre><code>[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh -h \n/tmp/get-ocp-installer.sh usage:\n-v  Specify version, default is \"latest-4.12\".\n-s  Specify full path of pull-secret.txt file, default is \"~/pull-secret.txt\".\n-d  Specify directory to extract the release image in, default is current directory.\n-h  Display help/usage information.\n\n[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh\nDownloading the openshift-client-linux.tar.gz file.\nExtracting the openshift-client installer.\nThe openshift-baremetal-installer installed successfully.\n\nMaking sure required packages are installed.\nLast metadata expiration check: 0:21:44 ago on Mon 13 Feb 2023 03:02:05 PM EST.\nPackage ansible-2.9.27-1.el8ae.noarch is already installed.\nPackage python3-shade-1.32.0-2.20220110211405.47fe056.el8ost.noarch is already installed.\nPackage python3-openstackclient-4.0.2-2.20220427020029.el8ost.noarch is already installed.\nDependencies resolved.\nNothing to do.\nComplete!\ndone.\nGenerating the install-config.yaml file.\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [Generate configs] **************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************\nok: [localhost]\n\nTASK [Learn kni instances in the project] **************************************************************************************\nok: [localhost]\n\nTASK [Show kni  instances] **************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        {\n            \"OS-DCF:diskConfig\": \"MANUAL\",\n            \"OS-EXT-AZ:availability_zone\": \"leaf1\",\n            \"OS-EXT-SRV-ATTR:host\": null,\n            \"OS-EXT-SRV-ATTR:hostname\": null,\n            \"OS-EXT-SRV-ATTR:hypervisor_hostname\": null,\n    \u2026\n\nPLAY RECAP **********************************************************************************************\nLocalhost        : ok=6    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\ndone.\nUpdating the install-config yaml file.\nConfiguring DHCP and DNS.\nIs this your first attempt to install OpenShift [y|n]: y\n[kni@bootstrap ~]$\n</code></pre> </li> <li> <p>Review the contents of the ~/GoodieBag/install-configs.yaml file.  All IP addresses for the server instances should be updated from the default of X.X.X.X, and the pullSecrets and sshKey variables should be updated with the correct information.  </p> <pre><code>[kni@bootstrap ~]$ more GoodieBag/install-config.yaml\n\u2026\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:a7:80:64\"\n        role: master\n        rootDeviceHints:\n          deviceName: \"/dev/vda\"\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://10.30.0.129\"\n          username: \"deployodf\"\n          password: \"Passw0rd\"\n\u2026\n</code></pre> </li> <li> <p>To ensure the installation doesn\u2019t get interrupted if there is a disconnect in your ssh session, run the installation in a tmux window.  The tmux utility was installed by the get-ocp-installer.sh script.  </p> <pre><code>[kni@bootstrap ~]$ tmux\n</code></pre> </li> <li> <p>Deploy OpenShift using the openshift-baremetal-install command.  </p> <pre><code>[kni@bootstrap ~]$ openshift-baremetal-install --dir ~/clusterconfigs --log-level debug create cluster\n</code></pre> </li> <li> <p>You can monitor your deployment in another window.  </p> <pre><code>[kni@bootstrap ~]$ sudo virsh list\n Id   Name                        State\n -------------------------------------------\n 1    kni-test2-ntpgv-bootstrap   running\n\n[kni@bootstrap ~]$ sudo virsh console &lt;bootstrap-vm&gt;\n\n[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc\n(kni-test2) [kni@bootstrap ~]$ openstack server list --insecure\n+--------------------------------------+------------------+---------+----------\n| ID                                   | Name             | Status  | Networks                                                                        | Image          | Flavor |\n+--------------------------------------+------------------+---------+----------\n| d339db1a-4e50-4c69-8063-d7d92bfcc190 | ipmi_kni-master1 | ACTIVE  | virtualipmi=10.30.0.132                                                         | virtualipmi    |        |\n| ccfaf78c-80f5-4b01-b3ee-ab0fd82a07c0 | ipmi_kni-master2 | ACTIVE  | virtualipmi=10.30.0.85                                                          | virtualipmi    |        |\n| 14a636dd-3709-48e9-a6e2-be689d434ac7 | ipmi_kni-master3 | ACTIVE  | virtualipmi=10.30.0.49                                                          | virtualipmi    |        |\n| 77fda0c4-8a88-4970-806e-540e3946dd3a | ipmi_kni-worker1 | ACTIVE  | virtualipmi=10.30.0.45                                                          | virtualipmi    |        | \n| 80c13e57-2869-4473-99e2-c4748daaf84c | ipmi_kni-worker2 | ACTIVE  | virtualipmi=10.30.0.147                                                         | virtualipmi    |        |\n| 32f1de18-8bdb-4bdf-bdc3-57ddb0e522d8 | ipmi_kni-worker3 | ACTIVE  | virtualipmi=10.30.0.210                                                         | virtualipmi    |        |\n| 85369613-44ec-47fa-9a9b-2476ca9278e8 | kni-master1      | ACTIVE  | baremetal0=10.20.0.8; provisioning0=10.10.0.200                                 | pxeboot        |        |\n| 438088a3-0182-4f9f-bcce-485bce5973d3 | kni-master2      | ACTIVE  | baremetal0=10.20.0.119; provisioning0=10.10.0.160                               | pxeboot        |        |\n| 0239711f-e661-47f3-bc71-eea60eec8763 | kni-master3      | ACTIVE  | baremetal0=10.20.0.100; provisioning0=10.10.0.209                               | pxeboot        |        |\n| 26798b59-1d6c-4d93-b51a-26b59c725a33 | kni-worker1      | SHUTOFF | baremetal0=10.20.0.77; provisioning0=10.10.0.91                                 | pxeboot        |        |\n| 3f11240b-5806-40f9-92d4-82a58df4053b | kni-worker2      | SHUTOFF | baremetal0=10.20.0.236; provisioning0=10.10.0.94                                | pxeboot        |        |\n| 75015e47-7210-4bb8-8f45-8da5f3f78829 | kni-worker3      | SHUTOFF | baremetal0=10.20.0.124; provisioning0=10.10.0.109                               | pxeboot        |        || 27c33f7a-ea95-4cd6-aec9-691fbcfe27c1 | bootstrap        | ACTIVE  | baremetal0=10.20.0.122; vlan1117=10.9.65.140; provisioning0=10.10.0.45 | rhel82-update1 |        |\n+--------------------------------------+------------------+---------+----------\n</code></pre> <p>Other commands that will come in handy in the later state of the deployment:</p> <pre><code>[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n[kni@bootstrap ~]$ oc get clusteroperators\n[kni@bootstrap ~]$ oc get nodes\n</code></pre> <p>We hope it works! If it has, then at the end of the deployment you will see something like this:</p> <pre><code>INFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.kni-test2.hexo.lab\nINFO Login to the console with user: \"kubeadmin\", and password: \"mv22Z-Tv2Zb-pTFgq-xAvki\"\nDEBUG Time elapsed per stage:\nDEBUG     Infrastructure: 29m15s\nDEBUG Bootstrap Complete: 9m54s\nDEBUG  Bootstrap Destroy: 13s\nDEBUG  Cluster Operators: 34m31s\nINFO Time elapsed: 1h13m54s\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#accessing-your-environment","title":"Accessing Your Environment","text":"<p>There should be a few ways to access your environment including using sshuttle or a jumphost.</p>"},{"location":"OCPBaremetalPI/#using-sshuttle","title":"Using sshuttle","text":"<p>The OpenShift Console is not available on the VPN network the lab environment is deployed on but rather the private network of your project.  To access the console, you can use the sshuttle utility which allows you to create a VPN connection using an ssh connection to the bootstrap server.  You need root access on the client machine but not on the bootstrap server.  The sshuttle utility requirements are python 2.3 or higher which are already installed on the bootstrap server. </p>"},{"location":"OCPBaremetalPI/#linux-client-installation","title":"Linux Client Installation","text":"<ol> <li> <p>Install using the dnf command from the EPEL repository.  </p> <pre><code>$ dnf install -y sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required to use sshuttle.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.  </p> <pre><code>$ sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n</code></pre> </li> <li> <p>Access your OpenShift Console in your browser using the link:  </p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#macos-client-installation","title":"MacOS Client Installation","text":"<ol> <li> <p>If not installed already, install homebrew.  Open a terminal window and grab the install.sh script from GitHub and run it.  Wait for the command to finish.  If you are prompted to enter a password, enter your Mac user\u2019s login password and press ENTER.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Make the brew command available inside the terminal window.</p> <pre><code>\"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\n. ~/.zprofile\n</code></pre> </li> <li> <p>Install sshuttle.</p> <pre><code>$ brew install sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.</p> <pre><code>~ % sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n[local sudo] Password: \nkni@172.20.17.124's password: \nc : Connected to server.\n</code></pre> <p>NOTE: As traffic is generated on the VPN connection, warning messages will be displayed in the terminal.</p> <p>Example:</p> <p>s: warning: closed channel 7 got cmd=TCP_DATA len=517  c : warning: closed channel 11 got cmd=TCP_STOP_SENDING len=0   s: warning: closed channel 11 got cmd=TCP_DATA len=517   s: warning: closed channel 11 got cmd=TCP_EOF len=0</p> </li> <li> <p>Access your OpenShift Console in your browser using the link:</p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#using-a-jumphost-from-openstack","title":"Using a JumpHost from OpenStack","text":"<ol> <li> <p>Access your OpenStack environment, select Compute-&gt;Instances.  Click the Launch Instance icon on the right.  Enter the Instance Name, Description, AZ, and Count.  Click Next.  </p> <p>NOTE: At this time, the procedures below will yield issues with accessing the jumphost due to a known issue with injecting the ssh keys. Please use the procedures under Using sshuttle above.</p> </li> <li> <p>On the Source screen, make sure Select Boot Source is Image and Create New Volume is No.  In the Available list of images, find fedora-cloud37 and click the up arrow to the right of the entry to move it up to the Allocated section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Flavor screen, find t2.medium in the list of Available flavors, click the up arrow to the right of the entry to move it to Allocated.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Networks screen, add the vlan1117 and baremetal0 entries in the Available list to the Allocated list.  Scroll to the bottom, click Next.</p> </li> <li> <p>Click Next in the lower right on the Network Ports screen.  On the Security Groups screen, click the down arrow next to the default security group in the Allocated section to move it to the Available section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Key Pair screen, import your ssh keys using the Import Key Pair icon.  Remove the hextupleo_pub_key key from Allocated by clicking the down arrow to the far right.  Once complete, click the Launch Instance in the lower right as the remaining sections are not required or needing to be updated.</p> </li> <li> <p>Ssh to environment and enable x11 and maybe firefox/chrome if you\u2019d like.  </p> <p>``` [fedora@fedora-jumpbox ~]$ sudo dnf -y groupinstall gnome [fedora@fedora-jumpbox ~]$ sudo dnf -y group install \"Basic Desktop\" GNOME [fedora@fedora-jumpbox ~]$ sudo systemctl set-default graphical.target Removed /etc/systemd/system/default.target. Created symlink /etc/systemd/system/default.target \u2192 /usr/lib/systemd/system/graphical.target. [fedora@fedora-jumpbox ~]$ sudo -i [root@fedora-jumpbox ~]# passwd fedora Changing password for user fedora. New password:  BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word Retype new password:  passwd: all authentication tokens updated successfully. [root@fedora-jumpbox ~]# reboot</p> </li> <li> <p>Adding DNS is optional and not a requirement.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-odf-storage","title":"Deploying ODF (storage)","text":"<p>Official docs -&gt; https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/index </p> <ol> <li> <p>Install local storage operator; click Operators-&gt;OperatorHub.  Search for local.  Select the Local Storage Operator, in the pop up window, click the Install icon.  </p> <p></p> </li> <li> <p>Keep the defaults; scroll to the bottom of the screen and click the Install icon.  </p> <p></p> </li> <li> <p>Install ODF Operator; click Operators-&gt;OperatorHub, search for Data Foundation.  Select the OpenShift Data Foundation Operator, in the pop-up window keep the defaults, scroll to the bottom and click Install.  </p> <p></p> </li> <li> <p>Once the operator has successfully been installed, the GUI will indicate that a change has occurred and to refresh.  Create the StorageSystem; select Storage-&gt;Data Foundation.  Click the Storage System tab; click the Create StorageSystem icon on the right.  </p> <p></p> </li> <li> <p>For the Backing storage type, select Create a new StorageClass using local storage devices.  Click Next.  </p> <p></p> </li> <li> <p>The worker nodes have been preconfigured with 100GB secondary drives.  </p> <pre><code>[kni@bootstrap ~]$ ssh core@kni-worker1\nRed Hat Enterprise Linux CoreOS 412.86.202301191053-0\nPart of OpenShift 4.12, RHCOS is a Kubernetes native operating system\nmanaged by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\nhttps://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html\n\n[core@kni-worker1 ~]$ sudo fdisk -l | grep vdb\nDisk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors\n</code></pre> </li> <li> <p>On the Create local volume set screen, enter a name for the volume set.  In the example below, localvolumes is used.  Verify the Disks on all nodes (3 node) is selected in the Filter disks by section and All is selected for the Disk type.  Click the Next icon.  </p> <p></p> </li> <li> <p>A pop up confirmation window will display providing additional information to consider if this is a stretched cluster.   Click the Yes icon if the settings are correct to create the LocalVolumeSet.  </p> </li> <li> <p>On the Capacity and nodes screen, accept the defaults and click Next.  </p> <p></p> </li> <li> <p>On the Security and network screen, accept the default of SDN.  Click Next.</p> </li> <li> <p>On the Review and create screen, review the configuration and click Next if correct to create the Storage System.</p> </li> <li> <p>It will take several minutes to create the local volumes and configure the storage.  You can monitor the progress on the Storage-&gt;Data Foundation-&gt;StorageSystems screen.  Click the ocs-storage-cluster-storagesystem link to access the Overview dashboard.  As the system configures the storages, messages will be logged in the Activity section.  Once complete, the Status for Storage Cluster and Data Resiliency should be green.  </p> <p></p> </li> <li> <p>To verify local storage has been create, click the BlockPools menu item along the top; click the ocs-storagecluster-cephblockpool link.  In the Inventory widget, click the links to view the available Storage Classes or the Persistent Volume Claims.  </p> <p></p> <p>Storage Class:</p> <p></p> <p>PersistentVolumeClaims:</p> <p></p> </li> <li> <p>The local storage and volume claims can be viewed with the CLI.  </p> <pre><code>[kni@bootstrap ~]$ oc get all -n openshift-local-storage\nNAME                                         READY   STATUS    RESTARTS   AGE\npod/diskmaker-discovery-2xnxv                2/2     Running   0          3m46s\npod/diskmaker-discovery-9vjbf                2/2     Running   0          3m49s\npod/diskmaker-discovery-ff2cc                2/2     Running   0          4m2s\npod/diskmaker-manager-c54j8                  2/2     Running   0          2m10s\npod/diskmaker-manager-prgbh                  2/2     Running   0          2m10s\npod/diskmaker-manager-xtw58                  2/2     Running   0          2m10s\npod/local-storage-operator-9bc77c9cf-rzqjt   1/1     Running   0          32m\n\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/local-storage-discovery-metrics   ClusterIP   172.30.23.42    &lt;none&gt;        8383/TCP   11m\nservice/local-storage-diskmaker-metrics   ClusterIP   172.30.248.10   &lt;none&gt;        8383/TCP   2m10s\n\nNAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/diskmaker-discovery   3         3         3       3            3           &lt;none&gt;          11m\ndaemonset.apps/diskmaker-manager     3         3         3       3            3           &lt;none&gt;          2m10s\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/local-storage-operator   1/1     1            1           32m\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/local-storage-operator-9bc77c9cf   1         1         1       32m\n\n[kni@bootstrap ~]$ oc get pv\nNAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\nlocal-pv-2cc6fb0    100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-3faa9a90   100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-e2c0ad97   100Gi      RWO            Delete           Available           localvolumes            114s     3m45s\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-ocpv","title":"Deploying OCPv","text":""},{"location":"OCPBaremetalPI/#deleting-your-project","title":"Deleting Your Project","text":"<ol> <li> <p>To remove your entire project, use the Template in Ansible Automation Platform.</p> <p>Ansible Automation Platform</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Hextupleo - delete my project.</p> <p></p> </li> <li> <p>Update the project_name: and project_password: fields.  Click the Next icon in the lower left.</p> </li> <li> <p>Review the information on the Preview screen.  When ready to move forward, click the Launch icon in the lower left corner.  The Jobs Output screen for the new job will display ongoing output as the system progresses through the Ansible playbook.  You can monitor to completion or just check back to ensure your Job finishes successfully.</p> <p></p> </li> <li> <p>Verify your job finished successfully in the Jobs listing.</p> <p></p> </li> </ol>"},{"location":"OCPBaremetalPI/#troubleshooting","title":"Troubleshooting","text":"<p>Getting NTP applied for workers and masters.  </p> <pre><code>[kni@bootstrap ~]$ wget \n[kni@bootstrap ~]$ wget \n\n[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n\n[kni@bootstrap ~]$ oc apply -f 99_workers-chrony-configuration.yaml\n[kni@bootstrap ~]$ oc apply -f 99_masters-chrony-configuration.yaml\n</code></pre> <p>Check status of IPMI.  </p> <pre><code>kni@bootstrap ~]$ ipmitool -I lanplus -H 10.30.0.129 -U deployodf -P Passw0rd chassis power status\n</code></pre>"},{"location":"OCPBaremetalPI/#appendix","title":"Appendix","text":"<p>The get-ocp-installer.sh script is provided as a quick method to complete the prep work before installing OpenShift.  If you want to execute manually, the commands are provided below for your reference.  </p> <p>Set the environment variables and download the openshift-client-linux.tar.gz file.  </p> <pre><code>[kni@bootstrap ~]$ export VERSION=latest-4.12\n[kni@bootstrap ~]$ export RELEASE_IMAGE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/release.txt | grep 'Pull From: quay.io' | awk -F ' ' '{print $3}')\n[kni@bootstrap ~]$ export cmd=openshift-baremetal-install\n[kni@bootstrap ~]$ export pullsecret_file=~/pull-secret.txt\n[kni@bootstrap ~]$ export extract_dir=$(pwd)\n[kni@bootstrap ~]$ curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/openshift-client-linux.tar.gz | tar zxvf - oc\noc\n[kni@bootstrap ~]$ sudo cp oc /usr/local/bin\n[kni@bootstrap ~]$ oc adm release extract --registry-config \"${pullsecret_file}\" --command=$cmd --to \"${extract_dir}\" ${RELEASE_IMAGE}\n[kni@bootstrap ~]$ sudo cp openshift-baremetal-install /usr/local/bin\n</code></pre> <p>Verify the installer file has been downloaded.  </p> <pre><code>[kni@bootstrap ~]$ ls\nGoodieBag  nohup.out  oc  openshift-baremetal-install  pull-secret.txt\n</code></pre> <p>Create or generate install-config.yaml.</p> <p>For your convenience, there is an ansible playbook inside a Goodiebag directory that helps gather mac information for the OCP nodes. It uses openstack APIs to gather the information. Install the required packages first Note: This step would typically not be performed on different hardware.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf -y install ansible python3-shade python3-openstackclient\n</code></pre> <p>Generate the install-config.yaml  </p> <pre><code>[kni@bootstrap ~]$ ansible-playbook GoodieBag/generate-configs.yml\n</code></pre> <p>The file will look similar to this:  </p> <pre><code>[kni@bootstrap ~]$ cat GoodieBag/install-config.yaml \napiVersion: v1\nbasedomain: hexo4.lab\nmetadata:\n  name: \"kni-test\"\nnetworking:\n  machineCIDR: 10.20.0.0/24\n  networkType: OVNKubernetes\ncompute:\n  - name: worker\n     replicas: 3\ncontrolPlane:\n  name: master\n  replicas: 3\n  platform:\n    baremetal: {}\nplatform:\n  baremetal:\n    apiVIP: &lt;api-ip&gt;\n    ingressVIP: &lt;wildcard-ip&gt;\n    provisioningNetworkCIDR: &lt;CIDR&gt;\n    hosts:\n      - name: \"kni_worker3\"\n        bootMACAddress: \"fa:16:3e:0f:21:4d\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker2\"\n        bootMACAddress: \"fa:16:3e:5c:94:dd\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker1\"\n        bootMACAddress: \"fa:16:3e:86:ce:e8\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master3\"\n        bootMACAddress: \"fa:16:3e:2e:7a:86\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master2\"\n        bootMACAddress: \"fa:16:3e:23:03:f5\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:43:75:71\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\npullSecret: '&lt;pull_secret&gt;'\nsshKey: '&lt;ssh_pub_key&gt;'\n</code></pre> <p>You must update the ipmi IP addresses for each of the server instances, the pullSecret, and sshKey variables.  Get the ipmi addresses using the openstack server command.  First, you need to source the variables in the ~/GoodieBag/rc file.   <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc   &lt; - replace with your projectname\n(kni-test) [kni@bootstrap ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"      \" $8 }'\nipmi_kni_worker3      virtualipmi=10.30.0.106\nipmi_kni_worker2      virtualipmi=10.30.0.122\nipmi_kni_worker1      virtualipmi=10.30.0.139\nipmi_kni-master3      virtualipmi=10.30.0.37\nipmi_kni-master2      virtualipmi=10.30.0.150\nipmi_kni-master1      virtualipmi=10.30.0.132\n</code></pre> <p>Update each server\u2019s host section replacing the X.X.X.X with the correct IP address.</p> <pre><code>hosts:\n      - name: \"kni_worker3\"\n      bootMACAddress: \"fa:16:3e:0f:21:4d\"\n      role: worker\n      hardwareProfile: unknown\n      bmc:\n        address: \"ipmi://X.X.X.X\"  \u2190 replace with 10.30.0.106\n        username: \"kni-test\"\n        password: \"changeme\"\n</code></pre> <p>Update the pullSecret variable at the bottom of the file with the pull-secret.txt file contents you created in the Openshift Installation.  </p> <pre><code>(kni-test) [kni@bootstrap ~]$ cat pull-secret.txt \n{\"auths\":{\"cloud.openshift.com\":{\"auth\":.....\n\npullSecret:'{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3BlbnNoaWHVXQkdaVy1jTVhZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NMkRRN1RTWQxN2ExYjhmOTU6TQxN2Q2NzViOTESDA3UEFGOFNA4QzJTQVOTDNDUFU5SlJY8UV0pSWl\n\u2026\nnZnVzNfN3NDA1NWIyMWU4Zjckx2ZGJHVVzFJd1FNLXRmeUxFcFVRZnVuRTJWdkNpo3MEw3MlJGl2czNkRzLUdpeBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}'\n</code></pre> <p>Update the sshKey variable with your ssh public key.  If you have not generated it, use the ssh-keygen command.  </p> <pre><code>[kni@bootstrap ~]$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/kni/.ssh/id_rsa): &lt;enter&gt;\nEnter passphrase (empty for no passphrase): &lt;enter&gt; \nEnter same passphrase again: \nYour identification has been saved in /home/kni/.ssh/crapid_rsa.\nYour public key has been saved in /home/kni/.ssh/crapid_rsa.pub.\nThe key fingerprint is:\nSHA256:CVnMl7uiOYLFK1jNkWQZ7M29cbsNhdP13Hfvbsy5Yfg kni@bootstrap\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   ..o o.  .     |\n|    =  oo o   .  |\n|   + +o. . + . o.|\n|    + o.o.= o   *|\n|   + .  S+ =    +|\n|  . =   o +   . .|\n| o o . o . + . *.|\n|. o o +   . . o.*|\n|   . . .       Eo|\n+----[SHA256]-----+\n[kni@bootstrap ~]$ cat .ssh/id_rsa.pub \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3PtY3V32PbEbXpuVaaPV\n\u2026\nfionHKM8gdFbXo8yWqCTdT3HuMs9gjjjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap\n</code></pre> <p>Copy the id_rsa.pub contents and paste as the sshKey.</p> <pre><code>sshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3Pt  \n\u2026\nfjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap'\n</code></pre> <p>Openshift Baremetal IPI also requires DHCP and DNS to be configured.  Copy the respective files from the GoodieBag directory to /etc.  Enable and start the dnsmasq daemon.  </p> <pre><code>[kni@bootstrap ~]$ sudo cp GoodieBag/hosts /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/kni.dns /etc/dnsmasq.d/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf.upstream /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf /etc/\n[kni@bootstrap ~]$ sudo systemctl enable dnsmasq\nCreated symlink /etc/systemd/system/multi-user.target.wants/dnsmasq.service \u2192 /usr/lib/systemd/system/dnsmasq.service.\n[kni@bootstrap ~]$ sudo systemctl start dnsmasq\n</code></pre> <p>If this is not your first attempt at installing OpenShift, cleanup the remnants of the first attempt. </p> <pre><code>[kni@bootstrap ~]$ for i in $(sudo virsh list | tail -n +3 | grep bootstrap | awk {'print $2'})\ndo\n    sudo virsh destroy $i;\n    sudo virsh undefine $i;\n    sudo virsh vol-delete $i --pool default;\n    sudo virsh vol-delete $i.ign --pool default;\ndone\n[kni@bootstrap ~]$ rm -rf ~/clusterconfigs/auth ~/clusterconfigs/terraform* ~/clusterconfigs/tls ~/clusterconfigs/metadata.json\n</code></pre> <p>Create the clusterconfigs working directory in the kni home directory.</p> <pre><code>[kni@bootstrap ~]$ mkdir ~/clusterconfigs\n</code></pre>"}]}