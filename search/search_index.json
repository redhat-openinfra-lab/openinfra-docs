{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Red Hat NA-SSA Lab's Documentation Site.","text":"<p>This site is created using MKDocs.  For full documentation visit mkdocs.org.</p>"},{"location":"#update-the-site-or-add-documentation","title":"Update the Site or Add Documentation","text":"<p>The documentation is hosted in GitHub at https://github.com/redhat-openinfra-lab/openinfra-docs.</p> <p>Please refer to the Documentation Contribution for more information on contributing to this documentation.</p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages.\n    /images   # images that are used in docs pages\n</code></pre>"},{"location":"#helpful-commands","title":"Helpful commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#lab-environment","title":"Lab Environment","text":"<p>If you'd like to see the physical layout of the hardware in the lab, check out this diagram in  Lucid Charts  or if you don't have access, here's a static diagram (click to open a larger version in a new tab):</p> <p></p>"},{"location":"#lab-ssa-links","title":"Lab SSA Links","text":"<p>SSA Links</p>"},{"location":"#lab-vpn-access","title":"Lab VPN Access","text":"<ul> <li>Connect to the Red Hat VPN</li> <li>Access the Fortigate Administration Page.</li> <li>You will need to login as the <code>user_mgr</code> account. These credentials are in the BitWarden Vault under Fortigate User Management FW.</li> <li> <p>After logging in, navigate to the User &amp; Device -&gt; User Definition on the left:</p> <p></p> </li> <li> <p>Click Create New </p> <p>NOTE: If you are changing your password, click Edit User and change it there.</p> <ul> <li>User Type: Local User</li> <li>Login Credentials: Input user name and password, use your Kerberos name for consistency.</li> <li>Contact Information: Input your Red Hat e-mail address</li> <li>Extra Information: Select the following</li> </ul> <p></p> </li> </ul> <p>NOTE&gt; : If you accidentally create a user and you want to delete it, first edit the user, remove the group membership from it, disable group membership and then you will be able to delete it.</p> <p>Once you have the new credentials configured, log out in the top right corner.  Test your new credentials by connecting to the  Fortinet VPN Portal</p> <p>Connecting validated the credentials.  Download the Fortinet VPN Client from their Product Downloads page or in the portal, from the Download FortiClient dropdown list.</p> <p></p> <p>Select the appropriate client and download it from where you\u2019re redirected to.</p> <p>Install the client - this screenshot was taken from a forticlient running on a Mac. When you first start it, you will be prompted to configure the VPN.</p> <p></p> <p>Click Save.  You can then connect to the NA SSA Lab VPN.</p> <p>You can also use OpenConnect (version 8+) to connect to the NA SSA Lab VPN from a Linux desktop running GNOME. First install OpenConnect (sudo dnf install openconnect - or debian or whatever Linux flavor you are running equivalent to), then install NetworkManager-openconnect-gnome (sudo dnf install NetworkManager-openconnect-gnome).</p> <p>Open GNOME Settings, navigate to Network and select the plus button to add a new VPN profile:</p> <p></p> <p>Select \u201copenconnect\u201d.  In the properties type the following:</p> <p>Name: (Whatever you like but \u201cNA SSL Lab VPN\u201d sounds good to me!) Gateway: 209.132.179.151:20443</p> <p></p> <p>Save the new VPN profile and connect to the VPN. Please note that the VPN gateway uses a self signed certificate - you might be warned about that. Accept the certificate and proceed.</p> <p>You can also use an alternative client that works well on headless systems. It does not have any dependencies on GNOME or dbus etc - like the network-manager method described above.</p> <ul> <li>Install openforticlient (sudo dnf install openforticlient)  </li> <li>Connect to the VPN from the command line:  </li> <li>sudo openfortivpn 209.132.179.151:20443 --username=mlecki --trusted-cert 646fd76ad8c617bfd94f3318f25e592a88fd2735949dfde0281df19de43b47ce</li> </ul>"},{"location":"#private-bin","title":"Private Bin","text":"<p>https://privatebin.corp.redhat.com</p>"},{"location":"MiscellaneousNotes/","title":"Miscellaneous Notes","text":""},{"location":"MiscellaneousNotes/#keys-to-the-kingdom","title":"Keys to the Kingdom","text":"<p>For security purposes, all administrative and root logins are kept in BitWarden for safe keeping.  This is an enterprise approved password management application.  BitWarden is integrated with Red Hat Rover Groups for access management.</p>"},{"location":"MiscellaneousNotes/#red-hat-rover-groups","title":"Red Hat Rover Groups","text":"<p>You can find Rover Groups in the  Rover application.  In the Search bar, enter groups.  </p> <p>Access  Rover Groups ; the main page will display your group membership.  The <code>na-ssa-infrastructure</code> group is the group used for BitWarden membership.  Current owners of the group are Ken, Chris, and Peter.  They can help with any membership additions and deletions.</p>"},{"location":"MiscellaneousNotes/#join-the-group-e-mail","title":"Join the Group E-Mail","text":"<p>Once a user is added to the group, they will recieve an e-mail to accept the invitation.  Until they join, they won't be in the group.</p>"},{"location":"MiscellaneousNotes/#bitwarden","title":"BitWarden","text":"<p>BitWarden  can be access via your favorite browser.  If necessary create an account using your @redhat.com email address.  </p> <p>The Na Ssa Infrastructure Collection has all of the lab's passwords, keys, and secrets.  You can create your own Folders to organize the secrets.  Keep in mind, the Folders are only available to you.  They are not shared folders with the na-ssa-infrastructure group, just the collection is shared.  Feel free to organize however you want. </p>"},{"location":"MiscellaneousNotes/#weblinks-isos","title":"Weblinks &amp; ISOs","text":"<p>The rhel8 repo server (172.20.129.19) has an alias created for it for www.openinfra.lab and has a number of useful links on it.</p> <p>Additionally, if you host your ISO files on this server in <code>/var/www/html</code>, they will be auto-populated on this page to make copying of the links to them useful for pasting into IMM or any other UI where you are prompted for the URL to an ISO.</p>"},{"location":"MiscellaneousNotes/#gitea","title":"Gitea","text":"<p>This git installation is running as an Operator within the Openshift Baremetal Cluster.  In order to customize the deployment to enable SSH access, the default configuration needed to be modified.  This was accomplished by:</p> <ol> <li> <p>Under Workloads-&gt;ConfigMaps in the gitea namespace, examine the config ini that was being used.  In our case it was the cloud-infra-git-config ConfigMap.</p> </li> <li> <p>Create a new ConfigMap with the contents of the previous config and any changes that are neeeded.  We called this nassa-gitea-static-config.</p> </li> <li> <p>Under Operators-&gt;Installed Operators select the Gitea Operator and go to the Gitea tab.</p> </li> <li> <p>Select cloud-infra-git, go to YAML and added the following line to the spec:</p> </li> </ol> <pre><code>  giteaConfigMapName: nassa-gitea-static-config\n</code></pre>"},{"location":"slideDeckLinks/","title":"Presentation Links","text":"<p>OCP HA-DR 0422 Presentation by Venkat Kolli on MDR/RDR/Cluster HA</p> <p>OpenShift Virtualization RoadShow Presentation - Source Deck</p> <p>OCPv Technical Presentation Deep Dive on OCPv </p> <p>ODF Roadmap Latest - Internal Only Maintained by Eran Tamir</p>"},{"location":"slideDeckLinks/#bs-presentations","title":"B.'s presentations","text":"<p>Ceph Presentation - HackFest Deploying Stand-Alone Ceph Storage (integrating with OpenShift and OpenStack)</p>"},{"location":"Ceph/CephAuthorization/","title":"Authorization","text":""},{"location":"Ceph/CephAuthorization/#introduction","title":"Introduction","text":"<p>Ceph uses user accounts for internal communication between Ceph daemons, client applications accessing the cluster through librados, and for cluster administration.  Administrative and all client access have accounts that begin with the client. prefix.</p> <p>The Ceph super user account is client.admin with capabilities that allow the account to access everything and to modify the cluster configuration.  This is the default account that is used unless you specify a user name with the --name or --id options.</p> <p>NOTE: When using the --name option, use the client. prefix.  But when using the --id option, the client. prefix is assumed.  Alternately you can also export CEPH_ARGA='--id cephuser\".</p> <p>The keyring files are maintained in the /etc/ceph directory.  If your keyring is not in this directory, you must pass it the --keyring option along with the file name that contains the secret key.</p>"},{"location":"Ceph/CephAuthorization/#capabilities","title":"Capabilities","text":"<ul> <li>r grants read access</li> <li>w grants write access</li> <li>x grants authorization to execute extended object classes</li> <li>* grants full access</li> </ul>"},{"location":"Ceph/CephAuthorization/#profiles","title":"Profiles","text":"<ul> <li>profile rbd - grants read/write access to any RBD pool</li> <li>profile rbd-read-only - grants read only access to any RBD pool</li> </ul>"},{"location":"Ceph/CephAuthorization/#create-account","title":"Create Account","text":"<p>NOTE: to save the key to a file use the -o /etc/ceph/ceph.client.name.keyring option</p> <p>To create a client account to read and write to any RBD pool:</p> <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw'\n</code></pre> <p>Limit the user to a specific pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool'\n</code></pre></p> <p>Limit the user to specific objects in a specific pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool object_prefix my_prefix'\n</code></pre></p> <p>Limit the user to a specific namespace with a pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool namespace=myappspace'\n</code></pre></p> <p>Limit the user read/write access to a specific path (used by CephFS): <pre><code># ceph fs authorize cephfs client.myappuser /myapppath rw\n# ceph auth get client.myappuser\nexported keyring for client.myappuser\n[client.myappuser]\n        key = ABE9asvIhDW7FJE#kii982qavejJKEJjEji==\n        caps mds = 'allow rw path=/myapppath\n        caps mon = 'allow r'\n        caps osd = 'allow rw pool=cephfs_data'\n</code></pre></p> <p>Limit the user to a specific administraive commands against a mon: <pre><code># ceph auth get-or-create client.operator mon 'allow r allow command \"auth get-or-create\", allow command \"auth list\"' \n</code></pre></p> <p>List Users: <pre><code># ceph auth list\n</code></pre></p> <p>Get current key and permissions of a user: <pre><code># ceph auth print-key client.myappuser\n# ceph auth get client.myappuser\n</code></pre></p>"},{"location":"Ceph/CephAuthorization/#update-ceph-dashboard-admin-credentials","title":"Update Ceph Dashboard admin Credentials","text":"<pre><code>echo 'thepassword' &gt; dash.pass\nceph dashboard ac-user-set-password admin -i dash.pass\n</code></pre>"},{"location":"Ceph/CephAuthorization/#update-capabilities","title":"Update Capabilities","text":"<p>Use the ceph auth caps command and the same options as creating an account.  Keeping in mind that any update to the user will overwrite the existing capabilities, therefore make sure you include any existing capabilites that need to be retained along with any additional.</p>"},{"location":"Ceph/CephAuthorization/#delete-account","title":"Delete Account","text":"<pre><code># ceph auth del client.myappuser\n</code></pre> <p>NOTE: You should also remove the /etc/ceph/ceph.client.name.keyring file.</p>"},{"location":"Ceph/CephCrushMap/","title":"CRUSH Map","text":""},{"location":"Ceph/CephCrushMap/#introduction","title":"Introduction","text":"<p>Ceph calculates which OSDs should hold the objects by using a placement algorithm called CRUSH, Controlled Replication Under Scalable Hashing.  Objects are assigned to placement groups (PGs) and CRUSH determines which OSDs those placement groups should use to store their objects.</p> <p>Crush uses the concept of buckets.  Bucket types include root, region, datacenter, room, pod, pdu, row, rack chassis and host.  You can also had your own types.</p>"},{"location":"Ceph/CephCrushMap/#crush-map-commands","title":"CRUSH Map Commands","text":"<p>Dump the crush map into a binary file and then convert to text file: <pre><code># ceph osd getcrushmap -o /tmp/map.bin\n# crushtool -d /tmp/map.bin -o /tmp/map.txt\n</code></pre></p> <p>Compile a CRUSH map from text file, test it, and then import it: <pre><code># crushtool -c /tmp/map.txt -o /tmp/map.new.bin\n# crushtool -i /tmp/map.new.bin --test\n# ceph osd setcrushmap -i /tmp/map.new.bin\n</code></pre> Sample CRUSH rule to select SSD for primary copy and HDDs for replicas:</p> <p>rule ssd-first {       id 5       type replicated       min_size 1       max_size 10       step take default class ssd       step chooseleaf firstn 1 type host       step emit       step take default class hdd       step chooseleaf firstn -1 type host       step emit   }   </p> <p>Set the OSD Device Class: <pre><code># ceph osd crush set-device-class osd.1 ssd\n</code></pre></p> <p>Remove a device class: <pre><code># ceph osd crush rm-device-class hdd\n</code></pre></p> <p>List configured device classes: <pre><code># ceph osd crush class ls\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#customizing-crush-map","title":"Customizing CRUSH Map","text":"<p>Create buckets: <pre><code># ceph osd crush add-bucket *name* *type* \n</code></pre></p> <p>Organize the buckets <pre><code># ceph osd crush move *bucket* type=*parent*\n</code></pre></p> <p>Place the OSDs as leaves in the correct location: <pre><code># ceph osd crush set osd.1 1.0 root=default rack=rack1 host=hosta\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#adding-crush-rules","title":"Adding CRUSH Rules","text":"<p>Create a replicated rule to start from root, replicate across buckets of type, using devices of class: <pre><code># ceph osd crush rule create-replicated *name* root *type* *class*\n</code></pre></p> <p>Create an erasure-coded pool to start from crush-root, using crush-failure-domain and crush-device-class: <pre><code># ceph osd erasure-code-profile set *myprofile* k=4 m=2 crush-root=root crush-failure-domain=rack crush-device-class=ssd\n</code></pre></p> <p>List EC profiles and CRUSH Rules: <pre><code># ceph osd erasure-code-profile ls\n# ceph osd crush rule ls\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#calculating-pgs","title":"Calculating PGs","text":"<p>Single pool:</p> <p>totalPGs = (OSDs * 100 ) / numberOfReplicas</p> <p>Red Hat recommends the use of the Ceph Placement Groups per Pool Calculator, https://access.redhat.com/labs/cephpgc/, from the Red Hat Customer Portal Labs.</p>"},{"location":"Ceph/CephCrushMap/#osd-map-commands","title":"OSD Map Commands","text":"<pre><code># ceph osd dump\n# ceph osd getmap -o osd.map.bin\n# osdmaptool --print osd.map.bin\n# osdmaptool --export-crush crush.bin osd.map.bin\n# crushtool -d crush.bin crush.txt\n# crushtool -c crush.txt -o crush.new.bin\n# osdmaptool --import-crush crush.new.bin osd.map.bin\n# osdmaptool --test-map-pgs-dump /tmp/osd.map.bin\n</code></pre>"},{"location":"Ceph/CephFS/","title":"CephFS","text":""},{"location":"Ceph/CephFS/#introduction","title":"Introduction","text":"<p>A single CephFS filesystem can be created with multiple subvolumes defined within the CephFS for fine grained authorization and access control.</p> Option Description r Read access to the specified folder w Write access to the specified folder p Required to use layouts and quotas s Required to create snapshots <p>By default the Fuse client mounts the root directory; use the <code>ceph-fuse -r /directory</code> command to mount a subdirectory.</p>"},{"location":"Ceph/CephFS/#configuration","title":"Configuration","text":"<p>Create the CephFS volume; this creates the pools, the volumes, and starts the MDS service on the hosts. <pre><code># ceph fs volume create fs-name --placement=\"2 ceph01,ceph02\"\n</code></pre></p> <p>Create CephFS Manually: <pre><code># ceph osd pool create cephfs_data [erasure]\n# ceph osd pool create cephfs_metadata \n# ceph osd pool application enable cephfs_data cephfs\n# ceph osd pool application enable cephfs_metadata cephfs\n# ceph fs new fs-name cephfs_metadata cephfs_data\n</code></pre></p> <p>Add erasure coded pool to your CephFS: <pre><code># ceph fs add_data_pool fs-name data-pool\n</code></pre></p> <p>Deploy the CephFS service and verify the service is up: <pre><code># ceph orch apply mds fs-name --placement=\"2 ceph01 ceph02\"\n# ceph mds stat\n</code></pre></p>"},{"location":"Ceph/CephFS/#authorization","title":"Authorization","text":"<p>All clients must be authorized to access the CephFS filesystem.</p> <p>Authorize a client to use extended attributes and snapshots for files in the /mydirectory: <pre><code># ceph fs authorize mycephfs client.user / r /mydirectory rwps\n</code></pre></p> <p>NOTE: When authorizing both extended attributes and snapshots, they must be in alpa order. NOTE: Prevent the deletion of files from UID/GID=0 by using the <code>root_squash</code> option.</p>"},{"location":"Ceph/CephFS/#mounting-with-kernel-client","title":"Mounting with kernel client","text":"<p>Use the kernel client with LINX kernel versions of 4.0 and newer.  You can mount the root filesystem or subdirectories.  </p> <ul> <li>Verify the ceph-common package is installed.</li> </ul> <p>Mount: <pre><code># mount -t ceph mon1,mon2,etc:/path mount-point -o name=client.user1,fs=,mycephfs\n</code></pre></p> <p>NOTE: Subdirectories in the CephFS can be mounted as well. NOTE: You can also leave off the <code>mon1,mon2,etc</code> and Ceph will use the monitors in the /etc/ceph/ceph.conf file.</p> Option Name Description name=name Cephx client ID to use; default is guest fs=fs-name Name of CephFS filesystem to mount secret=secret-value Value of the secret key for the client (no longer needed) secretfile=filename Path to file that contains client secret (no longer needed) rsize=bytes Maximum read size in bytes wsize=bytes Maximum write size in bytes; default none <p>Persistent Mount /etc/fstab Entry: <pre><code>mon1,mon2:/ /mnt/cephfs ceph name=user1,_netdev 0 0\n</code></pre></p>"},{"location":"Ceph/CephFS/#mounting-with-fuse-client","title":"Mounting with Fuse client","text":"<p>Use the FUSE client when the LINUX kernel version is 4.0 or earlier.  Limitation is that you have to mount the root filesystem.  </p> <p>NOTE: This was stated in the online course.  The documentation indicates otherwise.  Use the -r option to instruct the client to treat the path as its root.</p> <ul> <li>Verify the ceph-common and ceph-fuse packages are installed.</li> <li>Verify the /etc/ceph/ceph.conf exists.</li> <li>When using the FUSE client as a non-root user, add user_allow_other in the /etc/fuse.conf file.</li> </ul> <p>Authorize the client; providing read/write access and ability to create snapshots: <pre><code># ceph fs authorize mycephfs client.fsuser / r /fsuser-directory rws\n</code></pre></p> <p>Mount: <pre><code># ceph-fuse /mnt/mycephfs -n client.fsuser --client-fs=mycephfs\n</code></pre></p> <p>Persistent Mount /etc/fstab Entry: <pre><code>mon-hostname:/ /mnt/cephfuse fuse.ceph ceph.id=myuser,_netdev 0 0\n</code></pre> NOTE:  If you don't use the default name and location of the user's keyring file, then use the --keyring option to specify the path to the file.</p>"},{"location":"Ceph/CephFS/#remove-cephfs-filesystem","title":"Remove CephFS Filesystem","text":"<p>Mark CephFS down, then remove it: <pre><code># ceph fs set fs-name down true\n# ceph fs rm fs-name --yes-i-really-mean-it\n</code></pre></p> <p>NOTE: the mon_allow_pool_delete option must be set to true for the above to succeed.  </p>"},{"location":"Ceph/CephFS/#mds-autoscaler","title":"MDS Autoscaler","text":"<p>CephFS requires at least one active MDS server and one standby for HA.  The MDS Autoscaler ensures the availability of enough MDS daemons.  The modules monitors the number of ranks and number of standby daemons and adjusts the number of MDS daemons that the orchestrator spawns.</p> <p>To enable: <pre><code>ceph mgr module enable mds_autoscaler\n</code></pre></p> <p>To set the max number of active MDS daemons: <pre><code>ceph fs set mycephfs max_mds 2  \n</code></pre></p> <p>To set the max number of standby MDS daemons: <pre><code>ceph fs set mycephfs standby_count_wanted 2  \n</code></pre></p>"},{"location":"Ceph/CephFS/#mdf-failover","title":"MDF Failover","text":"<p>When the active MDS becomes unresponsive, a MON daemon waits for the number of seconds specified by the mds_beacon_grace parameter.  If unresponsive after this number of seconds, the MON marks the MDS daemon as laggy and one of the standby daemons becomes active.</p> <pre><code># ceph config get mds mds_beacon_grace\n15.000000\n</code></pre>"},{"location":"Ceph/CephFS/#cephfs-mirror","title":"CephFS Mirror","text":"<p>To enable and deploy the feature: <pre><code># ceph mgr module enable mirroring\n# ceph orch apply cephfs-mirror node-name\n</code></pre></p> <p>For each CephFS peer, you must create a user on the target cluster: <pre><code># ceph fs authorize fs-name client_ / rwps\n</code></pre></p> <p>Enable mirroring on the source cluster: <pre><code># ceph fs snapshot mirror enable fs-name\n</code></pre></p> <p>Prepare the target peer: <pre><code># ceph fs snapshot mirror peer_bootstrap create fs-name peer-name site-name\n</code></pre></p> <p>Import the bootstrap token on the source cluster: <pre><code># ceph fs snapshot mirror peer_bootstrap import fs-name boostrap-token\n</code></pre></p> <p>Configure a directory for snapshot mirroring on the source clsuter: <pre><code>ceph fs snapshot mirror add fs-name path\n</code></pre></p>"},{"location":"Ceph/CephFS/#file-layout","title":"File Layout","text":"Attribute Name Description ceph.[file|dir].layout.pool The pool where Ceph stores the file's or directory's data objects ceph.[file|dir].layout.stripe_unit The size in bytes of a block of data that is used in RAID 0 distribution of a file/directory ceph.[file|dir].layout.stripe_count The number of consecutive stripe units that constitute a RAID 0 stripe ceph.[file|dir].layout.object_size File or directory data is split into RADOS objects of this size in bytes; 4MiB by default ceph.[file|dir].layout.pool_namespace The namespace in the pool that is used; if exists <p>Use the <code>getfattr</code> and <code>setfattr</code> commands to retrieve and set the attributes.  Keeping in mind, a file's attributes cannot be changed once the file is written.  A file's attributes are inherited by the parent.</p> <p>To change the pool from the replicated pool to an erasure coded pool, create the new pool, update allow_ec_overwrites to true, add it to the cephfs, and then set the layout on a new directory.  </p> <pre><code># ceph fs add_data_pool mycephfs cephfs_data_ec\n# setfattr -n ceph.dir.layout.pool -v cephfs_data_ec /mnt/mycephfs/mynewdirectory\n</code></pre>"},{"location":"Ceph/CephInstallation/","title":"Installation Instructions v5.x","text":""},{"location":"Ceph/CephInstallation/#introduction","title":"Introduction","text":""},{"location":"Ceph/CephInstallation/#goals","title":"Goals:","text":"<ul> <li>Enable field on Ceph v5.0</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to Ceph Product management and engineering teams at IBM</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with Ceph</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Ceph/CephInstallation/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Ceph 32G 8 <ul><li>1x pxe</li><li>1x ceph-frontend</li><li>1x ceph-backend</li> <ul><li>64GB</li><li>100GB</li>"},{"location":"Ceph/CephInstallation/#building-your-kni-lab","title":"Building Your KNI Lab","text":""},{"location":"Ceph/CephInstallation/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy Ceph Storage Cluster.  </p> </li> <li> <p>Update the project_name and project_password parameters. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p></p> </li> <li> <p>Wait the deployment to finish which can take up to ~10-15 minutes.  </p> </li> </ol>"},{"location":"Ceph/CephInstallation/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  Update the project_name and project_password along with the networks to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output. </p> <pre><code>external_network: vlan1117\nnetworks:  \n  - { name: \"ceph-frontend\", cidr: \"10.20.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n\u00a0\u00a0- { name: \"ceph-backend\", cidr: \"10.20.1.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 ceph nodes.  Update the project_name and project_password along with the instances to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output.  </p> <pre><code>instances:  \n  - { name: \"ceph1\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n\u00a0\u00a0- { name: \"ceph2\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n  - { name: \"ceph3\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n</code></pre> </li> </ol>"},{"location":"Ceph/CephInstallation/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Ceph/CephInstallation/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0 </p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  Take note of the IP addresses for the VLAN1117 network.  You will use the 172.20.17.X addresses to access the servers.  </p> </li> </ol> <p></p> <ol> <li> <p>Start each instance; In the Actions colume, select Start Instance for each node in the cluster.  </p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0  </p> </li> </ol> <p>To access your instance, ssh as the cloud-user using the VLAN IP addresses.  Make sure you are connected to the NA-SSA VPN.</p>"},{"location":"Ceph/CephInstallation/#ceph-v5-installation","title":"Ceph v5. Installation","text":"<p>The full Red Hat documentation for the Ceph installation is available here.  The below precedures are for the OpenInfra Lab environment and have been scaled down to only include the required steps.  </p>"},{"location":"Ceph/CephInstallation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Red Hat Enterprise Linux 8.4 EUS or later.  </li> <li>Ansible 2.9 or later.  </li> <li>Valid Red Hat subsription with the appropriate entitlements.  </li> <li>Root-level access to all nodes.  </li> <li>An active Red Hat Network or service account to access the Red Hat Registry.  </li> </ul> <p>NOTE: Ensure that you are connected to the NA-SSA VPN</p> <ol> <li> <p>Login to ceph1.  Update the /etc/hosts files with the IP and names.</p> <p>NOTE: Your IP address will be different.  </p> <pre><code>ssh cloud-user@172.20.17.117\n$ vi /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n172.20.17.40     ceph1 \n172.20.17.120    ceph2 \n172.20.17.191    ceph3 \n\n10.20.1.190      ceph1-stg\n10.20.1.125      ceph2-stg\n10.20.1.245      ceph3-stg\n</code></pre> </li> <li> <p>Grab the repo from the DNS Utility server</p> <pre><code>$ sudo curl http://172.20.129.19/hextupleo-repo/rhel9.repo -o /etc/yum.repos.d/rhel9.repo\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1379  100  1379    0     0   269k      0 --:--:-- --:--:-- --:--:--  448k\n[cloud-user@ceph1 ~]$ cat /etc/yum.repos.d/rhel8.repo\n[ansible-2.9-for-rhel-8-x86_64-rpms]\nname=ansible-2.9-for-rhel-8-x86_64-rpms\nbaseurl=http://172.20.129.19/repos/ansible-2.9-for-rhel-9-x86_64-rpms/\nenabled=1\ngpgcheck=0\n...\n[rhel-8-for-x86_64-highavailability-rpms]\nname=rhel-8-for-x86_64-highavailability-rpms\nbaseurl=http://172.20.129.19/repos/rhel-9-for-x86_64-highavailability-rpms/\nenabled=1\ngpgcheck=0\n</code></pre> </li> <li> <p>Update all packages using dnf on all servers.</p> <pre><code>$ cat /etc/redhat-release \nRed Hat Enterprise Linux release 8.7 (Ootpa)\n$ uname -a\nLinux ceph1 4.18.0-425.3.1.el8.x86_64 #1 SMP Fri Sep 30 11:45:06 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n$ sudo dnf update -y\n...\nUpgraded:\n  NetworkManager-1:1.40.0-5.el8_7.x86_64                          NetworkManager-libnm-1:1.40.0-5.el8_7.x86_64                 NetworkManager-team-1:1.40.0-5.el8_7.x86_64                                      \n  NetworkManager-tui-1:1.40.0-5.el8_7.x86_64                      authselect-1.2.5-2.el8_7.x86_64                              authselect-compat-1.2.5-2.el8_7.x86_64                                           \n  authselect-libs-1.2.5-2.el8_7.x86_64                            curl-7.61.1-25.el8_7.1.x86_64                                dbus-1:1.12.8-23.el8_7.1.x86_64                                                  \n  dbus-common-1:1.12.8-23.el8_7.1.noarch                          dbus-daemon-1:1.12.8-23.el8_7.1.x86_64                       dbus-libs-1:1.12.8-23.el8_7.1.x86_64                                             \n...\nInstalled:\n  bubblewrap-0.4.0-1.el8.x86_64               fwupd-1.7.8-1.el8.x86_64                  grub2-tools-efi-1:2.02-142.el8_7.1.x86_64 kernel-4.18.0-425.10.1.el8_7.x86_64   kernel-core-4.18.0-425.10.1.el8_7.x86_64\n  kernel-modules-4.18.0-425.10.1.el8_7.x86_64 libatasmart-0.19-14.el8.x86_64            libblockdev-2.24-11.el8.x86_64            libblockdev-crypto-2.24-11.el8.x86_64 libblockdev-fs-2.24-11.el8.x86_64       \n  libblockdev-loop-2.24-11.el8.x86_64         libblockdev-mdraid-2.24-11.el8.x86_64     libblockdev-part-2.24-11.el8.x86_64       libblockdev-swap-2.24-11.el8.x86_64   libblockdev-utils-2.24-11.el8.x86_64    \n  libbytesize-1.4-3.el8.x86_64                libgcab1-1.1-1.el8.x86_64                 libgudev-232-4.el8.x86_64                 libgusb-0.3.0-1.el8.x86_64            libsmbios-2.4.1-2.el8.x86_64            \n...\nComplete!\n$ sudo reboot\nConnection to 172.20.17.117 closed by remote host.\nConnection to 172.20.17.117 closed.\n</code></pre> <p>NOTE: Don't forget to do all servers in the cluster.  </p> </li> <li> <p>Generate the ssh key files for the root user on ceph1.  Update the authorized_keys file on all nodes and append the contents of the id_rsa.pub file.  </p> </li> <li> <p>Install the cephadm-ansible package on ceph1 (or the first node in the cluster).  </p> <pre><code>$ sudo dnf install -y cephadm-ansible\n...\nInstalled:\n  ansible-2.9.27-1.el8ae.noarch                    cephadm-ansible-1.8.0-1.el8cp.noarch                    python3-jmespath-0.9.0-11.el8.noarch                    sshpass-1.09-4.el8.x86_64                   \n\nComplete!\n</code></pre> </li> <li> <p>Create the inventory hosts and registry-login.json files on ceph1.  Change the permissions on the registry-login.json file.</p> <pre><code>$ cd /usr/share/cephadm-ansible \n$ vi hosts\nceph1\nceph2\nceph3\n\n[admin]\nceph1\n$ sudo mkdir /root/ceph\n$ sudo vi /root/ceph/registry.json\n{\n \"url\":\"registry.redhat.io\",\n \"username\":\"myuser1\",\n \"password\":\"mypassword1\"\n}\n$ sudo chmod 600 registry.json     \n</code></pre> <p>NOTE: The user name is the user name that you use to login to registry.redhat.io.  This is used to download the ceph containers.</p> </li> </ol>"},{"location":"Ceph/CephInstallation/#installation","title":"Installation","text":"<ol> <li> <p>Run the Ceph ansible preflight playbook.  </p> <pre><code># sudo -i\n# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars \"ceph_origin=\"\n</code></pre> </li> <li> <p>Create the bootstrap configuration file on ceph1 (or first node in the cluster).</p> <pre><code>service_type: host\naddr: ceph1\nhostname: ceph1\n---\nservice_type: host\naddr: ceph2\nhostname: ceph2\n---\nservice_type: host\naddr: ceph3\nhostname: ceph3\n---\nservice_type: mon\nplacement:\n  host_pattern: \"ceph[1-3]\"\n---\nservice_type: osd\nservice_id: initial_osds\nplacement:\n  host_pattern: \"ceph[1-3]\"\ndata_devices:\n  paths:\n   - /dev/vdb\n</code></pre> </li> <li> <p>Run the cephadm bootstrap command.  </p> <pre><code># cephadm bootstrap --mon-ip 172.20.17.40 --apply-spec /root/ceph/initial-cluster-config.yaml --initial-dashboard-password changeme --dashboard-password-noupdate --registry-json /root/ceph/registry-login.json --cluster-network 10.20.1.0/24\n</code></pre> </li> <li> <p>Once the bootstrap is complete, check the status of the cluster with the <code>ceph status</code> command.  </p> </li> <li> <p>If firewalld is enabled, ensure the following ports are opened on all nodes that run the <code>MON</code> and/or <code>OSD</code> service:  </p> <p>MON:</p> <pre><code># firewall-cmd --zone-public --add-port=6789/tcp\n# firewall-cmd --zone-public --add-port=6789/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-public --add-service=ceph-mon\n# firewall-cmd --zone-public --add-service=ceph-mon --permanent\n</code></pre> <p>OSD:</p> <pre><code># firewall-cmd --zone-public --add-port=6800-7300/tcp\n# firewall-cmd --zone-public --add-port=6800-7300/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-[public|cluster] --add-service=ceph\n# firewall-cmd --zone-[public|cluster] --add-service=ceph --permanent\n</code></pre> </li> <li> <p>Ensure the MTU size is set to 9000 on the network interfaces.</p> <pre><code># nmcli conn modify 'eth0' 802-3-ethernet.mtu 9000\n# nmcli conn down 'eth0'\n# nmcli conn up 'eth0'\n# ip link show 'eth0\n</code></pre> </li> <li> <p>Set the labels for the servers.</p> </li> </ol> <pre><code># ceph orch host ls\nHOST           ADDR         LABELS                  STATUS  \ncephstorage01  172.20.0.11  _admin mon mgr grafana          \ncephstorage02  172.20.0.12  mon mgr rgw _admin              \ncephstorage03  172.20.0.13  mon mgr rgw                     \ncephstorage04  172.20.0.14  mon mgr rgw                     \ncephstorage05  172.20.0.15                                  \ncephstorage06  172.20.0.16                                  \n6 hosts in cluster\n\n# ceph orch host label add cephstorage01 mgr\n</code></pre> <p>Note: Labels can be mon, mgr, rgw, admin, or whatever you choose.</p>"},{"location":"Ceph/CephInstallation/#appendix","title":"Appendix","text":""},{"location":"Ceph/CephInstallation/#ceph-dashboard","title":"Ceph Dashboard","text":"<pre><code>ceph dashboard get-prometheus-api-host\n</code></pre> <pre><code>ceph dashboard get-alertmanager-api-host\n</code></pre> <pre><code>ceph dashboard get-grafana-api-url\n</code></pre>"},{"location":"Ceph/CephInstallation/#export-service-specification","title":"Export Service Specification","text":"<pre><code>ceph orch ls --service_type type --service_name name --export\n</code></pre>"},{"location":"Ceph/CephInstallation/#create-floating-ip-address","title":"Create floating IP address","text":"<pre><code>[stack@bgp-undercloud ~] openstack floating ip create --subnet 372459e8-25f9-4885-b71a-6889ffff02bf --project ceph-blm 18743df0-57aa-4571-9d62-439e0570b059\n</code></pre>"},{"location":"Ceph/CephInstallation/#ceph-health","title":"Ceph Health","text":"<pre><code>ceph health detail\n</code></pre> <pre><code>ceph healthcheck history ls\n</code></pre>"},{"location":"Ceph/CephLogging/","title":"Logging","text":"<p>For container-based deployments, by default, Ceph daemons logs to stderr and logs are captured by the container runtime environment.  These logs are sent to journald and are accessible through the <code>journalctl</code> command.  </p> <p>Ceph logging levels operate on a scale of 1 (terse) to 20 (verbose).  Use a single value for the log level and memory level to set them both to the same value or use different values for output log level and memory level separating the values with a forward slash (i.e. debug_mon = 1/5 sets log level to 1 and memory level to 5).  </p> <p>Get list of Ceph daemons, view logs:  <pre><code># systemctl list-units \"ceph*\"\n# journalctl -eu ceph-dd6eede2-e3a6-11ed-9d9d-fa163ee4fccc@mgr.ceph1.exaerd.service\n</code></pre></p> <p>To enable logging to files instead of journald, set <code>log_to_file</code> to true.  The will create the Ceph log files in /var/log/ceph/fsid.  </p> <p>By default, Cephadm sets up log rotation on each host to rotate these files. You can configure the logging retention schedule by modifying /etc/logrotate.d/ceph.CLUSTER_FSID.</p> <pre><code># ceph config set global log_to_file true \n# ceph config set log_to_stderr false\n</code></pre> <p>NOTE: Make sure to set log_to_stderr false to avoid double logging.</p>"},{"location":"Ceph/CephLogging/#running-configuration-vs-configuration-database-vs-configuration-file","title":"Running Configuration vs. Configuration Database vs. Configuration File","text":"<p>Ceph manages the configuration database of options which centralizes management by storing options in key value pairs.  But there are a handful of Ceph options that can be defined in the local Ceph configuration file, <code>/etc/ceph/ceph.conf</code>.  These few config options control how other Ceph components connect to the monitors to authenticate and fetch the configuration information from the database.  </p> <p>When the same option exists in the config database and the config file, the config database option has a lower priority; the config file takes precedence.  </p> <p>To get the current running configuration use the <code>ceph config show</code> command.  </p> <pre><code># ceph config show mon.osp-blm-controller-1  \n</code></pre>"},{"location":"Ceph/CephLogging/#configuration-checks","title":"Configuration Checks","text":"<p>Cephadm periodically scans each of the hosts in the storage cluster, to understand the state of the OS, disks, and NICs . These facts are analyzed for consistency across the hosts in the storage cluster to identify any configuration anomalies. The configuration checks are an optional feature.  </p> Config Check Description CEPHADM_CHECK_KERNEL_LSM Check to ensure all hosts are running SELINUX CEPHADM_CHECK_SUBSCRIPTION Verifies valid subscriptions CEPHADM_CHECK_PUBLIC_MEMBERSHIP At least one NIC is configured on public network CEPHADM_CHECK_MTU Ensure all OSD nodes are running the same MTU size CEPHADM_CHECK_LINKSPEED Ensure all nodes are running the same link speeds CEPHADM_CHECK_NETWORK_MISSING Ensure all nodes are running configured for the same subnets CEPHADM_CHECK_CEPH_RELEASE Ensure all nodes are running the same Ceph version CEPHADM_CHECK_KERNEL_VERSION Ensure the OS kernel version is consistent across the nodes <pre><code># ceph config set mgr mgr/cephadm/config_checks_enabled\n\n# ceph cephadm config-check status\n# ceph cephadm config-check ls\n</code></pre>"},{"location":"Ceph/CephLogging/#monitor-cephadm-log-messages","title":"Monitor cephadm Log Messages","text":"<pre><code>ceph config set mgr mgr/cephadm/log_to_cluster_level [debug|info]\nceph -W cephadm --watch-debug\nceph -W cephadm --verbose\n</code></pre>"},{"location":"Ceph/CephLogging/#logging-to-stderr","title":"Logging to <code>stderr</code>","text":"<pre><code>journalctl -u ceph-&lt;fsid&gt;@&lt;hostName&gt;\n</code></pre>"},{"location":"Ceph/CephLogging/#view-boot-log","title":"View boot log","text":"<pre><code>journalctl -b \n</code></pre>"},{"location":"Ceph/CephLogging/#data-location","title":"Data Location","text":"<p>Cephadm daemon data and logs are located in:</p> <ul> <li>/var/log/ceph/ - storage cluster logs (usually not present when logging to stderr) <li>/var/lib/ceph/ - cluster daemon data besides logs <li>/var/lib/ceph// - data for specific daemon <li>/var/lib/ceph//crash - crash reports for the storage cluster <li>/var/lib/ceph//removed - old daemon data that have been removed by cephadm"},{"location":"Ceph/CephNetworking/","title":"Networking","text":""},{"location":"Ceph/CephNetworking/#baremetal-lab-network-bonds","title":"Baremetal Lab Network Bonds","text":"<pre><code>nmcli conn add type bond con-name bond1 ifname bond1 bond.options \"mode=802.3ad,miimon=100\"\nnmcli conn add type ethernet slave-type bond con-name bond-ens4f0 ifname ens4f0 master bond1\nnmcli conn add type ethernet slave-type bond con-name bond-ens4f1 ifname ens4f1 master bond1\nnmcli conn add type vlan con-name vlan1100 dev bond1 id 1100 ip4 172.20.0.11/24 gw4 172.20.0.1 \nnmcli conn add type vlan con-name vlan1101 dev bond1 id 1101 ip4 172.20.1.21/24 \nnmcli conn del ens4f0\nnmcli conn del ens4f1\nnmcli conn mod bond1 ipv4.method disabled\nnmcli conn mod bond1 ipv6.method disabled\nnmcli conn mod bond1 802-3-ethernet.mtu 9000\nnmcli conn mod vlan1100 ipv4.dns 172.20.129.10\nnmcli conn mod vlan1101 ipv4.dns 172.20.129.10\nnmcli conn mod bond1 bond.options 'mode=4,lacp_rate=fast,updelay=1000,miimon=100,xmit_hash_policy=layer3+4'\nnmcli conn reload\nnmcli networking off &amp;&amp; nmcli networking on\n</code></pre>"},{"location":"Ceph/CephNetworking/#routes","title":"Routes","text":"<p>Add a route to network via gateway on device interface <pre><code>ip route add 10.20.0.0/24 via 10.40.0.1 dev eth1\n</code></pre></p> <p>Using NMCLI: <pre><code>nmcli conn modify eth1 _ipv4.routes \"10.20.0.0/24 10.40.0.1\"\n</code></pre></p>"},{"location":"Ceph/CephOSD/","title":"OSDs","text":""},{"location":"Ceph/CephOSD/#cpu-requirements","title":"CPU Requirements","text":"<p>NVMe - 10 cores per OSD Non-NVMe SSD - 2 cores per OSD HDD - .5 core per OSD  </p> <p>HDD: sockets * cores * GHz * 100 = estimated 4k randrw performance SSD: sockets * cores * GHz * 1500 = estimated 4k randrw performance  </p>"},{"location":"Ceph/CephOSD/#drive-recommendations","title":"Drive Recommendations","text":"<p>SSD Interface - NVMe SSD Technology - TLC  DWPD - 3   </p> <p>DWPD - Drive Writes per Day: how many times an SSD's usable capacity can be overwritten in 24-hours w/in the specified lifetime. TBW - TeraBytes Written: total amount of data written to the SSD over the specified lifetime.  </p> Capability QLC TLC MLC SLC Architecture 4-bits 3-bits 2-bits 1-bit Capacity Highest High Medium Lowest Endurance Low Medium High Highest Cost $ $$ $$$ $$$$ DWPD to TBW Conversion TBW to DWPD Conversion TBW = DWPD * SSD Capacity(TB) * Lifetime(years) * 365 Days DWPD = TBW / (SSD Capacity(TB) * Lifetime(years) * 365 Days)"},{"location":"Ceph/CephOSD/#osd-default-ratios","title":"OSD Default Ratios","text":"Platform nearfull full backfillfull Ceph"},{"location":"Ceph/CephOSD/#osd-flags","title":"OSD Flags","text":"<p>Backfill - adding/removing OSDs to the cluster.  Backfill does a per object comparison.  No blocked IO/writes. Recovery - when OSDs crash/fail and come back online.  Recovery uses the pglogs to determine changed objects. Rebalance - optimize the allocation of PGs.  </p> <p>Minimize the Data Pause during OSD or node loss Differences between backfill and recovery</p>"},{"location":"Ceph/CephOSD/#osd-configuration-and-bluestore","title":"OSD Configuration and Bluestore","text":"<p>Use the .asok socket to pull information from the OSD daemons.  The bluefs stats command will show the </p> <pre><code># pwd\n/var/run/ceph/8484edfa-8c65-11ed-a639-90e2babd60b8\n# ceph --admin-daemon ./ceph-osd.5.asok config show\n# ceph --admin-daemon ./ceph-osd.5.asok bluefs stats\n</code></pre>"},{"location":"Ceph/CephOSD/#count-of-bluestore-devices","title":"Count of Bluestore Devices","text":"<pre><code>ceph osd count-metadata osd_objectstore\n</code></pre>"},{"location":"Ceph/CephOSD/#add-osd-to-the-ceph-cluster","title":"Add OSD to the Ceph Cluster","text":"<pre><code># ceph orch daemon add osd serverName:/dev/vda\nCreated osd(s) 9 on host \u2018serverName\u2019\n</code></pre> <p>Details of OSD: <pre><code># ceph osd find 9\n{\n   \u201cOsd\u201d: 9\n   \u201cAdds\u201d:  {\n        \u201cAddrvec\u201d: [\n\u2026\n}\n</code></pre></p>"},{"location":"Ceph/CephOSD/#remove-osd-from-ceph-cluster","title":"Remove OSD from Ceph Cluster","text":"<p>Mark the OSD 'out'; this may already be the status. Mark the OSD 'down' Remove the OSD Once removed it will be in the DNE (does not exists status) Remove the OSD from the crush map Remove the authentication/authorization keys  </p> <pre><code># ceph osd out osd.XX\n# ceph osd down osd.XX\n# ceph osd rm osd.XX\n# ceph osd crush rm osd.XX\n# ceph auth del osd.XX\n</code></pre>"},{"location":"Ceph/CephOSD/#replace-osd","title":"Replace OSD","text":"<pre><code>ceph orch daemon rm osd.xx --zap --replace\n</code></pre> <p>Use the following command to ensure the device can be reused if necessary: <pre><code>ceph orch device zap --force 'serverName' /dev/sdx\n</code></pre></p> <p>NOTE: sgdisk --zap-all /dev/sdx</p>"},{"location":"Ceph/CephOSD/#updated-osd-device-class","title":"Updated OSD Device Class:","text":"<pre><code>for i in {0..59}\ndo \n    ceph osd crush rm-device-class osd.$i\n    ceph osd crush set-device-class ssd osd.$i\ndone\n</code></pre>"},{"location":"Ceph/CephOSD/#encryption","title":"Encryption","text":"<p>LUKS keys to unencrypt the drives are stored in the MON.  The key to authenticate to the MON is a cephx key. <pre><code>client.osd-lockbox.f7626ae7-a6df-4229-99c5-aaccb1aebf5c\n    key: AQAm2bVjC/V4ABAA+DIzvkQh7MqoSsUreWnmAQ==\n    caps: [mon] allow command \"config-key get\" with key=\"dm-crypt/osd/f7626ae7-a6df-4229-99c5-aaccb1aebf5c/luks\"\n</code></pre></p> <p>Clear LUKS key on OCP Cluster Device</p> <pre><code>$ lsblk\n...\nvdb  252:16   0  250G  0 disk  \n\u2514\u2500ocs-deviceset-default-storage-0-data-0jfpqp-block-dmcrypt\n     253:0    0  250G  0 crypt \n\n$ sudo cryptsetup luksClose --debug --verbose ocs-deviceset-default-storage-0-data-0jfpqp-block-dmcrypt\n</code></pre>"},{"location":"Ceph/CephOSD/#deepscrub","title":"Deepscrub","text":"<pre><code>ceph pg dump pgs_brief | grep deep\ndumped pgs_brief\n4.2d     active+clean+scrubbing+deep           [47,11,58]          47           [47,11,58]              47\n4.27     active+clean+scrubbing+deep           [51,44,48]          51           [51,44,48]              51\n3.5      active+clean+scrubbing+deep           [55,40,19]          55           [55,40,19]              55\n5.1a     active+clean+scrubbing+deep           [46,57,43]          46           [46,57,43]              46\n32.19    active+clean+scrubbing+deep            [41,15,1]          41            [41,15,1]              41\n4.45     active+clean+scrubbing+deep             [18,4,2]          18             [18,4,2]              18\n4.62     active+clean+scrubbing+deep            [38,25,9]          38            [38,25,9]              38\n3.6f     active+clean+scrubbing+deep            [12,33,3]          12            [12,33,3]              12\n</code></pre>"},{"location":"Ceph/CephOSD/#backfill","title":"Backfill","text":"<pre><code>ceph pg dump pgs_brief | grep backfill\n</code></pre>"},{"location":"Ceph/CephOSD/#ceph-pools","title":"Ceph Pools","text":""},{"location":"Ceph/CephOSD/#primary-affinity","title":"Primary Affinity","text":"<p>Manually control the selection of an OSD as the primary for a PG by setting the primary_affinity.  Mitigate issues or bottlenecks by configuring the cluster to avoid using slow disks or controllers for a primary OSD.</p> <p><pre><code># ceph osd primary-affinity &lt;osd-number&gt; &lt;affinity&gt;  \n</code></pre> Where affinity is a real number between 0 and 1.  </p>"},{"location":"Ceph/CephOSD/#create-pool","title":"Create Pool","text":"<p>Replicated Pool: <pre><code># ceph osd pool create &lt;name&gt; &lt;pg_num&gt; &lt;pgp_num&gt; replicated &lt;CRUSH_ruleSet&gt;\n</code></pre></p> <p>Where pg_num is the total configured number of placement groups and pgp_num is the effective number of placement groups.  This should be the same. For the CRUSH_ruleSet the osd_pool_default_crush_replicated_ruleset configuration parameter sets the default values.</p> <p>Erasure Coded Pool: <pre><code># ceph osd pool create &lt;name&gt; &lt;pg_num&gt; &lt;pgp_num&gt; erasure &lt;erasureProfile&gt; &lt;CRUSH_ruleSet&gt;\n</code></pre></p> <p>Where erasureProfile is the name of the profile created with the ceph osd erasure-code-profile set command.  The profile contains the k and m values and the erasure code plug in to use.</p> <ul> <li>plugin - optional; erasure coding algorithm</li> <li>directory - optional; location of plug in library (/usr/lib64/ceph/erasure-code) </li> <li>crush-failure-domain - optional; controls chunk placement, default is hosts can be set to osd or rack</li> <li>crush-device-class - optional; select OSDs backed by devices of this class for the pool where class is hdd, ssd, name</li> <li>crush-root - optional; set the root node of the CRUSH rule set </li> </ul>"},{"location":"Ceph/CephOSD/#auto-scaling","title":"Auto-scaling","text":"<p>The pg_autoscale_mode property is enabled by default and allows Ceph to make recommendations and automatically adjust the pg_num and pgp_num parameters.  This can be changed on a per pool basis:</p> <p>Status: <pre><code># ceph osd pool autoscale-status\n</code></pre></p> <p>Set autoscale on specific pool: <pre><code># ceph osd pool get &lt;name&gt; pg_autoscale_mode [off|on|warn]\n</code></pre></p> <p>Get the default autoscale mode: <pre><code># ceph config get mon osd_pool_default_pg_autoscale_mode\n</code></pre></p>"},{"location":"Ceph/CephOSD/#management","title":"Management","text":"<p>Set replicas for the pool: <pre><code># ceph osd pool set &lt;name&gt; size|min_size &lt;int&gt;\n</code></pre></p> <p>Set application type for the pool: <pre><code># ceph osd pool application enable &lt;poolName&gt; [rbd|rgw|cephfs]\n</code></pre></p> <p>List erasure-code-profiles <pre><code># ceph osd erasure-code-profile ls\n</code></pre></p> <p>Get a profile\u2019s details: <pre><code># ceph osd erasure-code-profile get\n</code></pre></p> <p>Create erasure-code-profile <pre><code># ceph osd erasure-code-profile set &lt;name&gt; k=4 m=2 \n</code></pre></p> <p>Set quotas: <pre><code># ceph osd pool &lt;name&gt; set-quota\n</code></pre></p> <p>Rename pool: <pre><code># ceph osd pool rename &lt;oldName&gt; &lt;newName&gt;\n</code></pre></p> <p>Delete the pool: <pre><code># ceph config get mon mon_allow_pool_delete\n# ceph tell mon.* config set mon_allow_pool_delete true\n# ceph osd pool delete &lt;name&gt; &lt;name&gt; \u2014yes-i-really-really-mean-it\n</code></pre></p> <p>Details of a placement group: <pre><code># ceph pg &lt;pgID&gt; query\n</code></pre></p> <p>To have ceph manage all devices automatically: <pre><code># ceph orch apply osd \u2014all-available-devices\n</code></pre> This creates a systemd service as osd.all-available-devices (ceph orch ls)</p>"},{"location":"Ceph/CephOSD/#osd-memory-target","title":"OSD Memory Target","text":"<p>Calculation for <code>osd_memory_target</code> HCI:  * 1073741824 * 0.2 /  <p>Non-HCI:  * 1073741824 * 0.7 /  <p>General: ( - 30%) /  <p>NOTE:  is total memory in server in GB and OSDs are the total number of OSDs on the host"},{"location":"Ceph/CephOSD/#nvme-firmware-update","title":"NVMe Firmware Update","text":"<p>Grab the ISO file from Samsung Website. Mount the ISO file: <pre><code>mount ./Samsung_SSD_970_EVO_Plus_4B2QEXM7.iso /mnt/iso\n</code></pre></p> <p>Extract the ISO and run the fumagician command: <pre><code>mkdir /tmp/fwupdate; cd /tmp/fwupdate\ngzip -dc /mnt/iso/initrd | cpio -idv --no-absolute-filenames\ncd root/fumagician\n./fumagician\n</code></pre></p>"},{"location":"Ceph/CephOSD/#performance-testing","title":"Performance Testing","text":""},{"location":"Ceph/CephOSD/#fio-utility","title":"FIO Utility","text":"<p>Random Write Testing (no caching): <pre><code>fio --name=rbd-test-direct1 --ioengine=libaio --direct=1 --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre></p> <p>With caching: <pre><code>fio --name=rbd-test-direct0 --ioengine=libaio --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre></p> <p>Using input file:</p> <pre><code>cat rbd.fio\n[global]\nioengine=rbd\nclientname=admin\npool=rbd\nrbdname=myimage\nrw=randwrite\nbs=4k\n[rbd_iodepth32]\niodepth=32\n\nfio ./rbd.fio\n</code></pre>"},{"location":"Ceph/CephOSD/#rados-bench-tool","title":"Rados Bench Tool","text":"<p>Random Write Testing (leave data) using 16 concurrent threads of 4194304 bytes for 10 seconds: <pre><code>rados bench -p rbd 10 write --no-cleanup\n</code></pre></p>"},{"location":"Ceph/CephOSD/#dd-utility","title":"DD Utility","text":"<p>Write 1 Gib of data 4 times (no caching): <pre><code>dd if=/dev/zero of=here bs=1G count=4 oflag=direct\n</code></pre></p> <p>Wipe an msdos label from device: <pre><code># dd if=/dev/zero of=/dev/vdb bs=512 count=1\n</code></pre></p>"},{"location":"Ceph/CephOSD/#crimson-osd-tp-in-ceph-v7","title":"Crimson OSD - TP in Ceph v7","text":"<p> Crimson</p>"},{"location":"Ceph/CephRGW/","title":"RADOS Gateway (RGW)","text":""},{"location":"Ceph/CephRGW/#introduction-information","title":"Introduction / Information","text":"<p>Pool created for RGW:</p> Pool Name Description .rgw.root Stores information records default.rgw.control Control pool default.rgw.meta User keys and critical metadata default.rgw.log Logs of all buckets and object actions default.rgw.buckets.index Indexes of buckets default.rgw.buckets.data Bucket data default.rgw.bucket.non-ec MPU uploads"},{"location":"Ceph/CephRGW/#service-specification-file","title":"Service Specification File","text":"<pre><code># vi rgw-service.yaml\nservice_type: rgw\nservice_id: myrealm.myzone\nservice_name: rgw.myrealm.myzone\nplacement:\n  count: 4\n  hosts:\n  - serverd.lab.example.com\n  - servere.lab.example.com\nspec:\n  rgw_frontend_port: 8080\n  rgw_realm: realm_name\n  rgw_zone: zone_name\n  ssl: true\n  rgw_frontend_ssl_certificate: |\n    -----BEGIN PRIVATE KEY-----\n    ...\n    -----END PRIVATE KEY-----\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n  networks:\n    - 172.20.17.0/24    \n</code></pre> <p>Deploy the service: <pre><code># ceph orch apply -i rgw-service.yaml\nScheduled rgw.myrealm.myzone update...\n</code></pre></p>"},{"location":"Ceph/CephRGW/#multisite-deployment","title":"Multisite Deployment","text":""},{"location":"Ceph/CephRGW/#configuration","title":"Configuration","text":"<p>Zone: backed by its own RHCS cluster with one or more RGWs associated with it. Zone Group: set of one or more zones.  Data replicated to all zones in the zone group with one zone being the primary zone. Realm: Represents the global namespace; contains one or more zone groups; one zone group is the primary zone group.  </p> <p>Configure the primary zone:</p> <ul> <li>Create a realm  </li> <li>Create a primary zone group  </li> <li>Create primary zone  </li> <li>Create system user  </li> <li>Commit the changes  </li> <li>Create RGW service for the primary zone  </li> <li>Udpate the configuration database  </li> </ul> <pre><code># radosgw-admin realm create --default --rgw-realm=gold\n# radosgw-admin zonegroup create --rgw-zonegroup=us --master --default --endpoints=http://cephstorage01:80\n# radosgw-admin zone create --rgw-zone=datacenter01 --master --rgw-zonegroup=us --endpoints=http://cephstorage01:80 --access-key=blah --secret=blahblah --default\n# radosgw-admin user create --uid=sysadm --display-name=\"SysAdmin\" --access-key=blah --secret=unknown --system\n# radosgw-admin period update --commit\n# ceph orch apply rgw gold-service --realm=gold --zone=datacenter01 --placement=\"1 cephstorage01\"\n# ceph config set client.rgw rgw_zone datacenter01\n</code></pre> <p>Configure the secondary zone:  </p> <ul> <li>Pull the realm configuration  </li> <li>Pull the period  </li> <li>Create a secondary zone  </li> <li>Commit the changes  </li> <li>Create RGW service for the secondary zone  </li> <li>Update the configuration database  </li> </ul> <pre><code># radosgw-admin realm pull --rgw-realm=gold --url=http://cephstorage01:80 --access-key=blah --secret=unknown --default\n# radosgw-admin period pull --url=http://cephstorage01:8000 --access-key=blah --secret=unknown \n# radosgw-admin zone create --rgw-zone=datacenter02 --master --rgw-zonegroup=us --endpoints=http://cephstg01:80 --access-key=blah --secret=blahblah --default\n# radosgw-admin period update --commit\n# ceph orch apply rgw gold-service --realm=gold --zone=datacenter02 --placement=\"1 cephstg01\"\n# ceph config set client.rgw rgw_zone datacenter02\n</code></pre>"},{"location":"Ceph/CephRGW/#failover","title":"Failover","text":"<p>During a primary site failure, the secondary site continues to serve read and write operations but new buckets and users cannot be created unless a secondary site is promoted as a replacement.</p> <p>To promote a secondary zone, modify the zone and zone group, commit the period. <pre><code># radosgw-admin zone modify --master --rgw-zone=datacenter02\n# radosgw-admin zonegroup modify --rgw-zonegroup=us --endpoints=http://cephstg01:80\n# radosgw-admin period update --commit\n</code></pre></p>"},{"location":"Ceph/CephRGW/#s3-api","title":"S3 API","text":""},{"location":"Ceph/CephRGW/#user-authentication","title":"User Authentication","text":"<p>RADOS Gateway supports only a subset of the Amazon S3 API policy language applied to buckets. No policy support is available for users, groups, or roles. Bucket policies are managed through standard S3 operations rather than using the radosgw-admin command.</p> <p>Create an RGW User: <pre><code>radosgw-admin user create --uid=s3user --display-name=\"User Name\" --email=user@example.com [--access-key=blah] [--secret=unknown]  \n</code></pre></p> <p>NOTE: If the access key and secret are not specified, the system will generate them.</p> <p>Regenerate only the secret key of an existing user: <pre><code># radosgw-admin key create --uid=s3user --access-key=\"8PI2D9ARWNGJI99K8TOS\" --gen-secret\n</code></pre></p> <p>Generate additional access key for an existing user: <pre><code># radosgw-admin key create --uid=s3user --gen-access-key\n</code></pre></p> <p>Remove an access key from a user: <pre><code># radosgw-admin key rm --uid=s3user --access-key=\"8PI2D9ARWNGJI99K8TOS\"\n</code></pre></p> <p>Disable or enable an S3 user: <pre><code># radosgw-admin user [suspend|enable] --uid=s3user \n</code></pre></p> <p>Modify user's access level: <pre><code># radosgw-admin user modify --uid=s3user --access=[read|write|readwrite|full]\n</code></pre></p> <p>NOTE: The <code>full</code> access includes readwrite and the access control management capability.</p>"},{"location":"Ceph/CephRGW/#quotas","title":"Quotas","text":"<p>Set quota on number of objects; scope of user: <pre><code># radosgw-admin quota set --quota-scope=user --uid=s3user --max-objects=1000\n# radosgw-admin quota enable --quota-scope=user --uid=s3user\n</code></pre></p> <p>Set quota on size for loghistory; scope of bucket: <pre><code># radosgw-admin quota set --quota-scope=bucket --uid=loghistory --max-objects=1024B\n# radosgw-admin quota enable --quota-scope=bucket --uid=loghistory\n</code></pre></p> <p>Global quotas: <pre><code># radosgw-admin global quota set --quota-scope=bucket --max-objects=2500\n# radosgw-admin global quota enable --quota-scope=bucket \n</code></pre></p> <p>NOTE: Use the radosgw-admin period update --commit command to commit the changes or alternatively restart the RGW instances to pick up the change.  </p> <p>Retrieve user info and stats: <pre><code># radosgw-admin user info --uid=s3user\n# radosgw-admin user stats --uid=s3user \n</code></pre></p> <p>Usage statistics of a user at a specific time: <pre><code># radosgw-admin usage show --uid=s3user --start-date=start --end-date=end\n</code></pre></p>"},{"location":"Ceph/CephRGW/#url-access-method","title":"URL Access Method","text":"<p>Default access method is http://server.example.com/bucket.  To enable wildcard URLs using the http://bucket.server.example.com method, set the <code>rgw_dns_name</code> parameter.  </p> <pre><code># ceph config set client.rgw rgw_dns_name dns_suffix\n</code></pre> <p>NOTE: Where dns_suffix is the FQDN to be used.  </p>"},{"location":"Ceph/CephRGW/#bucket-metadata","title":"Bucket Metadata","text":"<p>View the metadata of a bucket: <pre><code># radosgw-admin metadata get bucket:testbucket\n{\n    \"key\": \"bucket:testbucket\",\n    \"ver\": {\n        \"tag\": \"_hLrerQczwHjyMJIyj-TAdNU\",\n        \"ver\": 1\n    },\n    \"mtime\": \"2023-04-20T18:40:27.109229Z\",\n    \"data\": {\n        \"bucket\": {\n            \"name\": \"testbucket\",\n            \"marker\": \"e85a3c82-f275-441f-acc1-e117e6b23cb9.28819.1\",\n            \"bucket_id\": \"e85a3c82-f275-441f-acc1-e117e6b23cb9.28819.1\",\n            \"tenant\": \"\",\n            \"explicit_placement\": {\n                \"data_pool\": \"\",\n                \"data_extra_pool\": \"\",\n                \"index_pool\": \"\"\n            }\n        },\n        \"owner\": \"operator\",\n        \"creation_time\": \"2023-04-20T18:40:23.269355Z\",\n        \"linked\": \"true\",\n        \"has_bucket_info\": \"false\"\n    }\n}\n</code></pre></p>"},{"location":"Ceph/CephRGW/#swift-api","title":"Swift API","text":"<p>Install the swift client: <pre><code># sudo pip3 install --upgrade python-swiftclient\n</code></pre></p> <p>Create a subuser: <pre><code># radosgw-admin subuser create --uid=s3user --subuser=username:swift --access=[read|write|readwrite|full]\n</code></pre></p> <p>NOTE: --uid specifies an existing account.e   </p> <p>Create swift authentication key: <pre><code># radosgw-admin key create --subuser=username:swift --key-type=swift --secret=unknown\n</code></pre></p> <p>Remove swift subuser key:  <pre><code># radosgw-admin key rm --subuser=username:swift \n</code></pre></p> <p>Modify swift user access: <pre><code># radosgw-admin subuser modify --subuser=username:swift --access=[read|write|readwrite|full]\n</code></pre></p> <p>Remove swift user access: <pre><code># radosgw-admin subuser rm --subuser=username:swift [--purge-data] [--purge-keys]\n</code></pre></p> <p>Creating users within tenants: <pre><code># radosgw-admin user create --tenant devtenant --uid=devuser --display-name=\"A Swift User\" --subuser=devuser:swift --key-type swift --secret=unknown\n</code></pre></p> <p>NOTE: Any further reference to the subuser, must include the tenant (i.e. devtenant$devuser:swift)</p> <p>Get info/List containers <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown stat [container]\n# swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown list\n</code></pre></p> <p>Create a container: <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown post mycontainer\n</code></pre></p> <p>Upload an object to a container: <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown upload mycontainer /tmp/10B.bin\n</code></pre></p>"},{"location":"Ceph/CephRHELUpgrade/","title":"RHEL9 Upgrade","text":"<p>Get a list of OSDs on the server to remove: <pre><code>ceph osd tree | grep -A12 storage05 | grep ssd | awk '{print $1}'\n</code></pre></p> <p>Remove the OSDs <pre><code>ceph orch osd rm 0 7 15 21 27 34 39 44 50 55 --zap -replace\n</code></pre></p> <p>Check the services running; move/redeploy them if necessary: <pre><code>ceph orch apply mds cephfs --placement=\"2 cephstorage06 cephstorage04\"\n</code></pre></p> <p>Relabel any hosts: <pre><code>ceph orch host label add cephstorage06 mds\n</code></pre></p> <p>Drain the services from the host: <pre><code>ceph orch host drain cephstorage05\n</code></pre></p> <p>Check status of the OSD removals: <pre><code>ceph orch osd rm status\n</code></pre></p> <p>Remove the host from the cluster: <pre><code>ceph orch host rm cephstorage05\n</code></pre></p> <p>After the OS is reloaded, copy the files from /etc/ceph on cephstorage01 to the rebuilt server: <pre><code>scp podman-auth.json cephstorage05:/etc/ceph/\n</code></pre></p> <p>On the rebuilt server: <pre><code>subscription-manager register\n</code></pre></p> <pre><code>subscription-manager repos --disable=*\nsubscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms\nsubscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms\n</code></pre> <pre><code>subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms\n</code></pre> <p>Run the ansible playbook to install pre-requisites: <pre><code>ansible-playbook -i hosts cephadm-preflight-rhel9.yml --limit cephstorage05\n</code></pre></p> <p>Add the host back to the cluster: <pre><code>ceph orch host add cephstorage05 172.20.0.15\n</code></pre></p> <p>Redeploy the OSDs: <pre><code>ceph orch apply -i colocated_osds.yaml\n</code></pre></p> <p>Correct the device class on the OSDs: <pre><code>for i in 0 7 15 21 27 34 39 44 50 55; do ceph osd crush rm-device-class osd.$i; ceph osd crush set-device-class ssd osd.$i; done\n</code></pre></p>"},{"location":"Ceph/CephRados/","title":"RADOS Block Devices","text":""},{"location":"Ceph/CephRados/#introduction","title":"Introduction","text":"<p>To manage the RBD images, use the <code>rbd</code> command to create, list, retrieve info, resize, and remvoe block devices in the Ceph Storage platform.</p> <p>The rbd_default_pool parameter specifies the name of the default pool used to store RBD images.  Use the command <code>ceph config get osd rbd_default_pool</code> to get the default pool name.</p> <p>Create an erasure coded RBD pool on SSDs and initialize the pool for RBD images: <pre><code># ceph osd erasure-code-profile set ecrbd k=4 m=2 crush-device-class=ssd\n# ceph osd pool create customrbd 32 32 ecrbd\n# ceph osd pool application enable ecrbd rbd\n# rbd pool init\n</code></pre></p> <p>Create an RBD image: <pre><code># rbd create test_pool/image1 --size=128M\n</code></pre></p>"},{"location":"Ceph/CephRados/#accessing-the-rbd-image-via-kernel-client-krbd","title":"Accessing the RBD image via Kernel Client (krbd)","text":"<p>Use the rbd map/unmap commands to map the RBD devices to /dev/rbdx devices <pre><code># rbd map rbd/imagename\n# rbd unmap /dev/rbd0\n</code></pre></p> <p>Show mapped devices: <pre><code># rbd showmapped\n</code></pre></p> <p>Setting up Automount: <pre><code># vi /etc/ceph/rbdmap\n# RbdDevice     Parameters\n#poolname/imagename id=client,keyring=/etc/ceph/ceph.client.keyring\ntest_pool/image1    id=testpool,keyring=/etc/ceph/ceph.client.testpool.keyring\n:wq\n# vi /etc/fstab\nUUID=e4f488bd-5775-4c5e-b376-eee627699f2b   /   xfs defaults    0   0\nUUID=7B77-95E7  /boot/efi   vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt  0   2\n/dev/rbd/testpool/image1    /mnt/image1    xfs   noauto 0 0 \n:wq\n# rbdmap map\n# rbd showmapped\nid  pool       namespace  image   snap  device   \n0   test_pool             image1  -     /dev/rbd0\n# systemctl enable rbdmap\n</code></pre></p>"},{"location":"Ceph/CephRados/#snapshots-and-cloning","title":"Snapshots and Cloning","text":"Name Description layering To enable cloning striping v2 support for enhanced performance exclusive lock Required when using RBD Mirroring object-map Object map support (requires exclusive lock) fast-diff Requires object map and exclusive lock deep-flatten Flattens all snapshots of the image journaling Required when using RBD Mirroring data-pool EC data pool support <p>NOTE: Must use fsfreeze --freeze to halt all I/O to the image before taking a snapshot.  Use the --unfreeze parameter to resume filesystem operations.</p> <p>NOTE: Deleting an RBD image fails if a snapshot exists.  Use <code>rbd snap purge</code> to delete the snapshots.</p>"},{"location":"Ceph/CephRados/#snapshots","title":"Snapshots","text":"<p>Snapshots are read-only copies of an RBD image created at a specific point in time.</p> <p>Creating a snapshot: <pre><code># rbd snap create pool/image@snapname\n# rbd snap ls pool/image\n</code></pre></p> <p>Rollback to a previous snapshot: <pre><code># rbd snap rollback pool/image@snapname\n</code></pre></p> <p>Delete a snapshot: <pre><code># rbd snap rm pool/image@snapname\n</code></pre></p> <p>Get snapshot disk usage: <pre><code># rbd disk-usage pool/image\nNAME           PROVISIONED  USED  \nimage1@mysnap        5 GiB  36 MiB\nimage1               5 GiB     0 B\n&lt;TOTAL&gt;              5 GiB  36 MiB\n</code></pre></p>"},{"location":"Ceph/CephRados/#cloning","title":"Cloning","text":"<p>Clones are read/write copies of an RBD image that use a protected RBD snapshot as a base.  Clones can be flatten, or converted to an independent image from the source.</p> <p>Create the clone, protect the snap from deletion, create the clone from the protected snapshot: <pre><code># rbd snap create pool/image@snapname\n# rbd snap protect pool/image@snapname\n# rbd clone pool/image@snapname pool/cloneimage\n</code></pre></p> <p>To set the COR option one the specific image or globally: <pre><code># ceph config set client rbd_clone_copy_on_read true\n# ceph config set global rbd_clone_copy_on_read true\n</code></pre></p> <p>Clone management commands: <pre><code># rbd children pool/image@snapname\n# rbd clone pool/image@snapname pool/cloneimage\n# rbd flatten pool/cloneimage\n</code></pre></p>"},{"location":"Ceph/CephRados/#importing-and-exporting-images","title":"Importing and Exporting Images","text":"<p>The <code>rbd export</code> command allows you to export an RBD image (or snapshot) to a file.  Use the <code>rbd import</code> command to import the file to another RBD image.  </p> <p>Export: <pre><code># rbd export pool/image /tmp/img-exp.dat\n</code></pre></p> <p>NOTE: Use the <code>--export-format 1|2</code> option to convert earlier format 1 images to the newer format 2.</p> <p>Differential Export: <pre><code># rbd export-diff pool/image /tmp/exp-diff.dat\n</code></pre></p> <p>Import: <pre><code># rbd import /tmp/img-exp.dat pool/image\n</code></pre></p> <p>NOTE: use the <code>export-format 1|2</code> option to specify the data format of the data to be imported.  Use the <code>--image-format 1|2</code> option  to specify the data format to import as along with the --stripe-count, --object-size, and --image-feature options.  </p> <p>Differential Import: <pre><code># rbd import-diff /tmp/img-exp.dat pool/image\n</code></pre></p>"},{"location":"Ceph/CephRados/#take-out-the-trash","title":"Take out the trash","text":"<p>Use the <code>rbd trash mv</code> command to move an image from a pool to the trash.  Then delete the image from the trash using the <code>rbd trash rm</code> command.</p>"},{"location":"Ceph/CephRados/#rbd-mirrors","title":"RBD Mirrors","text":"<p>RBD Mirroring supports both active/passive and active/active along with two modes, pool mode and image mode.  </p> <ul> <li>Pool mode - automatically enables mirroring for each RBD image created in a mirrored pool.  Ceph creates the image on the remote cluster.  </li> <li>Image mode - selectively enabled for individual RBD images.  </li> </ul> <p>Data is asynchronously mirrored using either journal-based or snapshot based mirroring.  </p> <ul> <li>Journal-based - to ensure point-in-time and crash consistent replication.  Data is written to the associated journal before the actual image.  The remote cluster reads the journal and replays the updates to its local copy of the image.  </li> <li>Snapshot-based - uses scheduled or manually created snapshots to replicate crash-consistent images.  The remote cluster determines any data or metadata updates between two mirror snapshots and copies the deltas to the image's local copy.  The RBD fast-diff image feature enables quick determination of updated data blocks without having to scan the entire image. <p>NOTE: The complete delta between two snapshots must be synced prior to use during a failover.  Any partial sync will be rolled back the moment of failover.</p> </li> </ul>"},{"location":"Ceph/CephRados/#configuration","title":"Configuration","text":"<p>To configure mirroring, ensure the pool is defined on both the local and remote cluster. <pre><code># rbd mirror pool peer bootstrap create --site-name primary poolname &gt; /tmp/primary.token\n</code></pre></p> <p>NOTE: Copy the /tmp/primary.token to the secondary cluster.  </p> <p>On secondary site: <pre><code># ceph orch apply rbd-mirror --placement=nodename\n# rbd mirror pool peer bootstrap import --site-name secondary --direction rx-only test_pool /tmp/bootstrap.token\n</code></pre></p> <p>Enable pool mirroring: <pre><code># rbd mirror pool enable pool [pool|image]\n</code></pre></p> <p>Enable image mirroring: <pre><code># rbd mirror image enable pool/image\n</code></pre></p> <p>Snapshot-based mirroring: <pre><code># rbd mirror image enable pool/image snapshot\n</code></pre></p> <p>NOTE: Journal mirror must not be enabled; use <code>rbd mirror image disable pool/image</code> if necessary.</p>"},{"location":"Ceph/CephRados/#failover-procedure","title":"Failover Procedure","text":"<p>On the primary site: <pre><code># rbd mirror image demote pool/image\n</code></pre> On the secondary site: <pre><code># rbd mirror image promote pool/image\n</code></pre></p> <p>NOTE: Use the --force option during the promotion if you cannot demote the primary site.  </p>"},{"location":"Ceph/CephSizing/","title":"Sizing Guidelines","text":""},{"location":"Ceph/CephSizing/#minimum-hardware-architecture","title":"Minimum Hardware Architecture","text":"<ul> <li>3 Nodes (SSDs) or 4 Nodes (HDDs)</li> <li>RGW can be virtualized</li> <li>CephFS can be added by adding 2 nodes</li> </ul> <p>Support Configurations</p>"},{"location":"Ceph/CephSizing/#individual-ceph-components","title":"Individual Ceph Components","text":"Service Cores RAM Notes MON 2 8 GiB Min of 3; Max of 5 MGR 2 8 GiB 1 MGR colocated with MON RGW 2 8 GiB Min 2 per Object Zone; 1/Failure Domain MDS 2 16 GiB Min 2; 1/Failure Domain; add 2 nodes/total of 5 NVMe OSD 6 5 GiB/OSD 8 GiB recommend; 2 OSD/NVMe SSD OSD 2 5 GiB/OSD 8 GiB recommended; 1 OSD/SSD HDD OSD 1 5 GiB/OSD 1 OSD/HDD Grafana 2 8 GiB Prometheus 2 8 GiB <p>NOTE: Cores are physical cores of a current IA64 CPU model &gt; 2 GHz</p>"},{"location":"Ceph/CephSizing/#iops-optimized","title":"IOPS Optimized","text":"<p>NVMe:</p> Component Description CPU 6 cores per NVMe drive (2 OSDs per drive) RAM 16 GiB baseline plus 8 GiB/OSD Disks Mixed use or write intensive NVMe SSDs (DWPD &gt;=3) Network 25 GbE per 4 NVMe drives <p>SSD: </p> Component Description CPU 2 cores/SSD drive RAM 16 GiB baseline plus 8 GiB/OSD Disks Mixed use or write intensive NVMe SSDs (DWPD &gt;=3) Network 25 GbE per 8 SSD drives"},{"location":"Ceph/CephSizing/#throughput-optimized","title":"Throughput Optimized","text":"Component Description CPU 2 cores/HDD drive RAM 16 GiB baseline plus 5 GiB/OSD Disks Block 7200 RPM; rocksDB SSD/NVMe 4% of HDD size; 4-6:1 SAS/SATA SSD or 12-18:1 for NVMe Network 10 GbE per 12 HDD drives"},{"location":"Ceph/CephSizing/#cost-and-capacity-optimized","title":"Cost and Capacity Optimized","text":"Component Description CPU 1 cores/HDD drive RAM 16 GiB baseline plus 5 GiB/OSD Disks 7200 RPM Enterprise HDDs Network 10 GbE per 12 SSD drives"},{"location":"Ceph/CephTuning/","title":"Performance Tuning","text":"<ul> <li>IOPS - HDDs are in the range of 50-200; SSDs are thousands to hundreds of thousands; NVMe are some hundreds of thousands. </li> <li> <p>Throughput - HDDs around 150Mb/s; SSDs are ~500Mb/s; NVMe are ~2,000Mb/s.  </p> </li> <li> <p>Tune the BlueStore back end used by OSDs</p> </li> <li>Adjust the schedule for automatic data scrubbing and deep scrubbing</li> <li>Adjust the schedule of asynchronous snapshot trimming (snapshot cleanup)</li> <li>Control rate of backfill and recovery operations with OSD failures/additions</li> </ul>"},{"location":"Ceph/CephTuning/#blog-links","title":"Blog Links","text":"<p>CPU Scaling BlueStore (Default vs. Tuned) Performance Comparison Ceph Block Storage Performance on All-Flash Cluster with BlueStore backend RHCS Bluestore performance Scalability ( 3 vs 5 nodes ) RHCS 3.2 Bluestore Advanced Performance Investigation </p>"},{"location":"Ceph/CephTuning/#recovery-and-backfill","title":"Recovery and Backfill","text":"<ul> <li> <p>Recovery occurs when OSD becomes inaccessible and then back online.  OSD has to recover the latest copy of the data.  Default is to allow 3 simultaneous recovery operations for HDDs and 10 for SSDs.</p> </li> <li> <p>Backfill occurs with new OSDs and when and OSD dies and Ceph reassigns its PGs to other OSDs.  Default is to allow 1 PG backfill to/from an OSD at a time.</p> </li> </ul> Parameter Definition osd_recovery_op_priority range of 1-63; default is 3 osd_recovery_max_active concurrent recovery operations per OSD in parallel osd_recovery_threads Number of threads for data recovery osd_max_backfills concurrent backfill operations per OSD osd_backfill_scan_min Minimum number of objects for backfill scan osd_backfill_scan_max Maximum number of objects for backfill scan osd_backfill_full_ratio Threshold for backfill requests to an OSd osd_rbackfill_retry_interval Seconds to wait before retrying backfill requests"},{"location":"Ceph/CephTuning/#iops-optimized","title":"IOPS optimized","text":"<p>Workloads on block devices.  Typical deployments require high-performance SAS drives for storage and journals place on SSDs or NVMe.  </p> <ul> <li>Use two OSDs per NVMe device.  </li> <li>NVMe drives have data, the block database, and WAL collocated on the same storage device.  </li> <li>Assuming a 2 GHz CPU, use 10 cores per NVMe or 2 cores per SSD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 2 OSDs.  </li> </ul>"},{"location":"Ceph/CephTuning/#throughput-optimized","title":"Throughput optimized","text":"<p>Workloads on RGW.  </p> <p>NOTE: Workloads on RGW are often throughput-intensive and are backed on HDDs.  However, the bucket index pool is typically I/O-intensive so store it on SSD/NVMe.  One index/bucket that is stored in one RADOS object.  When a bucket stores more than 100K objects, the index performance degrades.  Use sharding for buckets by setting the <code>rgw_override_bucket_index_max_shards</code> parameter.  Recommended value is number of objects/100,000.  As the index grows, Ceph must reshard the bucket.  Enable automatic resharding with <code>rgw_dynamic_resharding</code>.  </p> <ul> <li>Use one OSD per HDD.  </li> <li>Place the block database and WAL on SSDs or NVMes.  </li> <li>Use at least 7,200 RPM HDD drives.  </li> <li>Assuming a 2 GHz CPU, use one-half core per HDD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 12 OSDs.</li> </ul>"},{"location":"Ceph/CephTuning/#capacity-optimized","title":"Capacity optimized","text":"<p>Workloads that require a large amount of data as inexpensively as possible; usually trading performance for price using SATA drives.</p> <ul> <li>Use one OSD per HDD.  </li> <li>HDDs have data, the block database, and WAL collocated on the same storage device.  </li> <li>Use at least 7,200 RPM HDD drives.  </li> <li>Assuming a 2 GHz CPU, use one-half core per HDD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 12 OSDs.  </li> </ul>"},{"location":"Ceph/CephTuning/#performance-stress-test","title":"Performance Stress Test","text":""},{"location":"Ceph/CephTuning/#rados-bench-command","title":"RADOS Bench Command","text":"<pre><code># rados -p &lt;mypool&gt; bench &lt;seconds&gt; --io-type [write|seq|rand] -b &lt;objsize&gt; -t concurrency [--no-cleanup]\n</code></pre> <p>Defaults:        --io-size  4096 bytes        --io-threads  16        --io-total  1 GB        --io-pattern   seq</p>"},{"location":"Ceph/CephTuning/#fragmentation","title":"Fragmentation","text":"<p>Check fragmentation on the OSDs:</p> <pre><code># ceph tell &lt;osd.ID&gt; bluestore allocator score block\n</code></pre> <p>Value 0 to .7 is considered acceptable. Value .7 to .9 is considered safe fragmentation. Value .9 and above indicates severe fragmentation that is causing performance issues.  </p>"},{"location":"Ceph/CephTuning/#scrubbing","title":"Scrubbing","text":"<p>You can control scrubbing globally or at the pool level.  To control at the pool level use the <code>ceph osd pool set poolName parameterValue</code> command.</p> Parameter Description noscrub If set to true, Ceph doesn't light scrub the pool; default is false nodeep-scrub If set to true, Ceph doesn't deep scrub the pool; default is false scrub_min_interval Wait a minimum number of seconds between scrubs; default is 0 (uses global parameter) scrub_max_interval Do not wait more than this number of seconds before performing a scrub; default is 0 deep_scrub_interval Interval for deep scrubbing; default is 0"},{"location":"Ceph/CephTuning/#light-scrubbing","title":"Light Scrubbing","text":"<p>Verifes an objects presence, checksum, and size.</p> Parameter Description osd_scrub_end_hour = end_hour Specifies the time to stop scrubbing; 0 - 23 osd_scrub_load_threshold Perform scrub is system load is below the threshold; default is .5 osd_scrub_min_interval Wait a minimum number of seconds between scrubs; default is 86,400 osd_scrub_internval_randomize_ratio Add a random dealy to the value defined in the osd_scrub_min_interval parameter; default is .5 osd_scrub_max_interval Do not wait more than this number of seconds before performing a scrub regardless of load; default 604,800 osd_scrub_priority Set the priority for scrub operations; default is 5; relative to osd_client_op_priority (default 64) <pre><code>ceph pg scrub &lt;pg-id&gt;\n</code></pre>"},{"location":"Ceph/CephTuning/#deep-scrubbing","title":"Deep Scrubbing","text":"<p>Reads the data and recalculates and verifies the objects checksum.</p> <p>You can enable/disable deep scrubbing at the cluster level by using the <code>ceph osd set nodeep-scrub</code> and <code>ceph osd unset nodeep-scrub</code> commands.</p> Parameter Description osd_deep_scrub_interval Interval for deep scrubbing; default is 604,800 osd_scrub_sleep Introduces a pause between deep scrub disk reads.  Increase the value to slow down scrub operations; default is 0 <pre><code>ceph pg deep-scrub &lt;pg-id&gt;\n</code></pre>"},{"location":"Ceph/CephTuning/#snapshot-trimming","title":"Snapshot Trimming","text":"<p>Ceph schedules the removal of the snapshot data as an asynchronous operation when a snapshot is removed. To reduce the impact, configure a pause after the deletion with the osd_snap_trim_sleep parameter.  This adds a delay before allowing the next snapshot trim operation. Default is 0.</p> <p>Control the snapshot trimming process using the osd_snap_trim_priority parameter.  Default 5</p>"},{"location":"Ceph/CephTuning/#logging","title":"Logging","text":"<p>Ceph logging levels are on a scale of 1 (terse) to 20 (verbose).  A single value can be used for both log level and memory level or separate them with a slash for different values (i.e. 1/5 where 1 is for log level and 5 is for memory log level).</p> <p>Get the logging config for daemons: <pre><code>ceph --admin-daemon ./ceph-osd.11.asok config show | grep debug\n</code></pre></p> <p>NOTE: The asok files are found in /var/run/ceph/fsid.</p>"},{"location":"Ceph/Cephv6installation/","title":"Installation Instructions v6.x","text":""},{"location":"Ceph/Cephv6installation/#introduction","title":"Introduction","text":""},{"location":"Ceph/Cephv6installation/#goals","title":"Goals:","text":"<ul> <li>Enable field on Ceph v6.0</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to Ceph Product management and engineering teams at IBM</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with Ceph</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Ceph/Cephv6installation/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Ceph 32G 8 <ul><li>1x pxe</li><li>1x ceph-frontend</li><li>1x ceph-backend</li> <ul><li>64GB</li><li>100GB</li>"},{"location":"Ceph/Cephv6installation/#building-your-kni-lab","title":"Building Your KNI Lab","text":""},{"location":"Ceph/Cephv6installation/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy Ceph Storage Cluster.  </p> </li> <li> <p>Update the project_name and project_password parameters. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p></p> </li> <li> <p>Wait the deployment to finish which can take up to ~10-15 minutes.  </p> </li> </ol>"},{"location":"Ceph/Cephv6installation/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  Update the project_name and project_password along with the networks to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output. </p> <pre><code>external_network: vlan1117\nnetworks:  \n  - { name: \"ceph-frontend\", cidr: \"10.20.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n\u00a0\u00a0- { name: \"ceph-backend\", cidr: \"10.20.1.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 ceph nodes.  Update the project_name and project_password along with the instances to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output.  </p> <pre><code>instances:  \n  - { name: \"ceph1\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n\u00a0\u00a0- { name: \"ceph2\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n  - { name: \"ceph3\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n</code></pre> </li> </ol>"},{"location":"Ceph/Cephv6installation/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Ceph/Cephv6installation/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0 </p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  Take note of the IP addresses for the VLAN1117 network.  You will use the 172.20.17.X addresses to access the servers.  </p> <p> </p> </li> <li> <p>Start each instance; In the Actions colume, select Start Instance for each node in the cluster.  </p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0  </p> </li> </ol> <p>To access your instance, ssh as the cloud-user using the VLAN IP addresses.  Make sure you are connected to the NA-SSA VPN.</p>"},{"location":"Ceph/Cephv6installation/#ceph-v6-installation","title":"Ceph v6. Installation","text":"<p>The full Red Hat documentation for the Ceph installation is available here.  The below precedures are for the OpenInfra Lab environment and have been scaled down to only include the required steps.  </p>"},{"location":"Ceph/Cephv6installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Red Hat Enterprise Linux 9.2 EUS or later.  </li> <li>Ansible 2.9 or later.  </li> <li>Valid Red Hat subsription with the appropriate entitlements.  </li> <li>Root-level access to all nodes.  </li> <li>An active Red Hat Network or service account to access the Red Hat Registry.  </li> </ul> <p>NOTE: Ensure that you are connected to the NA-SSA VPN</p> <ol> <li> <p>Login to ceph01.  Update the /etc/hosts files with the IP and names.</p> <p>NOTE: Your IP address will be different.  </p> <pre><code>ssh cloud-user@ceph01\n$ vi /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n172.20.17.40     ceph01 \n172.20.17.120    ceph02 \n172.20.17.191    ceph03 \n\n10.40.0.190      ceph01-stg\n10.40.0.125      ceph02-stg\n10.40.0.245      ceph03-stg\n</code></pre> </li> <li> <p>Grab the rhel9 repository file from the DNS Utility server</p> <pre><code>$ sudo curl http://172.20.129.10/hextupleo-repo/rhel9.repo -o /etc/yum.repos.d/rhel8.repo\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1379  100  1379    0     0   269k      0 --:--:-- --:--:-- --:--:--  448k\n[cloud-user@ceph1 ~]$ cat /etc/yum.repos.d/rhel9.repo\n[rhel-9-for-x86_64-highavailability-rpms]\nname=rhel-9-for-x86_64-highavailability-rpms\nbaseurl=http://172.20.129.13/repos/rhel-9-for-x86_64-highavailability-rpms/\nenabled=1\ngpgcheck=0\n...\n[rhceph-6-tools-for-rhel-9-x86_64-rpms]\nname=rhceph-6-tools-for-rhel-9-x86_64-rpms\nbaseurl=http://172.20.129.13/repos/rhceph-6-tools-for-rhel-9-x86_64-rpms/\nenabled=1\ngpgcheck=0\n</code></pre> </li> <li> <p>Update all packages using dnf on all servers.</p> <pre><code>$ cat /etc/redhat-release \nRed Hat Enterprise Linux release 9.1 (Plow)\n$ sudo dnf update -y\n...\nInstalled:\nansible-collection-ansible-posix-1.2.0-1.3.el9ost.noarch                  ansible-collection-community-general-4.0.0-1.1.el9ost.noarch                 \nansible-core-2.14.2-5.el9_2.x86_64                                        cephadm-ansible-2.15.0-1.el9cp.noarch                                        \ngit-core-2.39.3-1.el9_2.x86_64                                            libnsl2-2.0.0-1.el9.x86_64                                                   \nmpdecimal-2.5.1-3.el9.x86_64                                              python3.11-3.11.2-2.el9_2.1.x86_64                                           \npython3.11-cffi-1.15.1-1.el9.x86_64                                       python3.11-cryptography-37.0.2-5.el9.x86_64                                  \npython3.11-libs-3.11.2-2.el9_2.1.x86_64                                   python3.11-pip-wheel-22.3.1-2.el9.noarch                                     \npython3.11-ply-3.11-1.el9.noarch                                          python3.11-pycparser-2.20-1.el9.noarch                                       \npython3.11-pyyaml-6.0-1.el9.x86_64                                        python3.11-setuptools-wheel-65.5.1-2.el9.noarch                              \npython3.11-six-1.16.0-1.el9.noarch                                        sshpass-1.09-4.el9.x86_64                                                    \n\nComplete!    ...\n$ sudo reboot\nConnection to 172.20.17.117 closed by remote host.\nConnection to 172.20.17.117 closed.\n</code></pre> <p>NOTE: Don't forget to do all servers in the cluster.  </p> </li> <li> <p>Generate the ssh key files for the root user on ceph01.  Update the authorized_keys file on all nodes and append the contents of the id_rsa.pub file.  </p> </li> <li> <p>Install the cephadm-ansible package on ceph01 (or the first node in the cluster).  </p> <pre><code>$ sudo dnf install -y cephadm-ansible\n...\nInstalled:\n  ansible-2.9.27-1.el8ae.noarch                    cephadm-ansible-1.8.0-1.el8cp.noarch                    python3-jmespath-0.9.0-11.el8.noarch                    sshpass-1.09-4.el8.x86_64                   \n\nComplete!\n</code></pre> </li> <li> <p>Create the inventory hosts and registry-login.json files on ceph01.  Change the permissions on the registry-login.json file.</p> <pre><code>$ cd /usr/share/cephadm-ansible \n$ vi hosts\nceph1\nceph2\nceph3\n\n[admin]\nceph1\n$ sudo mkdir /root/ceph\n$ sudo vi /root/ceph/registry.json\n{\n \"url\":\"registry.redhat.io\",\n \"username\":\"myuser1\",\n \"password\":\"mypassword1\"\n}\n$ sudo chmod 600 registry.json     \n</code></pre> <p>NOTE: The user name is the user name that you use to login to registry.redhat.io.  This is used to download the ceph containers.</p> </li> </ol>"},{"location":"Ceph/Cephv6installation/#installation","title":"Installation","text":"<ol> <li> <p>Run the Ceph ansible preflight playbook.  </p> <pre><code># sudo -i\n# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars \"ceph_origin=custom\" -e \"custom_repo_url=http://172.20.129.13/repos/rhceph-6-tools-for-rhel-9-x86_64-rpms/\"\n</code></pre> <p>NOTE: Use the custom_repo_url when for a disconnected installation.</p> </li> <li> <p>Create the bootstrap configuration file on ceph01 (or first node in the cluster).</p> <pre><code>service_type: host\naddr: ceph01\nhostname: ceph01\n---\nservice_type: host\naddr: ceph02\nhostname: ceph02\n---\nservice_type: host\naddr: ceph03\nhostname: ceph03\n---\nservice_type: host\naddr: ceph04\nhostname: ceph04\n---\nservice_type: mon\nplacement:\n  host_pattern: \"ceph0[1-3]\"\n---\nservice_type: osd\nservice_id: initial_osds\nplacement:\n  host_pattern: \"ceph0[1-3]\"\ndata_devices:\n  paths:\n   - /dev/vdb\n</code></pre> </li> <li> <p>Run the cephadm bootstrap command.  </p> <pre><code># cephadm bootstrap --mon-ip 10.40.0.193 --apply-spec /root/ceph/initial-config.yaml --initial-dashboard-password changeme  --registry-json /root/ceph/registry-login.json --cluster-network 10.20.1.0/24\n</code></pre> </li> <li> <p>Once the bootstrap is complete, check the status of the cluster with the <code>ceph status</code> command.  </p> </li> <li> <p>If firewalld is enabled, ensure the following ports are opened on all nodes that run the <code>MON</code> and/or <code>OSD</code> service:  </p> <p>MON:</p> <pre><code># firewall-cmd --zone-public --add-port=6789/tcp\n# firewall-cmd --zone-public --add-port=6789/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-public --add-service=ceph-mon\n# firewall-cmd --zone-public --add-service=ceph-mon --permanent\n</code></pre> <p>OSD:</p> <pre><code># firewall-cmd --zone-public --add-port=6800-7300/tcp\n# firewall-cmd --zone-public --add-port=6800-7300/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-[public|cluster] --add-service=ceph\n# firewall-cmd --zone-[public|cluster] --add-service=ceph --permanent\n</code></pre> </li> <li> <p>Ensure the MTU size is set to 9000 on the network interfaces.</p> <pre><code># nmcli conn modify 'eth0' 802-3-ethernet.mtu 9000\n# nmcli conn down 'eth0'\n# nmcli conn up 'eth0'\n# ip link show 'eth0\n</code></pre> </li> <li> <p>Set the labels for the servers.</p> </li> </ol> <pre><code># ceph orch host ls\nHOST           ADDR         LABELS                  STATUS  \ncephstorage01  172.20.0.11  _admin mon mgr grafana          \ncephstorage02  172.20.0.12  mon mgr rgw _admin              \ncephstorage03  172.20.0.13  mon mgr rgw                     \ncephstorage04  172.20.0.14  mon mgr rgw                     \ncephstorage05  172.20.0.15                                  \ncephstorage06  172.20.0.16                                  \n6 hosts in cluster\n\n# ceph orch host label add cephstorage01 mgr\n</code></pre> <p>Note: Labels can be mon, mgr, rgw, admin, or whatever you choose.</p>"},{"location":"Ceph/Cephv6installation/#appendix","title":"Appendix","text":""},{"location":"Ceph/Cephv6installation/#export-service-specification","title":"Export Service Specification","text":"<pre><code>ceph orch ls --service_type type --service_name name --export\n</code></pre>"},{"location":"Ceph/Cephv6installation/#create-floating-ip-address","title":"Create floating IP address","text":"<pre><code>[stack@bgp-undercloud ~] openstack floating ip create --subnet 372459e8-25f9-4885-b71a-6889ffff02bf --project ceph-blm 18743df0-57aa-4571-9d62-439e0570b059\n</code></pre>"},{"location":"Ceph/dataservices/","title":"Data Services SSAs - Tools","text":"<p>Links taken from the Specialist Solution Architect Handbook - 2023</p>"},{"location":"Ceph/dataservices/#resources","title":"Resources","text":"<p>ODF 4.12 Resources ODF Subscription Guide  Red Hat Data Services Central Red Hat Ceph Storage in RHCC Red Hat OpenShift Data Foundation in RHCC </p>"},{"location":"Ceph/dataservices/#demos-and-workshops","title":"Demos and Workshops","text":"<p> Advanced Cluster Management with RHACM  OpenShift 4.12 Workshop  ODF Disaster Recovery Management </p>"},{"location":"Ceph/dataservices/#collaboration-sales-plays","title":"Collaboration &amp; Sales Plays","text":"<p> ODF Channel on Google Chat Mailing List: global-data-services-ssa-all@redhat.com</p>"},{"location":"Ceph/encryption/","title":"Encryption","text":""},{"location":"Ceph/encryption/#encryption-in-transit","title":"Encryption in Transit","text":"<p>The msgr2 protocol supports two connection modes:</p> <ul> <li> <p>crc</p> <ul> <li>Provides strong initial authentication when a connection is established with cephx.</li> <li>Provides a crc32c integrity check to protect against bit flips.</li> <li>Does not provide protection against a malicious man-in-the-middle attack.</li> <li>Does not prevent an eavesdropper from seeing all post-authentication traffic.</li> </ul> </li> <li> <p>secure</p> <ul> <li>Provides strong initial authentication when a connection is established with cephx.</li> <li>Provides full encryption of all post-authentication traffic.</li> <li>Provides a cryptographic integrity check.</li> </ul> </li> </ul> <p>NOTE: The default mode is crc.</p> <p>When installing ODF, the <code>In transit encryption</code> checkbox must be selected on the Security screen to enable this feature.  Documentation indicates it must be enabled during installation.</p>"},{"location":"How%20To/Certificates/","title":"Certificates","text":""},{"location":"How%20To/Certificates/#ca-key-and-certificate-files-how-the-ca-was-created","title":"CA Key and Certificate Files (How the CA was created)","text":"<p>The CA .key and .pem files are in the /etc/httpd/conf/ssl.key directory on the DNS Utility server in the same directory.</p> <pre><code># openssl genrsa -des3 -out openinfraCA.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.......................................................................................................................................+++++\n...........................................................................................+++++\ne is 65537 (0x010001)\nEnter pass phrase for openinfraCA.key: *******\nVerifying - Enter pass phrase for openinfraCA.key: *******\n\n\n# openssl req -x509 -new -nodes -key openinfraCA.key -sha256 -days 1095 -out openinfraCA.pem\nEnter pass phrase for openinfraCA.key:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:openinfra.lab\nEmail Address []:userName@redhat.com\n</code></pre> <p>Copy the new CA .pem file to the anchors directory; run update-ca-trust to add it to the list of trusted CA certificates:</p> <pre><code># cp openinfraCA.pem /usr/share/pki/ca-trust-source/anchors/\n# update-ca-trust\n</code></pre> <p>Use the trust list command to verify the new CA is included in the list of trusted CAs:</p> <pre><code># trust list | grep -B2 -A2 openinfra.lab \npkcs11:id=%91%54%10%D3%0D%E4%AD%A7%08%E7%18%EF%A8%62%F7%BF%59%D6%4D%6E;type=cert\n    type: certificate\n    label: openinfra.lab\n    trust: anchor\n    category: authority\n</code></pre>"},{"location":"How%20To/Certificates/#creating-a-certificate-for-a-serverservice","title":"Creating a Certificate for a Server/Service","text":"<p>There\u2019s two ways to do this, manual and scripted.  Both are provided here.</p>"},{"location":"How%20To/Certificates/#scripted-process","title":"Scripted Process","text":"<p>Login to the Lab DNS server (172.20.129.19). Switch to the root user.   Add an DNS entry for the FQDN of the Server/Service you are generating the SSL certificate for (lab DNS admin interafce is here http://172.20.129.10:5380/) Change directory to /root/ssl-certifcates and run the script.  </p> <pre><code>cd /root/ssl-certificates  \n./create-certificate.sh &lt;FQDN&gt;  \n</code></pre> <p>Example: ./create-certificate jira-sm.openinfra.lab  </p> <p>You will be prompted for the pass phrase to the openinfraCA.key. </p> <p>NOTE: The pass phrase for the CA private key is stored in Bitwarden in entry named \"OpenInfraLab CA Pass Phrase\".</p> <p>All output files will start with the FQDN. Example:  </p> <p>-rw-r--r--. 1 root root 1517 Mar 22 17:16 jira-sm.openinfra.lab.crt -rw-r--r--. 1 root root 1054 Mar 22 17:16 jira-sm.openinfra.lab.csr -rw-r--r--. 1 root root  254 Mar 22 17:16 jira-sm.openinfra.lab.ext -rw-------. 1 root root 1675 Mar 22 17:16 jira-sm.openinfra.lab.key  </p>"},{"location":"How%20To/Certificates/#manual-process","title":"Manual Process","text":"<p>Generate the key file:</p> <pre><code># openssl genrsa -out cephrgw.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.........+++++\n..............................................................+++++\ne is 65537 (0x010001)\n</code></pre> <p>Generate a Certificate Signing Request file:</p> <pre><code># openssl req -new -key cephrgw.key -out cephrgw.csr\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:cephrgw \nEmail Address []:youremail@redhat.com\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:Redhat1!\nAn optional company name []:      \n</code></pre> <p>Create an x509 V3 extension config file to define the Subject Alternate Names (SAN)</p> <pre><code># cat cephrgw.ext\nauthorityKeyIdentifier = keyid,issuer\nbasicConstraints = CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = cephrgw.cephlab.openinfra.lab\nDNS.2 = cephrgw\nDNS.3 = *.cephrgw\nDNS.4 = *.cephrgw.cephlab.openinfra.lab\n</code></pre> <p>Create the signed certificate using the .csr and .ext file along with the openinfraCA.pem and .key files:</p> <pre><code># openssl x509 -req -in cephrgw.csr -CA openinfraCA.pem -CAkey openinfraCA.key -CAcreateserial -out cephrgw.crt -days 365 -sha256 -extfile cephrgw.ext\nSignature ok\nsubject=C = US, ST = North Carolina, L = Raleigh, O = Red Hat, OU = OpenInfrastructure Lab, CN = cephrgw, emailAddress = bmclaren@redhat.com\nGetting CA Private Key\nEnter pass phrase for openinfraCA.key: Redhat1!\n</code></pre> <p>NOTE: The option \u2013CAcreateserial generates the openinfraCA.srl file.  In subsequent calls to create a signed certificate, use the -CAserial parameter with this file (-CAserial openinfraCA.srl).  The file is used to keep track of the unique serial numbers.</p>"},{"location":"How%20To/DNSServer/","title":"DNS Server","text":""},{"location":"How%20To/DNSServer/#overview","title":"Overview","text":"<p>The lab DNS server is a pod on the OCP Baremetal Cluster in the openinfra project running a Technitium DNS Server.  The management login to the server is here:</p> <p>http://172.20.129.10:5380</p> <p>Upon logging in, you will see a Dashboard which will give you a high level overview of the DNS Server usage:</p> <p>The hope is that we can then utilize the data here to then be able to clean up unused DNS entries as projects grow stagnant as it will be easy to determine the last usage of a DNS record.</p>"},{"location":"How%20To/DNSServer/#overview-of-dns-terms","title":"Overview of DNS Terms","text":"<p>This section is going to cover some DNS basics for those who may not be familiar with DNS.</p>"},{"location":"How%20To/DNSServer/#dns-zones","title":"DNS Zones","text":"<p>Zones are how a DNS Server organizes records.  When looking at the zones in the DNS server, you will see them named in one of two ways:</p> <p>Using a Domain Name</p> <p>Examples:</p> <p></p> <p>These are as you would expect, the domain name openinfra.lab contains all DNS entries that look like .openinfra.lab.  The  may or may not contain a dot notation indicating a sub-domain: <ul> <li>www.openinfral.lab</li> <li>servera.openinfra.lab</li> <li>partA.partB.openinfra.lab &lt;- partB is the sub-domain of openinfra.lab</li> </ul> <p>Typically within a DNS server, zones are used to delegate control over who can manage what domains.  For our purposes, this is not needed since our whole team is administrators within the lab.  Therefore, we are opting to use zones as an organization methodology and giving each subdomain its own zone.  This is why you see ocp-bm.openinfra.lab in the list.  The entries in there, such as api.ocp-bm.openinfra.lab could technically be in the openinfra.lab zone, but we opted to keep them separate for logical organization purposes.</p> <p>Using an Inverse Netmask</p> <p>Examples:</p> <p></p> <p>These are used to store reverse lookup records.  The naming convention is the reverse order of the network CIDR, dropping any zeroes in the netmask. For example, the first one listed stores reverse lookup records for IP addresses 172.20.1.0/24.  </p> <p>Record Types</p> <p>Each zone is comprised of different record types. Below is a table with the most common record types at the top (highlighted) and the least common ones at the bottom of the record types you may need to create.</p> Record Type Description SOA There can be only one Start of Authority (SOA) record for a zone. The SOA record contains settings that are required by secondary name servers or recursive resolvers to know the minimum TTL for caching non existent records. This is created for you by default when you create a zone, however you should edit this and set your e-mail address as the point of contact for the zone you create. NS Name Server (NS) records tell other DNS resolvers the domain names of the authoritative DNS Servers that are hosting the current zone. This is created for you by default when you create a zone. PTR Pointer records allow you to map a domain name to an IP address to allow performing reverse lookups i.e. finding domain name associated with an IP address. A PTR record for localhost is created for you by default when you create a reverse lookup zone A Address record allows you to assign an IPv4 address to your domain or sub domain. This is the type of record you will be creating most often. CNAME Canonical name records allow you to point your domain or sub domain to another domain name. This is what is commonly referred to as an alias. AAAA This address record allows you to assign an IPv6 address to your domain or sub domain. FWD The FWD record allows you to specify the forwarder server address to be used in conditional forwarder zones. SRV Service records allow certain applications to discover where a service is hosted. TXT Text records allow specifying any text data. These records are commonly used for domain name verification process and for Sender Policy Framework (SPF) record to prevent your domain being misused for email spam. CAA Certification Authority Authorization (CAA) record allows you to specify which Certificate Authority is allowed to issue SSL/TLS certificates for your domain name to prevent misuse. ANAME This record allows you to have a CNAME like facility at the zone's apex. It can also be used for a sub domain. The DNS Server resolves the provided domain name in the record and returns A or AAAA response as per the QTYPE. MX Mail Exchange (MX) records allow you specify email servers so that you can receive email for your domain. APP The APP record allows you to specify which DNS App should be used to process the request."},{"location":"How%20To/DNSServer/#creating-and-managing-dns-in-the-lab","title":"Creating and Managing DNS in the Lab","text":""},{"location":"How%20To/DNSServer/#standards","title":"Standards","text":"<p>We are trying to standardize on the DNS naming convention utilized in the lab and how the DNS Server is configured. At a high level:</p> <ul> <li>Each domain and/or sub-domain shall have its own Zone associated with it in the DNS server.</li> <li>When creating a new Zone, edit the SOA record and change the Responsible Person entry to be your Red Hat email address.</li> <li>Members wishing to create DNS records for their own projects/demos should utilize their kerberos username as a sub-domain when creating the Zone.  Example: phalmos.openinfra.lab, cjanisze.openinfra.lab.</li> <li>On EVERY record you create in the DNS server, you should add your name to the COMMENT field in the event someone has a question on the entry/device it points to.</li> <li>Only create reverse pointer records for entries which are going to be permanent/long lived.  Nothing is worse than having a DNS entry point to an IP and then having the IP - point to an incorrect DNS entry.</li> </ul>"},{"location":"How%20To/DNSServer/#zones","title":"Zones","text":"<p>The primary DNS Zone in the server is the openinfra.lab zone.</p> <p>When you need to create a new zone, go to the Zones tab and click Add Zone.  You will be prompted to name the zone:</p> <p></p> <p>For the zone name, add the fully qualified DNS entry for the zone, select Primary Zone and click Add. If you ever need to also include reverse lookups, you should never really have to create the Reverse Lookup Zone manually, as you can have the DNS server create that zone for you when you create the A records in your zone. </p> <p>Upon creation, the server will automatically create the NS and SOA records for you:</p> <p></p> <p>Before you create anything in this zone, edit the SOA record and change the Responsible Party to your Red Hat email address:</p> <p></p>"},{"location":"How%20To/DNSServer/#records","title":"Records","text":"<p>To add records to a Zone, click on the Zone name to show the Zone.  Click Add Record button and you will be prompted to create the record:</p> <p></p> <p>For each of the fields:</p> Field Setting Name This is the part of the DNS name which goes before the zone name.  For example, setting this to api will create a DNS entry for api.documentation.openinfra.lab.  Likewise, to create a wildcard entry, you can set this to .apps which will create a wildcard DNS entry of .apps.documentation.openinfra.lab. Type Typically this will be A for ipv4 and AAAA for ipv6.  For a complete list of DNS Record Types see the Record Types table in the previous section of this documentation. TTL You can leave this as is and it will default to 3600 IPv4 Address Obviously enter the IP address here. Comments Enter your name. <p>Check off the <code>Add reverse (PTR) record</code> if you require reverse lookups for your entry.  You may also need to check off <code>Create reverse zone for PTR record</code> if the reverse zone does not exist.  If it does not exist, an error will be thrown that it can\u2019t find the zone and then you will need to check this box off.  I didn\u2019t create this software, so don\u2019t ask me why it does not just create it.</p> <p>Click Save and that is it.</p>"},{"location":"How%20To/DNSServer/#dns-server-creation","title":"DNS Server Creation","text":"<p>This section documents how the DNS Server was created in the environment for historic purposes.</p>"},{"location":"How%20To/DNSServer/#service-account","title":"Service Account","text":"<p>The DNS server needs to be able to listen on privileged ports and therefore needs to execute in a security context that allows this.  To enable this, we create a Service Account called dns-openinfra-sa and add the privileged Security Context Constraints (SCC) to the service account:</p> <pre><code>oc create sa dns-openinfra-sa\noc adm policy add-scc-to-user -z dns-openinfra-sa privileged\n</code></pre> <p>This service account is referenced when we do the deployment of the DNS application.</p>"},{"location":"How%20To/DNSServer/#storage","title":"Storage","text":"<p>We create a Persistent Volume Claim for the application to store the configuration data:</p> <pre><code>cat &lt;&lt;EOF&gt;pvc.yaml\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: dns-pod-technitium-config\n  namespace: openinfra\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n  volumeMode: Filesystem\nEOF\noc apply -f ./pvc.yaml\n</code></pre> <p>This is then used by the deployment of the DNS application.</p>"},{"location":"How%20To/DNSServer/#network","title":"Network","text":"<p>In order to maintain consistency with what we have been using and not have to make major changes to already deployed infrastructure, we needed the DNS server to be attached to VLAN1101 and serve DNS on 172.20.129.10.  To do this, we created a Network Attachment Definition (NAD):</p> <pre><code>cat &lt;&lt;EOF &gt;nad.yaml\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: dns-openinfra-lab-nad\n  namespace: openinfra\nspec:\n  config: \u2018{\n    \"name\": \"dns-openinfra-lab\",\n    \"type\": \"bridge\",\n    \"cniVersion\": \"0.3.1\",\n    \"bridge\": \"br1\", \"vlan\": 1101, \"isGateway\": false, \"ipam\": {  \"type\":\n    \"static\", \"addresses\": [{ \"address\": \"172.20.129.10/24\" }],  \"routes\": [{\n    \"dst\": \"172.20.0.0/16\", \"gw\": \"172.20.129.1\" }] } }\nEOF\noc apply -f ./nad.yaml\n</code></pre> <p>This NAD is referenced when we do the deployment of the DNS application.</p>"},{"location":"How%20To/DNSServer/#application-deployment","title":"Application Deployment","text":"<p>This may or may not be the best way to deploy this application, but it worked for me.  I deployed it manually using the GUI and then extracted the YAML, made changes to it to get it to work.</p> <pre><code>cat &lt;&lt;EOF&gt;dns.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    alpha.image.policy.openshift.io/resolve-names: '*'\n    app.openshift.io/route-disabled: \"false\"\n    deployment.kubernetes.io/revision: \"1\"\n    image.openshift.io/triggers: '[{\"from\":{\"kind\":\"ImageStreamTag\",\"name\":\"dns-openinfra-lab:latest\",\"namespace\":\"openinfra\"},\"fieldPath\":\"spec.template.spec.containers[?(@.name==\\\"dns-openinfra-lab\\\")].image\",\"pause\":\"false\"}]'\n  labels:\n    app: dns-openinfra-lab\n    app.kubernetes.io/component: dns-openinfra-lab\n    app.kubernetes.io/instance: dns-openinfra-lab\n    app.kubernetes.io/name: dns-openinfra-lab\n    app.kubernetes.io/part-of: dns-openinfra-lab-app\n    app.openshift.io/runtime-namespace: openinfra\n  name: dns-openinfra-lab\n  namespace: openinfra\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: dns-openinfra-lab\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        k8s.v1.cni.cncf.io/networks: dns-openinfra-lab-nad\n      labels:\n        app: dns-openinfra-lab\n        deploymentconfig: dns-openinfra-lab\n    spec:\n      serviceAccountName: dns-openinfra-sa\n      containers:\n      - image: docker.io/technitium/dns-server\n        imagePullPolicy: Always\n        name: dns-openinfra-lab\n        securityContext: { \"privileged\": true }\n        ports:\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 443\n          protocol: UDP\n        - containerPort: 53\n          protocol: TCP\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 53443\n          protocol: TCP\n        - containerPort: 5380\n          protocol: TCP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 8053\n          protocol: TCP\n        - containerPort: 853\n          protocol: TCP\n        - containerPort: 853\n          protocol: UDP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /etc/dns\n          name: dns-openinfra-lab-config\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: dns-openinfra-lab-config\n        persistentVolumeClaim:\n          claimName: dns-pod-technitium-config\nEOF\noc apply -f dns.yaml\n</code></pre>"},{"location":"How%20To/DNSServer/#appendix","title":"Appendix","text":""},{"location":"How%20To/DNSServer/#official-documentation-links","title":"Official Documentation Links","text":"<p>User Guide/Help Topics HTTP API Documentation</p>"},{"location":"How%20To/DNSServer/#script-examples","title":"Script Examples","text":"<p>You can use tools such as curl to automate actions with the DNS server.  </p> <p>Creating API Token</p> <p>To use scripts, you will need an API Token.  Login to the DNS Server, click the drop down next to your name in the top title bar and select Create API Token:</p> <p></p> <p>DO NOT DO THIS AS THE ADMIN USER! USE YOUR OWN ACCOUNT TO CREATE THIS TOKEN!</p> <p>You will then be prompted to create the token:</p> <p></p> <p>Enter your login password to the server and enter a name you can use to identify the token.  Click Create.</p> <p>YOU MUST COPY THE TOKEN DISPLAYED. YOU WILL NEVER SEE THIS TOKEN AGAIN. IF YOU DO NOT COPY IT AND STORE IT SOME PLACE SAFE AND FORGET IT, YOU WILL NEED TO CREATE A NEW TOKEN.</p> <p>You can use this token as the TOKEN variable in the example scripts.</p> <p>Adding Records</p> <p>Here\u2019s a short example of how one could easily add records to the DNS server:</p> <p>Pre-requisites:</p> <ul> <li>Create an API Token</li> <li>Create a Zone for the records</li> </ul> <pre><code>#!/bin/bash\n\n# User and server details\nSERVER=\"http://172.20.129.10:5380\"\nDOMAIN=\"imm.openinfra.lab\"\nTOKEN=\"xXxXxXxXxXxXxXxXxXxXxXxXxXxXxX\"\nCREATE_REVERSE=\u201dtrue\u201d\nDNS_ENTRIES=dns_entries.txt\n\ncat &lt;&lt;EOF&gt;${DNS_ENTRIES}\nserver1.example.com 192.168.0.1\nserver2.example.com 192.168.0.2\nserver3.example.com 192.168.0.3\nEOF\n\negrep -v '^#' ${DNS_ENTRIES} | while read LINE\ndo\n  NAME=$(echo ${LINE} | awk '{print $1}')\n  IP=$(echo ${LINE} | awk '{print $2}')\n  FQDN=\"${NAME}.${DOMAIN}\"\n  if [[ \"${CREATE_REVERSE}\" == \"true\" ]]\n  then\n    curl --request POST --data 'package=unlimited' \"${SERVER}/api/zones/records/add?token=${TOKEN}&amp;domain=${FQDN}&amp;type=A&amp;ipAddress=${IP}&amp;ptr=true\"\n  else\n    curl --request POST --data 'package=unlimited' \"${SERVER}/api/zones/records/add?token=${TOKEN}&amp;domain=${FQDN}&amp;type=A&amp;ipAddress=${IP}\u201d\n  fi\ndone\n\nSearching for Inactive DNS Records\nThis script can be used to generate a CSV of inactive A records which have not been accessed since a specific date:\n\n#!/bin/bash\n\n# Return all A records that have no activity since this date\nINACTIVE_DATE=\"2021-01-01T00:00:00\"\n\n# User and server details\nDNS_SERVER=\"172.20.129.10\"\nDNS_SERVER_ADMIN=\"http://${DNS_SERVER}:5380\"\nDNS_TOKEN=\"xXxXxXxXxXxXxXxXxXxXxXxXxXxXxX\"\n\nWORK_DIR=$(mktemp -d -t $(basename $0).XXXXXX)\n\nfor ZONE in $(curl --request POST --data 'package=unlimited' \"${DNS_SERVER_ADMIN}/api/zones/list?token=${DNS_TOKEN}\" 2&gt;/dev/null | jq \".response.zones[].name\" | sed 's/\"//g')\ndo\n  mkdir -p ${WORK_DIR}/zones/${ZONE}\n  curl --request POST --data 'package=unlimited' \"${DNS_SERVER_ADMIN}/api/zones/records/get?token=${DNS_TOKEN}&amp;zone=${ZONE}&amp;domain=${ZONE}&amp;listZone=true\" 2&gt;/dev/null | jq \".response.records[] | select (.type==\\\"A\\\" and .lastUsedOn&lt;\\\"${INACTIVE_DATE}\\\")\" &gt;${WORK_DIR}/zones/${ZONE}/unused_records.txt 2&gt;/dev/null\n  if [[ ! -s ${WORK_DIR}/zones/${ZONE}/unused_records.txt ]]\n  then\n    rm -rf ${WORK_DIR}/zones/${ZONE}\n  fi\ndone\n\ncat /dev/null&gt;dns_unused_entries.csv\necho \"ZONE,FQDN,IP,COMMENT,LAST USED ON\" | tee -a dns_unused_entries.csv\nfor ZONE in $(ls ${WORK_DIR}/zones)\ndo\n  for FQDN in $(cat ${WORK_DIR}/zones/${ZONE}/unused_records.txt | jq '.name' | sed 's/\"//g')\n  do\n    COMMENT=$(cat ${WORK_DIR}/zones/${ZONE}/unused_records.txt | jq \". | select (.name==\\\"${FQDN}\\\")\" | jq .comments | sed 's/\"//g')\n    IP=$(cat ${WORK_DIR}/zones/${ZONE}/unused_records.txt | jq \". | select (.name==\\\"${FQDN}\\\")\" | jq .rData.ipAddress | sed 's/\"//g')\n    LAST_USED_ON=$(cat ${WORK_DIR}/zones/${ZONE}/unused_records.txt | jq \". | select (.name==\\\"${FQDN}\\\")\" | jq .lastUsedOn | sed 's/\"//g')\n    echo \"${ZONE},${FQDN},${IP},${COMMENT},${LAST_USED_ON}\" | tee -a dns_unused_entries.csv\n  done\ndone\nrm -rf ${WORK_DIR}\nexit 0\n</code></pre>"},{"location":"How%20To/DocumentationContribution/","title":"Documentation Contribution","text":"<p>The documentation is hosted in GitHub at https://github.com/redhat-openinfra-lab/openinfra-docs.</p>"},{"location":"How%20To/DocumentationContribution/#pre-requisites","title":"Pre-Requisites","text":"<p>Local client must have python and git.  After installing python, you will need to use pip to install the mkdocs and mkdocs-material packages.  Due to the use of pip we recommend using a Virtual Environment to work on the documentation.  This page will explain how to create/configure this.</p> <p>While it is not required, VS Code makes updating the documents easy and integrates with GitHub nicely.</p>"},{"location":"How%20To/DocumentationContribution/#update-the-site-or-add-documentation","title":"Update the Site or Add Documentation","text":"<p>As the site requires pip to install mkdocs and mkdocs-material, we recommend creating a Virtual Environment to work on the page documentation.  This page will explain how to do this.</p>"},{"location":"How%20To/DocumentationContribution/#configure-virtual-environment","title":"Configure virtual environment","text":"<ol> <li> <p>Install python3 and pip</p> </li> <li> <p>Create the virtual environment</p> </li> </ol> <pre><code>export VENV=${HOME}/virtual_environments/mkdocs\npython3 -m venv ${VENV}\n</code></pre> <ol> <li> <p>Activate the Virtual Environment (You will want to do this step everytime you want to work on the doumentation)</p> </li> <li> <p>Use pip to install mkdocs and mkdocs-material</p> </li> </ol> <pre><code>python3 -m pip install --upgrade mkdocs mkdocs-material\n</code></pre>"},{"location":"How%20To/DocumentationContribution/#clone-github-repository","title":"Clone Github Repository","text":"<p>Clone the site to your local repository</p> <pre><code>git clone git@github.com:redhat-openinfra-lab/openinfra-docs.git\n</code></pre>"},{"location":"How%20To/DocumentationContribution/#working-locally","title":"Working locally","text":"<p>Ensure you are in the openinfra-docs directory that you cloned from github.</p> <p>Checkout a new branch to make your changes. <pre><code>git checkout -b &lt;newBranch&gt;\n</code></pre></p> <p>Use mkdocs to run a local web-server so you can view your changes real-time.  Remember, you will need to activate the virtual environment using Step 3 above before trying to start the mkdocs server. If you are working on the documentation on your desktop, you can simply start the server using:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will start a web-server listening on port 8000 on your loopback device.  If you are using something other than your desktop, such as a Virtual Machine, you will need to open a port on your firewall and utilize the <code>-a</code> switch when starting the mkdocs serve command to specify an IP:PORT to start on.  Example:</p> <pre><code>cd openinfra-docs\nmkdocs serve -a 172.20.135.10:8000\n</code></pre> <p>The site is configured to automatically publish changes to the gh-pages sites when a <code>push</code> to the master branch is executed.  Check the status of the pages build and deployment under the Actions menu.</p>"},{"location":"How%20To/DocumentationContribution/#commiting-changes","title":"Commiting Changes","text":"<p>Once you have your changes made, commit them to your branch, pull the latest main branch from the repo, merge your changes, and then push to the repo.</p> <pre><code>git add &lt;file&gt;\ngit commit -m \"description of changes made\"\ngit checkout main\ngit pull\ngit merge &lt;yourBranch&gt;\ngit push\n</code></pre>"},{"location":"How%20To/ansiblesync/","title":"Ansible Tower","text":"<p>To sync any changes from the cloud-infra-git-gitea.apps.ocp-bm.openinfra.lab hextupleo repo, access AAP, expand Resources, and select Projects.</p> <p>Click on the hextupleo-local-git project followed by the Sync icon.  </p> <p></p>"},{"location":"How%20To/jirachangerequest/","title":"Lab Change Requests","text":"<p>Use these procedures to requests a change in the NA-SSA Lab using Jira Service Management.  </p> <ol> <li> <p>Log in to your Jira Service Management account.  </p> </li> <li> <p>Click on the \u201cCreate\u201d button located in the top of the screen.  </p> <p></p> </li> <li> <p>Select the \u201cChange Request\u201d option from the list of issue types.  </p> </li> <li> <p>Fill in the required fields in the Create Issue form such as Project, Summary, Description, Priority, and other custom fields that may be relevant to your organization's processes.  </p> <p></p> </li> <li> <p>Assign the issue to the appropriate team member or group responsible for the change.  </p> </li> <li> <p>Add any necessary comments or attachments to provide more information or context for the change request.  </p> </li> <li> <p>Save the issue and wait for approval from the appropriate parties.  </p> <ul> <li>If the change request is approved, the approving party will set the status to \"Awaiting Implementation\" to indicate that it is ready for implementation.  The approving party will assign the issue to the team member or group responsible for implementing the change.  </li> <li>If the change request is declined, the approving party will set the status to \"Declined\" and add comments explaining why the request was declined.  </li> <li>If the change request is no longer needed, the approving party will set the status to \"Cancelled\".  </li> <li>Once the change request has been implemented, the implementation team will set the status to \"Resolved\" to indicate that the change has been completed.  </li> </ul> </li> <li> <p>Verify that the change has been successful and meets the requirements of the change request.  </p> </li> <li> <p>Finally, set the status to \"Closed\" to indicate that the change request is completed and closed.</p> </li> </ol> <p>That's it! These steps should help you create a change request using Jira Service Management and manage its lifecycle through various status changes.  Remember to communicate with your team members and stakeholders throughout the process to ensure everyone is aware of the status of the change request.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AffinityRules/","title":"Configure Affinity / Anti-Affinity Rules","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AffinityRules/#official-documentation","title":"Official Documentation","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AffinityRules/#process","title":"Process","text":"<p>For the some applications, a customer may want to ensure that the specific Virtual Machines are never executed on the same underlying host as other Virtual Machines.  To accomplish this, an anti-affinity rule may be created.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AffinityRules/#label-the-virtual-machine-pod","title":"Label the Virtual Machine (Pod)","text":"<p>Each Virtual Machine needed a label applied to the pod.  To do this, edit the Virtual Machine and add a label to it.  In our example here, we will have two different identifiers, AS and MS nodes.  For this rule, we do not want any AS nor MS Virtual Machine to run on the same host as any other AS or MS machine:</p> <pre><code>spec:\n  template:\n    metadata:\n      labels:\n        customer_node_type: AS\n</code></pre> <p>The label won\u2019t appear on the pod until the Virtual Machine is restarted. All associated Virtual Machines should be labelled first.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AffinityRules/#apply-affinity-rules","title":"Apply Affinity Rules","text":"<p>Add the Anti-Affinity rule to the Virtual Machine:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution: \n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: customer_node_type\n                  operator: In\n                  values:\n                  - AS\n                  - MS\n              topologyKey: kubernetes.io/hostname\n</code></pre> <p>Restart the Virtual Machine. This will have the label take effect as well as the Anti-Affinity Rule.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AzureAD/","title":"Integrate with Azure AD","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AzureAD/#official-documentation","title":"Official Documentation","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/AzureAD/#process","title":"Process","text":"<p>On the left go to <code>Administration -&gt; Cluster Settings</code> and go to Configuration Tab and search for Oauth.</p> <p></p> <p>Click on <code>OAuth</code>, scroll to the bottom where it lists Identity Providers, drop down <code>Add</code> and select <code>OpenID Connect</code>:</p> <p></p> <p>Set the name to azure and populate the other fields from the <code>Azure Application Registration</code>.  The last thing you may need to change is the preferred_name mapping.  You may need to get information from the AD Administrator to know what this maps to.  The POC that this was done for had to map this to premisessamaccountname within AD to ensure the user names shown properly in Openshift.</p> <p></p> <p>If you make any changes to the Identity Provider, ensure you delete both the user and the identity in Openshift to ensure that there\u2019s no tokens getting stuck anywhere.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/CPUManager/","title":"Configure CPU Pinning","text":"<p>This is achieved within Openshift using CPU Manager.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/CPUManager/#official-documentation","title":"Official Documentation","text":"<p>OCP Virtualization KB</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/CPUManager/#process","title":"Process","text":"<p>Edit the machine config pool</p> <p><code>oc edit machineconfigpool worker</code></p> <p>Add the custom-kubelet identifier for cpumanager:</p> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  creationTimestamp: \"2023-05-06T13:32:03Z\"\n  generation: 2\n  labels:\n    custom-kubelet: cpumanager-enabled\n</code></pre> <p>Create a Kubelet Configuration for CPU Manager:</p> <pre><code>export KUBECONFIG=~/ocp-trial/kubeconfig\ncat &lt;&lt; EOF &gt; cpumanager-kubeletconfig.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: KubeletConfig\nmetadata:\n  name: cpumanager-enabled\nspec:\n  machineConfigPoolSelector:\n    matchLabels:\n      custom-kubelet: cpumanager-enabled\n  kubeletConfig:\n     cpuManagerPolicy: static\n     cpuManagerReconcilePeriod: 5s\nEOF\noc create -f ./cpumanager-kubeletconfig.yaml\n</code></pre> <p>Once this configuration is rolled out to the nodes, a Virtual Machine can utilize pinned CPU\u2019s by editing the Virtual Machine YAML and adding the following:</p> <pre><code>spec:\n  template:\n    spec:\n      domain:\n        cpu:\n          dedicatedCpuPlacement: true\n          isolateEmulatorThread: true &lt;---- this is optional, and will\n                                             reserve an extra CPU to run\n                                             qemu threads, likely best\n                                             for I/O intensive workloads\n</code></pre> <p>Obviously remove the \u201c&lt;---- comment\u201d. While you can configure <code>isolateEmulatorThreads</code> to <code>true</code>, you will lose the ability to LiveMigrate. This is being tracked in this issue: https://issues.redhat.com/browse/CNV-28774</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/DedicatedLiveMigration/","title":"Configure Dedicated Live Migration Network","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/DedicatedLiveMigration/#official-documentation","title":"Official Documentation","text":"<p>The official documentation basically just links to a description of the <code>Cluster Settings</code> where you can set the Live Migration Network.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/DedicatedLiveMigration/#process","title":"Process","text":"<p>First, you need to create a Network Attachment Definition (NAD) for the Live Migration Network.  Here is an example that creates a network called <code>cnv-migration-network</code> and excludes some IP addresses that already exist:</p> <pre><code>export KUBECONFIG=~/ocp-trial/kubeconfig\ncat &lt;&lt; EOF &gt; create_cnv_migration_nad.yaml\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: cnv-migration-network\n  namespace: openshift-cnv\nspec:\n  config: '{\n    \"cniVersion\": \"0.3.1\",\n    \"type\": \"macvlan\",\n    \"master\": \"bond1\",\n    \"mode\": \"bridge\",\n    \"ipam\": {\n          \"type\": \"whereabouts\",\n          \"range\": \"192.168.31.0/24\",\n          \"exclude\": [\n               \"192.168.31.222/32\",\n               \"192.168.31.223/32\",\n               \"192.168.31.224/32\",\n               \"192.168.31.225/32\",\n               \"192.168.31.226/32\",\n               \"192.168.31.227/32\"\n            ]\n    }\n  }'\nEOF\noc create -f ./create_cnv_migration_nad.yaml\n</code></pre> <p>Next, you need to configure the hyperconverged deployment to use the migration network. If you have not already created the HyperConverged deployment, you can set this in the GUI when you're creating the object:</p> <p></p> <p>If you have already created the HyperConverged object, you will need to edit it and add the <code>network:</code> line identifying the Network Attachment Definition to use for the migration network. as shown below:</p> <p><code>oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</code></p> <p>Example:</p> <pre><code>spec:  \n  certConfig:  \n    ca:  \n  \u2026  \n  liveMigrationConfig:  \n    completionTimeoutPerGiB: 800  \n    network: cnv-migration-network  \n    parallelMigrationsPerCluster: 5  \n    parallelOutboundMigrationsPerNode: 2  \n    progressTimeout: 150  \n</code></pre>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/","title":"Configure Logging","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/#official-documentation","title":"Official Documentation","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/#process","title":"Process","text":"<p>If pushing the logs to an external ElasticSearch instance, you will need the following pre-requisites:</p> <ul> <li>The Certificate CRT file which contains the CA information for the remote Elastic Search instance</li> <li>Username &amp; Password to authenticate to Elastic Search with</li> </ul> <p>If configuring Logging to push logs to an ElasticSearch instance running on the same cluster, these items will be automatically configured by the Logging Operator.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/#install-loggging-operator","title":"Install Loggging Operator","text":"<p>On the left go to <code>Operators -&gt; OperatorHub</code> and search for Openshift Logging Operator:</p> <p></p> <p>Click on <code>Red Hat Openshift Logging</code>, click <code>Install</code>, review the defaults and click <code>Install</code>.</p> <p>Once installed, click <code>View Operator</code> and go to the Cluster Logging tab. Click <code>Create Cluster Logging</code>. Accept the defaults and click <code>Create</code>.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/#configuring-log-forwarding","title":"Configuring Log Forwarding","text":"<p>Using an ElasticSearch Instance on the same Cluster</p> <p>Go to the Cluster Log Forwarder tab. Click <code>Create Cluster Log Forwarder</code>.</p> <p>Switch to the <code>YAML</code> view and make the following changes to:</p> <ul> <li>Remove the pipeline to send the logs to a remote server</li> <li>send the logs to the local (default) ElasticSearch provider</li> </ul> <p><pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  pipelines:\n    - inputRefs:\n        - application\n        - audit\n        - infrastructure\n      outputRefs:\n        - default\n      name: enable-default-log-store\n</code></pre> Click <code>Create</code>.</p> <p>Using an External ElasticSearch</p> <p>Create the ElasticSearch secret for the Remote ElasticSearch Instance:</p> <pre><code>oc create secret generic -n openshift-logging elasticsearch \\\n  --from-file=tls.key=tls.key \\\n  --from-file=tls.crt=tls.crt \\\n  --from-file=ca-bundle.crt=${CA_BUNDLE} \\\n  --from-literal=username=${USERNAME} \\\n  --from-literal=password=${PASSWORD}\n</code></pre> <p>Go to the Cluster Log Forwarder tab. Click <code>Create Cluster Log Forwarder</code>.</p> <p>Switch to the <code>YAML</code> view and make the following changes to:</p> <pre><code>apiVersion: \"logging.openshift.io/v1\"\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  outputDefaults:\n    elasticsearch:\n      structuredTypeKey: kubernetes.labels.logFormat\n      structuredTypeName: nologformat\n  outputs:\n    - name: remote-elasticsearch\n      secret:\n        name: elasticsearch\n      tls:\n        insecureSkipVerify: true\n      type: elasticsearch\n      url: 'https://[REMOTE ES IP]:[REMOTE ES PORT]'\n  pipelines:\n    - inputRefs:\n        - application\n        - audit\n        - infrastructure\n      labels:\n        logtype: poc\n      name: forward-to-remote\n      outputRefs:\n        - remote-elasticsearch\n      parse: json\nEOF\n</code></pre>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/Logging/#accessing-kibana","title":"Accessing Kibana","text":"<p>On the left, navigate to <code>Networking -&gt; Routes</code> and change to the openshift-logging project and you will see a route for kibana.</p> <p></p> <p>Click the Location route to open Kibana, login and you\u2019ll need to create indices for infra-*, audit-* and app-*</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/NMState/","title":"Install NMState Operator","text":"<p>Login to the cluster UI at:</p> <p>https://console-openshift-console.&lt;OCP Cluster Name&gt;.&lt;Domain Name&gt;/</p> <p>On the left go to <code>Operators -&gt; OperatorHub</code></p> <p>Search for nmstate</p> <p></p> <p>Click on the Kubernetes NMState Operator, click <code>Install</code>, accept the defaults and click <code>Install</code> again.</p> <p>In a couple of minutes, it will be installed:</p> <p></p> <p>Click <code>View Operator</code>, go to the NMState tab, click <code>Create NMState</code>.</p> <p>Accept the defaults and click <code>Create</code>.</p> <p></p> <p>Doing this will create the NodeNetwork* Custom Resource Definitions required to be able to work with the underlying network configuration in CoreOS on the Openshift Nodes.</p> <p>Create bond and associated bridge</p> <p>Once the NMState Operator is installed, we can use this to create additional bonds/bridges within the cluster. Some uses for these are:</p> <ul> <li>Virtualization Migration Network</li> <li>Virtualization Data Network</li> <li>Management Network for ODF</li> </ul> <p>Here are some example yaml files which can be used to create tese bonds and bridges.</p> <p>bond4</p> <pre><code>export KUBECONFIG=~/ocp-trial/kubeconfig\ncat &lt;&lt; EOF &gt; ocpv-network.yaml\napiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: ocpv-network\nspec:\n  desiredState:\n    interfaces:\n    - name: bond4\n      type: bond\n      state: up\n      mtu: 9000\n      ipv4:\n        enabled: false\n      link-aggregation:\n        mode: 802.3ad\n        port:\n        - enp55s1f8\n        - enp55s1f9\nEOF\noc create -f ./ocpv-network.yaml\n</code></pre> <p>br-bond4</p> <pre><code>export KUBECONFIG=~/ocp-trial/kubeconfig\ncat &lt;&lt; EOF &gt; ocpv-bridge.yaml\napiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: ocpv-bridge\nspec:\n  desiredState:\n    interfaces:\n    - name: br-bond4\n      description: Linux bridge with bond4 as a port\n      type: linux-bridge\n      state: up\n      mtu: 9000\n      ipv4:\n        enabled: false\n        dhcp: false\n      bridge:\n        options:\n          stp:\n            enabled: false\n        port:\n        - name: bond4\nEOF\noc create -f ./ocpv-bridge.yaml\n</code></pre>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/activedirectory/","title":"Integrate with LDAP/AD","text":""},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/activedirectory/#enable-ldap","title":"Enable LDAP","text":"<p>In the GUI, navigate to Administration, Cluster Settings.  Search or scroll down to Oauth, click the link.  On the OAuth details page, scroll to the bottom and click the Add button, select LDAP.</p> <p>Update the fields appropriately.  </p> <ul> <li>Name field will be used to display the user's idenity on the login screen.</li> <li>URL field must include the domain the and field to search for when the user logs in.</li> <li>BindDN is the DN from AD that is used for lookups</li> <li>Bind password is the password for the BindDN user.</li> </ul> <p></p> <p>Scroll down to the Attributes section.  Update the Preferred username to the user friendly name defined in AD.</p> <p>NOTE:  For our environment this is the sAMAccountName but can vary for other customer environments.</p> <p>Scroll down to the CA file.  If using TSL/ldaps, include the CA Cert used by the AD server.  You can get this using the openssl command.  </p> <pre><code>% openssl s_client -showcerts -connect activedirectory.openinfra.lab:636\nCONNECTED(00000005)\ndepth=0 CN = activedirectory.openinfra.lab\nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 CN = activedirectory.openinfra.lab\nverify error:num=21:unable to verify the first certificate\nverify return:1\ndepth=0 CN = activedirectory.openinfra.lab\nverify return:1\n---\nCertificate chain\n 0 s:CN = activedirectory.openinfra.lab\n   i:DC = lab, DC = openinfra, CN = openinfra-ACTIVEDIRECTORY-CA\n   a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256\n   v:NotBefore: Apr 16 00:29:00 2024 GMT; NotAfter: Apr 16 00:29:00 2025 GMT\n-----BEGIN CERTIFICATE-----\nMIIGgTCCBWmgAwIBAgITUwAAAALFqNGtH01IBQAAAAAAAjANBgkqhkiG9w0BAQsF\n...\nHX4mEB0+2Km5JOexCcM2zhY0+UHBtt/hfP25mSsbfk2UnIkgjw==\n-----END CERTIFICATE-----\n---\nServer certificate\nsubject=CN = activedirectory.openinfra.lab\nissuer=DC = lab, DC = openinfra, CN = openinfra-ACTIVEDIRECTORY-CA\n---\n...\n</code></pre> <p>Click the Add icon.  This will create a Secret called ldap-bind-password-xxxxx and a ConfigMap called  ldap-ca-xxxxx in the openshift-config namespace and the LDAP Identity provider OAuth CRD.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/activedirectory/#ldap-sync","title":"LDAP Sync","text":"<p>Create the ldapsync-config.yaml file <pre><code>kind: LDAPSyncConfig\napiVersion: v1\nurl: ldaps://activedirectory.openinfra.lab\nbindDN: 'CN=Chris J,CN=Users,DC=openinfra,DC=lab'\nbindPassword: \n  file: '/etc/secrets/bindPassword'\ninsecure: false\nca: ad-crt.crt\ngroupUIDNameMapping:\n  \"CN=admins,CN=Users,DC=openinfra,DC=lab\": server-admin\n  \"CN=redhat,CN=Users,DC=openinfra,DC=lab\": redhat-users\naugmentedActiveDirectory:\n    groupsQuery:\n        derefAliases: never\n        pageSize: 0\n    groupUIDAttribute: dn\n    groupNameAttributes: [ cn ]\n    usersQuery:\n        baseDN: \"DC=openinfra,DC=lab\"\n        scope: sub\n        derefAliases: never\n        filter: (objectclass=person)\n        pageSize: 0\n    userNameAttributes: [ sAMAccountName ]\n    groupMembershipAttributes: [ memberOf ]\n</code></pre></p> <p>NOTE: The groupUIDNameMapping: parameter is the mappying of groups in AD to the groups that either exist or should be created in OpenShift.  These names must be lowercase and conform to Kubernetes naming standards.</p> <p>To only import specific groups, create a whitelist.txt file that contains the CNs of the groups to import.</p> <pre><code>$ cat whitelist.txt\nCN=admins,CN=Users,DC=openinfra,DC=lab\nCN=redhat,CN=Users,DC=openinfra,DC=lab\n</code></pre> <p>Sync the groups: <pre><code>oc adm groups sync --sync-config=ldapsync-config.yaml --whitelist=whitelist.txt --confirm\n</code></pre></p> <p>NOTE: Leave off the --confirm if you want to run a dry-run.</p> <p>To add cluster-admin role to the new server-admin group: <pre><code>oc adm policy add-cluster-role-to-group cluster-admin *server-admin*\n</code></pre></p> <p>NOTE: By default users that are authenticated will be in the group system:authenticated:oauth which is attached to the self-provisioner cluster role binding.  Meaning they are allowed to create projects.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/imageregistry/","title":"Image Registry and VDDK Configuration","text":"<p>For OCPv, it is highly recommended to create the VDDK container.  The VMware Virtual Disk Development kit accelerates the transferring of virtual disk from vSphere to OpenShift.  Once the container image is built with the VDDK, it gets pushed to the image-registry.</p> <p>NOTE: Per our documentation, storing the VDDK image in a public registry might violate the VMware license terms.  </p> <p>Configure an S3 bucket as the image store using Noobaa.  Manage the image-registry and configure the backing store in the cluster operator.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/imageregistry/#create-the-s3-bucket","title":"Create the S3 Bucket","text":"<p>Configure an S3 bucket to be used as the backing store to the internal image-registry.  </p> <p>Create a bucket in the <code>openshift-image-registry</code> namespace.  Once you have the bucket created, create a secret with the credentials.</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: openshift-image-registry\n  namespace: openshift-image-registry\nspec:\n  storageClassName: openshift-storage.noobaa.io\n  bucketName: image-registry-bucket\nEOF\n</code></pre> <p>Set up the following variables.</p> <pre><code>BUCKET_NAME=$(oc get obc -n openshift-image-registry image-registry-bucket -o jsonpath='{.spec.bucketName}')\n\nAWS_ACCESS_KEY_ID=$(oc get secret -n openshift-image-registry image-registry-bucket -o yaml | grep -w \"AWS_ACCESS_KEY_ID:\" | head -n1 | awk '{print $2}' | base64 --decode)\n\nAWS_SECRET_ACCESS_KEY=$(oc get secret -n openshift-image-registry image-registry-bucket -o yaml | grep -w \"AWS_SECRET_ACCESS_KEY:\" | head -n1 | awk '{print $2}' | base64 --decode)\n\nROUTE_HOST=$(oc get route s3 -n openshift-storage -o=jsonpath='{.spec.host}')\n</code></pre> <p>Create the image-registry-private-configuration-user secret in the openshift-image-registry namespace. <pre><code>oc create secret generic image-registry-private-configuration-user --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${AWS_ACCESS_KEY_ID} --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${AWS_SECRET_ACCESS_KEY} --namespace openshift-image-registry\n</code></pre></p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/imageregistry/#configure-the-image-registry","title":"Configure the image-registry","text":"<p>Manage the image-registry and configure the backing store in the cluster operator.</p> <p>To enable https, extract the external route certificate. <pre><code>oc extract secret/router-certs-default  -n openshift-ingress  --confirm\n</code></pre></p> <p>NOTE: This will create the files tls.key and tls.crt in the current directory.</p> <p>Create a configMap with the CA certificate using the tls.crt file. <pre><code>oc create configmap image-registry-s3-bundle --from-file=ca-bundle.crt=./tls.crt  -n openshift-config\n</code></pre></p> <p>Update the cluster operator with the S3 bucket information and change the spec.managementState from <code>Removed</code> to <code>Managed</code> and the spec.replicas from <code>1</code> to <code>2</code>.  Make sure to use the full bucket name and the route contained in the $BUCKET_NAME and $ROUTE_HOST variables obtained from above.  The spec.storage.s3.trustedCA.name the name of the configMap created in the previous step.</p> <p>NOTE: Using an S3 bucket as the backing store allows us to configure multiple replicas and provide high availability to the image-registry service.</p> <pre><code>oc edit config.imageregistry.operator.openshift.io cluster\n</code></pre> <pre><code>spec:\n  httpSecret: 04c54e5a8e787ce951a7f5b899bf08d88983075c894d2a40cd1096fb75f852a033e7e1ab536544dd5996938a9d7e160eecbca89a71f004e89824b3428bbd46ff\n  logLevel: Normal\n  managementState: Managed\n  observedConfig: null\n  operatorLogLevel: Normal\n  proxy: {}\n  replicas: 2\n  requests:\n    read:\n      maxWaitInQueue: 0s\n    write:\n      maxWaitInQueue: 0s\n  rolloutStrategy: RollingUpdate\n  storage:\n    managementState: Unmanaged\n    s3:\n      bucket: image-registry-bucket-af047e2e-f030-4775-b255-89b9ab7f85b5\n      region: us-east-1\n      regionEndpoint: https://s3-openshift-storage.apps.blm-bmb.hexo.lab\n      trustedCA:\n        name: image-registry-s3-bundle\n      virtualHostedStyle: false\n  unsupportedConfigOverrides: null\n</code></pre> <p>NOTE: If the image-registry was already in a <code>Managed</code> state, modify the spec.rolloutStrategy from <code>RollingUpdate</code> to <code>Recreate</code>.</p> <p>After a few minutes the image-registry should be in an <code>Available</code> state. <pre><code>oc get co image-registry\n</code></pre> <pre><code>NAME             VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nimage-registry   4.15.31   True        False         False      5s      \n</code></pre></p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/imageregistry/#create-the-vddk-container","title":"Create the VDDK Container","text":"<p>Pre-requisites:  </p> <ul> <li>Image Registry is configured  </li> <li>podman is installed  </li> <li>File system preserves symbolic links  </li> </ul> <p>Download the VDDK tar file and extract in temporary diretory.</p> <p>NOTE: You must have a valid login to Broadcom and the appropriate support or access level.  The customer should have this with their VMware support.</p> <p>Make sure you are logged in to the OCP cluster or have exported the <code>KUBECONFIG</code> variable.</p> <pre><code>tar -xzf VMware--vix-disklib-8.0.1-21562716.x86_64.tar.gz\n</code></pre> <p>NOTE: this will create the vmware-vix-disklib-distrib directory in your current directory.</p> <p>Create a Dockerfile: <pre><code>cat &gt; Dockerfile &lt;&lt;EOF\nFROM registry.access.redhat.com/ubi8/ubi-minimal\nUSER 1001\nCOPY vmware-vix-disklib-distrib /vmware-vix-disklib-distrib\nRUN mkdir -p /opt\nENTRYPOINT [\"cp\", \"-r\", \"/vmware-vix-disklib-distrib\", \"/opt\"]\nEOF\n</code></pre></p> <p>Build the VDDK image.  Make sure to use the URL of the image-registry route for your cluster. <pre><code>podman build . -t default-route-openshift-image-registry.apps.blm-bmb.hexo.lab/openshift-mtv/vddk:8.0.1\n</code></pre></p> <p>NOTE: You must include the namespace in the image path.</p> <p>Verify your image: <pre><code>podman image ls\n</code></pre></p> <pre><code>REPOSITORY                                                                       TAG         IMAGE ID      CREATED        SIZE\ndefault-route-openshift-image-registry.apps.blm-bmb.hexo.lab/openshift-mtv/vddk  8.0.1       55152a0e1b49  7 seconds ago  173 MB\nregistry.access.redhat.com/ubi8/ubi-minimal                                      latest      12c4198317ec  4 weeks ago    93.9 MB\n</code></pre> <p>Login to the registry.  Make sure to use the URL of the image-registry route for your cluster. <pre><code>export TOKEN=$(oc create token builder -n openshift-mtv)\n</code></pre></p> <pre><code>podman login --tls-verify=false -u builder -p $TOKEN default-route-openshift-image-registry.apps.blm-bmb.hexo.lab\n</code></pre> <p>Push the image to the registry using the URL of the image-registry route for your cluster and full path to the image. <pre><code>podman push --tls-verify=false default-route-openshift-image-registry.apps.blm-bmb.hexo.lab/openshift-mtv/vddk:8.0.1\n</code></pre></p> <p>The image is only available to the openshift-mtv namespace but will be used by all namespaces the VMs are migrated to.  You can make the image available to all namespaces but creating a new roleBinding.</p> <pre><code>oc apply -f image-puller-rb.yaml\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  annotations:\n    openshift.io/description: Allows all pods in all namespaces to pull images from\n      this namespace.\n  name: allow-image-pullers\n  namespace: openshift-mtv\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:image-puller\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:authenticated\n</code></pre> <p>When you create your VMware provider, you can use the image-registry service URL and port along with the full path to the VDDK image.</p> <p></p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/mustgather/","title":"The must-gather Tool","text":"<p>Link to our documentation</p> <p>When opening a support case for OSV, run the following must-gathers:</p> <pre><code>oc adm must-gather\n</code></pre> <pre><code>oc adm must-gather --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.17.3\n</code></pre> <p>MTV must-gather Documentation</p> <p>Check the link for instructions to filter the data based on namespace, migration, or VM.</p> <pre><code>oc adm must-gather --image=registry.redhat.io/migration-toolkit-virtualization/mtv-must-gather-rhel8:2.7.8\n</code></pre>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/secondaryScheduler/","title":"Secondary Scheduler","text":"<p>The kube-scheduler binary includes multiple plugins.  The plugins are configured by creating one or more scheduler profiles.  The example below uses Trimaran; a collection of load-aware plugins.</p> <ul> <li>TargetLoadPacking: Implements a packing policy up to a configured CPU utilization, then switches to a spreading policy among the hot nodes. (Supports CPU resource.)  </li> <li>LoadVariationRiskBalancing: Equalizes the risk, defined as a combined measure of average utilization and variation in utilization, among nodes. (Supports CPU and memory resources.)  </li> <li>LowRiskOverCommitment: Evaluates the performance risk of overcommitment and selects the node with lowest risk by taking into consideration (1) the resource limit values of pods (limit-aware) and (2) the actual load (utilization) on the nodes (load-aware). Thus, it provides a low risk environment for pods and alleviate issues with overcommitment, while allowing pods to use their limits.  </li> </ul>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/secondaryScheduler/#install-the-secondary-scheduler-operator","title":"Install the Secondary Scheduler Operator","text":"<p>Create a namespace to install the secondary scheduler in.</p> <p>Install the Secondary Scheduler from Operator Hub in the newly created namespace.</p> <p>Before creating an instance of the secondary scheduler, create the config map.</p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\nleaderElection:\n  leaderElect: false\nprofiles:\n  - schedulerName: secondary-scheduler\n    plugins:\n      score:\n        disabled:\n          - name: NodeResourcesBalancedAllocation\n          - name: NodeResourcesLeastAllocated\n        enabled:\n          - name: TargetLoadPacking\n    pluginConfig:\n      - name: TargetLoadPacking\n        args:\n          defaultRequests:\n            cpu: \"2000m\"\n          defaultRequestsMultiplier: \"1\"\n          targetUtilization: 70\n          metricProvider:\n            type: Prometheus\n            address: ${PROM_URL}\n            token: ${PROM_TOKEN}\n</code></pre> <p>NOTE:  Update the PROM_URL and the PROM_TOKEN with the appropriate values. <pre><code>PROM_URL=https://`oc get routes prometheus-k8s -n openshift-monitoring -o json |jq \".status.ingress\"|jq \".[0].host\"|sed 's/\"//g'`  \nPROM_TOKEN=`oc create token prometheus-k8s -n openshift-monitoring`\n</code></pre></p> <p>Apply the new configMap: <pre><code>oc create -n openshift-secondary-scheduler-operator configmap secondary-scheduler-config --from-file=config.yaml\n</code></pre></p> <p>Create an instance of the secondary scheduler</p> <pre><code>apiVersion: operator.openshift.io/v1\nkind: SecondaryScheduler\nmetadata:\n  name: cluster\n  namespace: openshift-secondary-scheduler-operator\nspec:\n  logLevel: Normal\n  managementState: Managed\n  operatorLogLevel: Normal\n  schedulerConfig: secondary-scheduler-config\n  schedulerImage: 'registry.k8s.io/scheduler-plugins/kube-scheduler:v0.28.9'\n</code></pre> <p>NOTE: See scheduler-plugin-images for container image compatibility.</p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/secondaryScheduler/#schedule-vm-using-the-secondary-scheduler","title":"Schedule VM using the Secondary Scheduler","text":"<p>Update the VMs yaml: <pre><code>...\nspec:\n  dataVolumeTemplates:\n    - apiVersion: cdi.kubevirt.io/v1beta1\n      kind: DataVolume\n      metadata:\n        creationTimestamp: null\n        name: centos-stream9-blue-snipe-20\n      spec:\n        sourceRef:\n          kind: DataSource\n          name: centos-stream9\n          namespace: openshift-virtualization-os-images\n        storage:\n          resources:\n            requests:\n              storage: 30Gi\n  running: true\n  template:\n    metadata:\n      annotations:\n        vm.kubevirt.io/flavor: small\n        vm.kubevirt.io/os: centos-stream9\n        vm.kubevirt.io/workload: server\n      creationTimestamp: null\n      labels:\n        kubevirt.io/domain: centos-stream9-blue-snipe-20\n        kubevirt.io/size: small\n    spec:\n      schedulerName: secondary-scheduler\n      architecture: amd64\n      domain:\n...\n</code></pre></p> <p>NOTE: The VM must be restarted for the new scheduler to kick in.</p> <p>Verify VM is using the secondary-scheduler: <pre><code>oc describe pod/virt-launcher-centos-stream9-secondary-scheduler-vm1-r275d -n blm-project\n</code></pre></p> <p>Example output: <pre><code>...\nEvents:\n  Type    Reason                 Age   From                 Message\n  ----    ------                 ----  ----                 -------\n  Normal  Scheduled              105s  secondary-scheduler  Successfully assigned blm-project/virt-launcher-centos-stream9-secondary-scheduler-vm1-r275d to kni-worker2\n...\n</code></pre></p>"},{"location":"How%20To/Proof%20Of%20Concept%20Topics/Openshift/secondaryScheduler/#helpful-links","title":"Helpful Links","text":"<p>How to bring your own scheduler into OpenShift</p> <p>Trimaran Scheduler GitHub repo</p>"},{"location":"OCPv/highAvailability/","title":"Kubernetes HA for VMs","text":"<p>Hypervisors</p> <ul> <li>Baremetal - executes on the host's hardware as a lyaer of a lightweight operating system.  </li> <li>Hosted Hypervisor - executes on the host's operating system; allowing you to use the host for purposes other than virtualization.</li> </ul> <p>Kubernetes Features for VMs</p> <ul> <li>Load balancing using the service/route features</li> <li>Readiness/Liveness Probes</li> <li>Sticky sessions</li> <li>Watchdog devices - monitor OS and do not detect application features</li> <li>Machine health checks - only available for clusters that are installed as baremetal IPI</li> <li>Live migration</li> <li>VM run strategies - .spec.running or .spec.runStrategy</li> <li>Fencing nodes - reboots/deletes <code>Machine</code> CRDs to solve problems with automatically provisioned nodes</li> </ul>"},{"location":"OCPv/highAvailability/#load-balance","title":"Load Balance","text":"<p>Service resource definition:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend \n  namespace: prod2 \nspec:\n  type: ClusterIP\n  selector:\n    tier: backend \n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <p>Route resource definition:</p> <pre><code>apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: helloworld\n  namespace: ha-loadbalance\n  annotations:\n    router.openshift.io/cookie_name: \"hello\"\nspec:\n  host: hello-ha-loadbalance.apps.ocp4.example.com\n  port:\n    targetPort: 80\n  to:\n    kind: Service\n    name: helloworld\n</code></pre> <p>Note: Annotating a route with the router.openshift.io/cookie_name=\"cookie_name\" annotation creates a custom-named cookie for the route.</p>"},{"location":"OCPv/highAvailability/#update-run-strategy","title":"Update Run Strategy","text":"<p>Edit the VM resource to modify the behavior.</p> <p><code>RerunOnFailure</code> restarts the VM when it fails; VM not restarted when OS is gracefully shutdown. <code>Always</code> ensures the VM is always running. <code>Halted</code> ensure the VM is not running. <code>Manual</code> performs no automatic actions.   </p> <p>Note: the .spec.runing or .spec.runStrategy are mutually exclusive.</p>"},{"location":"OCPv/highAvailability/#configure-health-probes","title":"Configure Health Probes","text":"<ul> <li>HTTP Get - check is successful if response code is 200-399</li> <li>TCP Socket - establishes a connection to the app's TCP port</li> </ul> <p>Readiness Probe using HTTP Get:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n...output omitted...\nspec:\n  ...output omitted...\n  template:\n    metadata:\n    ...output omitted...\n    spec:\n      domain:\n        ...output omitted...\n      readinessProbe:\n        httpGet:\n          path: /health\n          port: 8080\n        initialDelaySeconds: 120 \n        periodSeconds: 20\n        timeoutSeconds: 10\n        failureThreshold: 3\n        successThreshold: 3\n      evictionStrategy: LiveMigrate\n...\n</code></pre> <p>Liveness Probe using TCP Socket:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n...output omitted...\nspec:\n  ...output omitted...\n  template:\n    metadata:\n    ...output omitted...\n    spec:\n      domain:\n        ...output omitted...\n      livenessProbe:\n        tcpSocket: 1\n          port: 3306\n        initialDelaySeconds: 120\n        periodSeconds: 20\n      evictionStrategy: LiveMigrate\n...\n</code></pre>"},{"location":"OCPv/highAvailability/#configure-watchdog-device","title":"Configure Watchdog Device","text":"<p>Add the emulated watchdog device to your VM resource to activate watchdog monitoring. Use the oc edit command to add a new device section to the VM resource:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n...output omitted...\nspec:\n  ...output omitted...\n  template:\n    metadata:\n    ...output omitted...\n    spec:\n      domain:\n        cpu:\n          ...output omitted...\n        devices:\n          disks:\n          - bootOrder: 1\n            disk:\n              bus: virtio\n            name: mariadb-server\n          interfaces:\n          - macAddress: \"02:48:09:00:00:00\"\n            masquerade: {}\n            name: default\n          networkInterfaceMultiqueue: true\n          rng: {}\n          watchdog:\n            i6300esb:\n              action: poweroff\n            name: mywatchdog\n...\n</code></pre> <p>Note:  1. OCPv can only emulate the Intel 6300ESB chipset. 2. The <code>action</code> parameter can be set to poweroff, reset, or shutdown.</p>"},{"location":"OCPv/highAvailability/#machine-health-checks","title":"Machine Health Checks","text":"<p>Limitations:</p> <ul> <li>Control plane hosts are not currently supported and are not remediated if they are unhealthy.</li> <li>Only hosts owned by a machine set are remediated by a machine health check.</li> <li>If the node for a host is removed from the cluster, the machine health check identifies that the host is unhealthy and remediates it immediately.</li> <li>A host with the master role cannot be remediated.</li> <li>A host is remediated immediately if the Machine resource enters a Failed status.</li> <li>For cloud environments, a machine health check relies on cloud provider integration for the machine to forcibly reboot, reprovision, and rejoin the cluster.</li> </ul>"},{"location":"OCPv/livemigrations/","title":"Live Migrations","text":"<p>To perform a live migration, the VM must meet the following conditions:</p> <ul> <li>The underlying persistent volume claim (PVC) must use ReadWriteMany (RWX) access mode.  </li> <li>The pod network is not configured with the bridge binding type.  </li> <li>Ports 49152 and 49153 must be available in the VM's virt-launcher pod; live migration fails if these ports are specified in a masquerade network interface.  </li> </ul> <p>You must enable the sriovLiveMigration feature gate in the HyperConverged custom resource (CR) to perform a live migration for VMs attached to an SR-IOV network interface.</p> <p>Default Limits:</p> Key Description Default Value parallelMigrationsPerCluster Max migrations that can run in parallel 5 parallelOutboundMigrationsPerNode Max migrations moving from a node 2 bandwidthPerMigration Limits the bandwidth; in MiB/s 64 completionTimeoutPerGiB If migration exceeds the defined time, in GiB/s of memory, migration is canceled. 800 progressTimeout The migration is canceled if the memory copy fails to make progress in this time (seconds) 150"},{"location":"OCPv/livemigrations/#node-selector","title":"Node Selector","text":"<p>Kubernetes default scheduler is used along with <code>.spec.template.spec.nodeSelector</code> labels to select the worker node the VM should be scheduled on.  Another option is to install the Secondary Scheduler Operator which takes load into consideration as well. </p> <pre><code>metadata:\n  name: example-vm-node-selector\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nspec:\n  template:\n    spec:\n      nodeSelector:\n        example-key-1: example-value-1\n        example-key-2: example-value-2\n...\n</code></pre>"},{"location":"OCPv/livemigrations/#affinity","title":"Affinity","text":"<p>Use node affinity rules to schedule a VM to run on a group of nodes. You can also specify anti-affinity rules to prevent a VM from running on a particular group of nodes. To allow more flexibility during scheduling, you can specify if the affinity rule is required for the VM to run, or preferred but not required.</p> <pre><code>metadata:\n  name: example-vm-node-affinity\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: example.io/example-key\n            operator: In\n            values:\n            - example-value-1\n            - example-value-2\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: example-node-label-key\n            operator: In\n            values:\n            - example-node-label-value\n...\n</code></pre>"},{"location":"OCPv/livemigrations/#tolerations","title":"Tolerations","text":"<p>A toleration specifies that a VM can be scheduled on a node if the VM's taint matches the taints on the node. However, if a VM is configured to tolerate a taint, it is not required to be scheduled onto a node configured with said taint.</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: example-vm-tolerations\nspec:\n  tolerations:\n  - key: \"key\"\n    operator: \"Equal\"\n    value: \"virtualization\"\n    effect: \"NoSchedule\"\n...\n</code></pre> <p>Scheduler Profiles:  </p> <ul> <li>LowNodeUtilization   </li> <li>HighNodeUtilization  </li> <li>NoScoring - low latency profile that strives for the quickest scheduling cycle by disabling all score plug-ins  </li> </ul>"},{"location":"OCPv/livemigrations/#live-migration-via-cli","title":"Live Migration via CLI","text":"<p>Storage class must support RWX; patch the Kubernetes storage class if necessary:</p> <pre><code>oc patch -n openshift-cnv cm kubervirt-storage-class-defaults -p \\\n'{\"data\":{\"nfs-storage.accessMode\":\"ReadWriteMany\"}}'\n</code></pre> <p><pre><code>$ oc get vmi -n &lt;nameSp&gt;\n</code></pre> NAME                     AGE     PHASE     IP             NODENAME      READY fedora-competent-orca    18h     Running   10.131.1.129   kni-worker2   True rhel9-xenial-crocodile   7d21h   Running   10.128.2.247   kni-worker3   True</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachineInstanceMigration\nmetadata:\n  name: migration-job\nspec:\n  vmiName: rhel9-xenial-crocodile\n</code></pre> <pre><code>oc create -f yaml.file -n &lt;nameSp&gt;\n</code></pre>"},{"location":"OCPv/livemigrations/#node-maintenance","title":"Node Maintenance","text":"<p>Node Maintenance Resource:</p> <pre><code>apiVersion: nodemaintenance.kubevirt.io/v1beta1\nkind: NodeMaintenance\nmetadata:\n  name: maintenance-node2\nspec:\n  nodeName: node2\n  reason: \"Node maintenance\"\n</code></pre> <p>Using the CLI</p> <pre><code>oc adm cordon node2\n</code></pre> <pre><code>oc adm drain node2 --delete-empty-dir-data --ignore-daemonsets --force\n</code></pre> <p>Note:</p> <ul> <li>The --delete-emptydir-data option prevents the drain operation from failing when some pods or VMs use the local node storage as an ephemeral volume. In that case, RHOCP restarts the pod or VM on the other nodes with a new empty volume.  </li> <li>With the --ignore-daemonsets option, RHOCP skips moving the Daemon Set pods.  </li> <li>The --force option prevents the drain operation from failing when some pods that RHOCP does not manage are running on the node.  </li> </ul> <p>To remove the node from maintenance mode:</p> <pre><code>oc adm uncordon node2\n</code></pre> <p>During node drain, OCPv performs live migrations for the VMs with the <code>evictionStrategy</code> set to LiveMigrate.  For VMs without the <code>evictionStrategy</code> parameter, OCPv shuts down the VMs and restarts them on another node.</p> <p>If the <code>evictionStrategy</code> is set but the underlying storage used doesn't support live migrations, the drain process will be blocked.  For these VMs, remove the <code>evictionStrategy</code> parameter to prevent the drain process from blocking. To remove the parameter, stop the VM, navigate to the Details tab, and then scroll down to the Scheduling and resource requirements section. Remove the LiveMigrate flag from the Eviction Strategy parameter.</p>"},{"location":"OCPv/livemigrations/#article-on-vmware-change-block-tracking","title":"Article on VMware - Change Block Tracking","text":"<p>https://kb.synology.com/en-us/DSM/tutorial/How_to_enable_CBT_manually_for_a_virtual_machine</p>"},{"location":"OCPv/mtv/","title":"Migration Toolkit for Virtualization","text":""},{"location":"OCPv/mtv/#cold-vs-warm-migration","title":"Cold vs. Warm Migration","text":"<p>Cold * Shutdown, conversion and then disk transfers * virt-v2v (el9) * Disks are transferred sequentially</p> <p>Warm * Pre-copy, shutdown, disk transfer, and then conversion * Containerized Data Importer (CDI) + virt-v2v (el8) * Disks are transferred in parallel by different pods</p>"},{"location":"OCPv/snapshots/","title":"VM Snapshots and Clones","text":""},{"location":"OCPv/snapshots/#snapshots","title":"Snapshots","text":"<p>When the VM is running, OCPv communicates with the QEMU guest agent running inside the VM to quiesce the file systems before taking the snapshot. This mechanism ensures that the file systems are in a consistent state. If you do not install the QEMU guest agent, then you can still take live snapshots, but the file systems might not be consistent. The QEMU guest agent is available for Linux and Microsoft Windows systems.</p> <p>If one VM disk relies on back-end storage that does not support snapshots, then OCPv displays a warning message when you take the snapshot. When you revert the snapshot, OCPv does not restore the data for that specific disk, which stays unchanged after the revert.</p> <p>A VM snapshot also records the VM configuration, such as the number of CPUs or the amount of memory. If you change the VM configuration after taking the snapshot, then OCPv restores the original configuration when you revert the snapshot.</p>"},{"location":"OCPv/snapshots/#custom-resources","title":"Custom Resources","text":"<ul> <li>VirtualMachineSnapshot: respresents a user request to create a snapshot, contains information about the current state of the VM.  </li> <li>VirtualMachineSnapshotContent: represents a provisioned resource on the cluster (snapshot).  Created by the VM snapshot controller and contains references to all resources required to restore the VM.  </li> <li>VirtualMachineRestore: represents a user request to restore a VM from a snapshot</li> </ul>"},{"location":"OCPv/snapshots/#snapshot-yaml","title":"Snapshot Yaml","text":"<pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: mariadb-server-2022-3-23\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: mariadb-server\n</code></pre>"},{"location":"OCPv/snapshots/#details-on-snapshot","title":"Details on Snapshot","text":"<pre><code>oc describe vmsnapshot &lt;snapshot_name&gt;\n</code></pre> <p>Note: vlues of the <code>status.indications</code> parameters can be online, GuestAgent, NoGuestAgent.</p>"},{"location":"OCPv/snapshots/#restore-yaml","title":"Restore Yaml","text":"<pre><code>oc get virtualmachinesnapshot\n</code></pre> <pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineRestore\nmetadata:\n  name: restore-db\nspec:\n  target:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: mariadb-server\n  virtualMachineSnapshotName: mariadb-server-2022-3-23\n</code></pre> <p>Note: A VM snapshot includes the VM configuration. If you change the VM disk configuration after you have taken a snapshot, review the following considerations before restoring the snapshot:  </p> <ul> <li> <p>If you removed a disk, then OpenShift Virtualization recreates the disk. If the initial disk does not support snapshots, then OCPv attaches a new blank disk with the same characteristics as the original disk. With this type of disk, OCPv does not restore the data present at the time you took the snapshot. </p> </li> <li> <p>If you add a disk, then OCPv removes the disk when you restore the snapshot. If you used a data volume resource to create the associated PVC, by using the OpenShift web console, for example, then OCPv deletes the PVC and all its data.</p> </li> </ul>"},{"location":"OCPv/snapshots/#delete-snapshots","title":"Delete Snapshots","text":"<pre><code>oc delete virtualmachinesnapshot &lt;snapName&gt;\n</code></pre>"},{"location":"OCPv/snapshots/#clones","title":"Clones","text":"<p>Be aware that a clone will likely have machine-specific data and configuration settings from the original VM. For example, Microsoft Windows clones get the same machine Security Identifier (SID). Red Hat Enterprise Linux clones get the same SSL certificates, SSH host keys, and Red Hat subscriptions.</p> <p>To prevent the clones from having these same identifying settings, you usually seal the original VM before using it to create clones. Sealing the VM is a process where you clear the machine-specific information.</p> <p>OCPv provides the virtctl guestfs command to start a container that includes the virt-sysprep tool. The container also attaches the VM's PVC as the /dev/vda block device. </p> <pre><code>$ virtctl guestfs -n &lt;nameSp&gt; &lt;pvcName&gt;\nUse image: registry.redhat.io/container-native-virtualization/libguestfs-tools@sha256:591e...64ae\nThe PVC has been mounted at /dev/vda\nWaiting for container libguestfs still in pending, reason: ContainerCreating, message:\n...output omitted...\nIf you don't see a command prompt, try pressing enter.\n[root@libguestfs-tools-dsk1 /]# virt-sysprep -a /dev/vda\n[   0.0] Examining the guest ...\n[   3.4] Performing \"abrt-data\" ...\n[   3.4] Performing \"backup-files\" ...\n[   4.1] Performing \"bash-history\" ...\n...output omitted...\n[   4.7] Setting a random seed\n[   4.7] Setting the machine ID in /etc/machine-id\n[   4.9] Performing \"lvm-uuids\" ...\n[root@libguestfs-tools-dsk1 /]# exit\nexit\n[user@host ~]$\n</code></pre> <p>DataVolume section of a Virtual Machine resource: <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\n...output omitted...\nspec:\n  dataVolumeTemplates:\n  - metadata:\n      creationTimestamp: null\n      name: front1-vm1-bvle4\n    spec:\n      pvc:\n        accessModes:\n        - ReadWriteMany\n        resources:\n          requests:\n            storage: 10Gi\n        storageClassName: ocs-external-storagecluster-ceph-rbd\n        volumeMode: Block\n      source:\n        pvc:\n          name: vm1\n          namespace: golden-vms\n  - metadata:\n      creationTimestamp: null\n      name: front1-webroot-diaki\n    spec:\n      pvc:\n        accessModes:\n        - ReadWriteMany\n        resources:\n          requests:\n            storage: 1Gi\n        storageClassName: ocs-external-storagecluster-ceph-rbd\n        volumeMode: Block\n      source:\n        pvc:\n          name: webroot\n          namespace: golden-vms\n...output omitted...\n</code></pre></p>"},{"location":"OCPv/snapshots/#clone-vm-disk-using-data-volume","title":"Clone VM Disk using Data Volume","text":"<p>The StorageProfile resource uses the cloneStrategy parameter to specify the cloning method:</p> <ul> <li>csi-clone</li> <li>snapshot</li> <li>copy; OCPv starts Kubernetes pods to perform a copy</li> </ul> <p>Note: if not specified, OCPv uses snapshots if possible, otherwise the copy method is used.</p> <p>Clone a individual disk using a DataVolume resource:</p> <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: documentroot-clone1\nspec:\n  storage:\n    resources:\n      requests:\n        storage: 1Gi\n    storageClassName: ocs-external-storagecluster-ceph-rbd\n  source:\n    pvc:\n      name: documentroot\n      namespace: golden-vms\n</code></pre>"},{"location":"OCPv/storage/","title":"Storage for VMs","text":""},{"location":"OCPv/storage/#bare-metal-storage-classes","title":"Bare Metal Storage Classes","text":"<p>odf-storage - created by LSO (you provide the name when you create the storageSystem) ocs-storagecluster-ceph-rbd ocs-storagecluster-ceph-rgw ocs-storagecluster-cephfs openshift-storage.noobaa.io  </p>"},{"location":"OCPv/storage/#cloud-default-storage-classes","title":"Cloud Default Storage Classes","text":"<p>AWS - gp3 Azure - managed-premium</p>"},{"location":"OCPv/storage/#resource-file","title":"Resource File","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: dbdata\nspec:\n  storageClassName: ocs-external-storagecluster-ceph-rbd  \n  accessModes:\n    - ReadWriteOnce  \n  resources:\n    requests:\n      storage: 10Gi  \n  volumeMode: Filesystem  \n</code></pre> <p>Note: volumeMode can be <code>Filesystem</code> (default)l; volume has a filesystem that you can mount inside the container or <code>Block</code>, volume is presented as a raw devices (i.e. /dev/xvda).</p>"},{"location":"OCPv/storage/#pvcs-for-ocpv","title":"PVCs for OCPv","text":"<p>With PVCs in Block mode, OpenShift Virtualization transfers the disk image into the volume. The VM disk uses the volume as its back-end device.  Better option is when the CSI driver supports snapshots/cloning and the source PVC is cloned.</p> <p>With PVCs in Filesystem mode, OpenShift Virtualization creates a disk.img file at the root of the PV file system and then copies the disk image into the file. The VM disk uses the disk.img file as its back-end device.</p>"},{"location":"OCPv/storage/#data-volumes","title":"Data Volumes","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: osdisk\nspec:\n  preallocation: true\n  storage:  \n    resources:\n      requests:\n        storage: 10Gi\n    storageClassName: ocs-external-storagecluster-ceph-rbd  \n  source:  \n    http:\n      url: http://images..example.com/rhel-8.5-x86_64-kvm.qcow2\n</code></pre> <p>Note: Using <code>preallocation</code> will reserve the entire requested space with the back-end storage; this can improve write performance.</p>"},{"location":"OCPv/storage/#select-a-data-volume-source","title":"Select a Data Volume Source","text":"<ul> <li> <p>Blank (creates PVC) The new Persistent Volume Claim (PVC) provides an unformatted raw device. You can use your operating system tools to partition and then format the new block device from inside the VM.  </p> </li> <li> <p>Import via URL (creates PVC) Red Hat OpenShift Virtualization downloads the virtual disk image from the URL that you provide and then extracts it into the new volume. The virtual disk image must be in the raw format or the QEMU copy on write version 2 (qcow2) format. The disk usually includes file systems that you can mount to access their data from inside the VM. Because the disk is read/write, Red Hat OpenShift Virtualization preserves all the changes you make across VM restarts.  </p> </li> <li> <p>Use an existing PVC OpenShift Virtualization attaches the PVC you provide as a disk. That PVC might come from a data volume you detached from another VM and that you want to reuse with your VM.  </p> </li> <li> <p>Clone existing PVC (creates PVC) OpenShift Virtualization copies the PVC you provide and then uses that copy as the VM disk. Because the source PVC must not be in use for the cloning process to start, you must stop any pods or VMs that are actively using it.  </p> </li> <li> <p>Import via Registry (creates PVC) OpenShift Virtualization downloads the given container image from a container registry. The container image contains a disk image in the raw or qcow2 format under the /disk/ directory. OpenShift Virtualization extracts that disk image and copies it into the volume.  </p> </li> </ul> <p>This source option is similar to importing disks from URLs, but it is useful when you already have a container registry deployed in your organization. Instead of installing a new file server to distribute your disk images, you can store these images in your container registry.  </p> <p>Because container registries can only manage images, you must store your disk images in container images under the /disk/ directory. These images are not meant to be run as containers, and they usually only contain the /disk/ directory and the disk image.  </p> <ul> <li>Container (ephemeral) OpenShift Virtualization downloads the given container image from a container registry and uses the disk in that container image as the VM disk. This source option is similar to the preceding option except that OpenShift Virtualization does not create DataVolume, PVC, or PV resources and therefore does not consume space on your back-end storage. </li> </ul> <p>You can mount the disk read/write into the VM, but the data you write are not persistent. Every time you restart the VM, OpenShift Virtualization discards your data and restores the disk image to its initial state. </p> <p>You can use ephemeral storage when you do not need to preserve data, such as temporary files.  </p>"},{"location":"OCPv/storage/#resizing","title":"Resizing","text":"<p>Verify the storage class supports resizing of PVCs:</p> <pre><code>oc get sc\noc get sc/ocs-storagecluster-ceph-rbd-virtualization -o yaml\nallowVolumeExpansion: true\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n...\n</code></pre>"},{"location":"OCPv/storage/#creating-a-pv","title":"Creating a PV","text":"<p>Kubernetes does not delete the static PV when you delete the PVC. If you do not need the PV anymore, then ask your cluster administrator to delete it. If you want to create a PVC that reuses the PV, then your cluster administrator must release the PV first. To release the PV, your cluster administrator can edit the resource and remove the uid parameter from the claimRef section:</p> <pre><code>...\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: websrv1-staticimgs\n    namespace: vm-project\n    resourceVersion: \"325655\"\n    # uid: d9c1a805-f1e8-4370-8d6e-c450dc9c3ef3\n    ...\n</code></pre>"},{"location":"OCPv/storage/#fc","title":"FC","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: fc-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  claimRef:\n    name: dbsrv1-binlog\n    namespace: vm-project\n  fc:\n    targetWWNs:\n      - \"50060e801049cfd1\"\n    lun: 0    \n</code></pre> <p>Note: Implies that the cluster nodes have HBAs connected to a FC storage array</p>"},{"location":"OCPv/storage/#nfs","title":"NFS","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\nspec:\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteMany  1\n  volumeMode: Filesystem  2\n  claimRef:\n    name: websrv1-logs\n    namespace: vm-project\n  nfs:  3\n    path: /exports-ocp/vm135\n    server: 10.20.42.42\n</code></pre>"},{"location":"OCPv/storage/#iscsi","title":"iSCSI","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: iscsi-pv\nspec:\n  capacity:\n    storage: 50Gi \n  accessModes:\n    - ReadWriteOnce \n  volumeMode: Block \n  claimRef: \n    name: websrv1-staticimgs\n    namespace: vm-project\n  iscsi: \n     targetPortal: 192.168.51.40:3260\n     iqn: iqn.1986-03.com.ibm:2145.disk1\n     lun: 0\n     initiatorName: iqn.1994-05.com.redhat:openshift-nodes\n</code></pre>"},{"location":"OCPv/storage/#inject-a-disk-image-into-a-volume","title":"Inject a Disk Image into a Volume","text":"<p>Use the virtctl command to inject a disk image: <pre><code>virtctl image-upload pvc &lt;pvcName&gt; --image-path=./webimgfs.qcow2 --no-create\n</code></pre></p>"},{"location":"OCPv/templates/","title":"Templates","text":"<p>Boot Source</p> <ul> <li>PVC (creates PVC): Clone an existing PVC available in the cluster to create a new PVC. </li> <li>Registry (creates PVC): Create a new PVC by importing content from a container registry.  </li> <li>URL (creates PVC): Create a new PVC by importing content from a URL with a HTTP or S3 endpoint.  </li> <li>PXE (network boot - adds network interface): Boots an operating system stored on a server on the network (a PXE bootable network attachment definition is required).  </li> </ul> <p>Flavor </p> <p>This field indicates the size of your VM in terms of CPU and memory and includes the following options:  </p> <ul> <li>Tiny: Creates a VM with 1 CPU and 1 GiB memory; recommended for testing VM creation.  </li> <li>Small: Creates a VM with 1 CPU and 2 GiB memory; this is the default option for any preconfigured template.  </li> <li>Medium: Creates a VM with 1 CPU and 4 GiB memory; appropriate for code testing or to store basic application resources. </li> <li>Large: Creates a VM with 2 CPU and 8 GiB memory; recommended for systems that require a significant consumption of resources. </li> <li>Custom: Specify custom values of CPU and memory as needed for your VM.  </li> </ul> <p>Workload </p> <p>This field indicates the workload type for your VM and includes the following options:  </p> <ul> <li>Desktop: A configuration for a desktop system that prioritizes VM density over guaranteed VM performance. VMs with this configuration are recommended for use in the OpenShift web console.  </li> <li>Server: The default option for any preconfigured template and compatible with various server workloads. This option balances performance and prioritizes VM density over VM performance.  </li> <li>High-Performance: Optimized for high-performance or high-consumption workloads. This option prioritizes guaranteed VM performance over VM density.  </li> </ul> <p>Storage A preconfigured Linux-based template has two partitions by default, cloud-init and root disk. However, it is possible to configure additional disks. Each field has several available options for customizing your template:  </p> <ul> <li>Source: You can create or import a disk from an existing or blank PVC, from an external source such as a container registry or URL, or by using a container in a registry that is accessible from the cluster.  </li> <li>Type: You can customize the storage type, such as a disk or CD-ROM, according to the needs of your VM.  </li> <li>Interface: You can select the communication interface of your disk based on compatibility standards and desired performance of your VM. The available options are virtio, SATA, or SCSI. </li> <li>Storage Class: Select the storage class for the disk. The storage profile sets the optimized access mode and the volume mode for the storage class.  </li> <li>Access mode: You can customize the disk's access mode, overriding the default storage profile settings. The available access modes are Single user (RWO), Shared access (RWX), or Read only (ROX).  </li> <li>Volume mode: You can choose between Filesystem and Block storage volume modes for your VM, depending upon the selected storage class.  </li> </ul> <p>Note: VMs must have a PVC with a shared ReadWriteMany (RWX) access mode to enable live migration.</p> <p>Networking</p> <p>Red Hat provides templates that are set with a default network interface connected to the pod network. You can configure additional network interfaces for your VM. For each field, you have several options available to customize your template:  </p> <ul> <li>Interface model: You can choose either <code>virtio</code> or <code>e1000e</code>, based on your VM's needs and required performance. The <code>virtio</code> model is optimized for best performance and is supported by most Linux distributions. Additional drivers are needed for Windows VMs. The <code>e1000e</code> model is supported by most operating systems, including Windows, but offers slower performance compared to the <code>virtio</code> model. </li> <li>Network: You can select from a list of available network attachment definitions to connect to additional networks. Additional networks must use the <code>bridge</code> binding method.  </li> <li>Type: You can select from a list of available binding methods. You must use the <code>masquerade</code> binding method on the default pod network. Additional networks must use the <code>bridge</code> binding method.  </li> <li>MAC Address: You can specify a custom MAC address for the network interface. MAC addresses are automatically assigned unless you specify a custom address.  </li> </ul> <p>Cloud-init</p> <p>Available on compatible systems, you can use the <code>cloud-init</code> service to configure the guest operating system post-installation. You can use this service to install tools and packages, configure users and passwords, and manage system applications.  </p>"},{"location":"OCPv/templates/#cdi-and-data-volumes","title":"CDI and Data Volumes","text":"<p>The Containerized Data Importer (CDI) is an add-on that manages persistent storage for VMs in OpenShift Virtualization. CDI provides the ability to import, upload, and clone existing PVCs for your VMs. CDI provides a custom resource definition (CRD) for DataVolume objects that orchestrate the import, clone, and upload operations associated with an underlying PVC.  </p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: vm-clone-pvc\n  labels:\n    kubevirt.io/vm: vm-clone-pvc\n  namespace: backup-vms \nspec:\n  running: false\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: vm-clone-pvc\n    spec:\n      domain:\n          cpu:\n          cores: 1\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: root-disk\n        resources:\n          requests:\n            memory: 2Gi\n      terminationGracePeriodSeconds: 0\n      volumes:\n        - dataVolume:\n            name: new-pvc-from-clone\n          name: root-disk\n  dataVolumeTemplates:  \n    - apiVersion: cdi.kubevirt.io/v1beta1\n      metadata:\n        name: new-pvc-from-clone\n      spec:\n        pvc:\n          accessModes:\n          - ReadWriteOnce\n          resources:\n            requests:\n              storage: 10Gi \n        source:\n          pvc:\n            namespace: storage-vm\n            name: database-storage-pvc\n</code></pre> <p>Red Hat Images:</p> <p>https://access.redhat.com/downloads/content/479/ver=/rhel---9/9.3/x86_64/product-software</p>"},{"location":"OCPv/templates/#custom-template-creation","title":"Custom Template Creation","text":"<p>To make custom templates available to all projects, create the template in the <code>openshift</code> namespace. To view the list of available templates use the <code>oc get templates -n openshift</code> command.</p> Parameter Required Description Name Yes Display name. Template Provider Yes Specifies the author. Template Support No Denotes support level Description No Descriptive description Operating System Yes List of available VM OS; selecting an OS automatically configures the default flavor and workload Boot Source Yes Manner of access to the OS image file i.e. URL Flavor Yes Selection of machine resource configuration for both memory and processing requirements Workload Type Yes Purpose for the VM; workstation or server <pre><code>kind: Template\napiVersion: template.openshift.io/v1\nmetadata:\n  name: vm-template-example 1\n  namespace: default 2\n  annotations:\n    name.os.template.kubevirt.io/rhel8.5: Red Hat Enterprise Linux 8.0 or higher\n    description: VM template example\n    iconClass: icon-rhel\n    template.kubevirt.ui/parent-support-level: Full 3\n    template.kubevirt.ui/parent-provider: Red Hat 4\n    template.kubevirt.ui/parent-provider-url: 'https://www.redhat.com' 5\n  labels:\n    os.template.kubevirt.io/rhel8.5: 'true' 6\n    flavor.template.kubevirt.io/tiny: 'true' 7\n    workload.template.kubevirt.io/server: 'true' 8\n    vm.kubevirt.io/template: rhel8-server-tiny\n    vm.kubevirt.io/template.namespace: openshift\n    template.kubevirt.io/type: vm\nparameters:\n  - description: VM name\n    from: rhel8-[a-z0-9]{16}\n    generate: expression\n    name: NAME 9\n  - description: Name of the DataSource to clone\n    name: DATA_SOURCE_NAME 10\n    value: rhel8\n  - description: Namespace of the DataSource\n    name: DATA_SOURCE_NAMESPACE 11\n    value: openshift-virtualization-os-images\n  - description: Randomized password for the cloud-init user cloud-user\n    from: '[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}'\n    generate: expression\n    name: CLOUD_USER_PASSWORD 12\nobjects:\n  - apiVersion: kubevirt.io/v1\n    kind: VirtualMachine\n    metadata:\n      annotations:\n        vm.kubevirt.io/validations: |\n          [\n            {\n              \"name\": \"minimal-required-memory\",\n              \"path\": \"jsonpath::.spec.domain.resources.requests.memory\",\n              \"rule\": \"integer\",\n              \"message\": \"This VM requires more memory.\",\n              \"min\": 1610612736\n            }\n          ]\n      labels:\n        app: '${NAME}'\n        vm.kubevirt.io/template: rhel8-server-tiny\n        vm.kubevirt.io/template.revision: '1'\n        vm.kubevirt.io/template.version: v0.19.3\n      name: '${NAME}'\n    spec:\n      running: false\n      template:\n        metadata:\n          annotations:\n            vm.kubevirt.io/flavor: tiny\n            vm.kubevirt.io/os: rhel8\n            vm.kubevirt.io/workload: server\n          labels:\n            kubevirt.io/domain: '${NAME}'\n            kubevirt.io/size: tiny\n        spec:\n          domain:\n            cpu:\n              cores: 1\n              sockets: 1\n              threads: 1\n            devices:\n              disks: 13\n                - name: containerdisk\n                  bootOrder: 1\n                  disk:\n                    bus: virtio\n                - disk:\n                    bus: virtio\n                  name: cloudinitdisk\n              interfaces: 14\n                - masquerade: {}\n                  name: default\n                  model: virtio\n              networkInterfaceMultiqueue: true\n              rng: {}\n            machine:\n              type: pc-q35-rhel8.4.0\n            resources:\n              requests:\n                memory: 1.5Gi\n          evictionStrategy: LiveMigrate\n          networks:\n            - name: default\n              pod: {}\n          terminationGracePeriodSeconds: 180\n          volumes:\n            - name: containerdisk\n              containerDisk:\n                image: registry.redhat.io/rhel8/rhel-guest-image\n            - name: cloudinitdisk\n              cloudInitNoCloud: 15\n                userData: |\n                  #cloud-config\n                  user: cloud-user\n                  password: 1n4u-4qfw-wxsg\n                  chpasswd:\n                    expire: false\n          hostname: '${NAME}'\n</code></pre>"},{"location":"ODF/ODF/","title":"Data Foundation Storage","text":"<p>ODF can make use of external Ceph when the OCP cluster is running on:</p> <ul> <li>Bare Metal</li> <li>VMware vSphere</li> <li>OSP (Tech Preview)</li> </ul>"},{"location":"ODF/ODF/#ocs-operator","title":"OCS Operator","text":"<p>Default CPU/RAM Requirements</p>"},{"location":"ODF/ODF/#cli-deployment","title":"CLI Deployment","text":"<p>Label the respective nodes (in this example the worker nodes): <pre><code>oc label nodes \\\n-l node-role.kubernetes.io/worker= \\\ncluster.ocs.openshift.io/openshift-storage=\n</code></pre></p> <p>Get the nodes the LSO operator discovered: <pre><code>oc get localvolumediscoveryresult -n openshift-local-storage \n</code></pre></p> <p>Get the disks available on one of the nodes: <pre><code>oc describe localvolumediscoveryresult/discovery-result-worker01 -n openshift-local-storage\n</code></pre></p> <p>NOTE: Use the Device ID when adding devices to the rook-ceph operator</p> <p>Get the PVs in use: <pre><code>oc get pv -n openshift-local-storage\n</code></pre></p> <p>Get the PV on a specific node using the label option: <pre><code>oc get pv -l kubernetes.io/hostname=worker01\n</code></pre></p> <p>NOTE: Status of Available means it can be added to the rook-ceph operator</p> <p>Example <code>LocalVolume</code> manifest: <pre><code>apiVersion: local.storage.openshift.io/v1\nkind: LocalVolume\nmetadata:\n  name: expanded-local-block\n  namespace: openshift-local-storage\nspec:\n  nodeSelector:\n    nodeSelectorTerms:\n    - matchExpressions:\n        - key: cluster.ocs.openshift.io/openshift-storage\n          operator: In\n          values:\n          - \"\"\n  storageClassDevices:\n    - storageClassName: localblock\n      volumeMode: Block\n      devicePaths:\n        - /dev/disk/by-id/virtio-aaf40cdd-da3d-4d6d-9\n        - /dev/disk/by-id/virtio-fe40db23-a129-4dff-9\n        - /dev/disk/by-id/virtio-a0d9e043-c4b9-4a6d-8\n</code></pre></p> <p>Apply the yaml file with the new device paths: <pre><code>oc apply -f file.yaml \n</code></pre></p> <p>Use the <code>oc patch</code> or <code>oc edit</code> commands to increase the <code>storageDeviceSets</code> attribute  <pre><code>oc patch storagecluster/ocs-storagecluster -n openshift-storage --type json -p \\\n'[{\"op\": \"replace\", \"path\": \"/spec/storageDeviceSets/0/count\", \"value\": 2}]'\n</code></pre></p> <p>To add nodes to the cluster: <pre><code>oc label nodes -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=\n</code></pre></p> <p>Get the osd-pods: <pre><code>oc get pods -n openshift-storage -l app=rook-ceph-osd\n</code></pre></p>"},{"location":"ODF/ODF/#storage-classes","title":"Storage Classes","text":"<pre><code>oc get storageclasses -o name\n</code></pre> <pre><code>oc describe sc &lt;storageClass&gt;\n</code></pre> <p>Create storage class for Windows VMs in OCPv with mapOptions parameter: <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: windows-vms\nparameters:\n  clusterID: openshift-storage\n  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage\n  csi.storage.k8s.io/fstype: ext4\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage\n  imageFeatures: layering\n  imageFormat: \"2\"\n  pool: ocs-storagecluster-windowsblockpool\n  mounter: rbd\n  mapOptions: \"krbd:rxbounce\"\nprovisioner: openshift-storage.rbd.csi.ceph.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre></p>"},{"location":"ODF/ODF/#persistent-volume","title":"Persistent Volume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0001\nspec:\n  capacity: \n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n</code></pre> <p>NOTE: Reclaim Policy can be <code>retain</code> or <code>delete</code></p>"},{"location":"ODF/ODF/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myClaim\nspec:\n  accessModes:\n  - ReadWriteMany\n  storageClassName: ocs-storagecluster-cephfs\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>NOTE: Raw block volumes are specified with the <code>volumeMode: Block</code> parameters in the spec section</p> <p>Patch the PVC to increase the size: <pre><code>oc patch pvc/examplePVC -p '{spec:{resources:{requests:{storage: 20Gi}}}}'\n</code></pre></p> <pre><code>oc rollout latest deploymentconfig.apps.openshift.io/exampleDeployentConfig\n</code></pre>"},{"location":"ODF/ODF/#claims-as-volumes","title":"Claims as Volumes","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: dockerfile/nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: mypd\n  volume:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: myclaim\n</code></pre> <p>NOTE: The <code>volumeDevices</code> is used instead of <code>volumeMounts</code> for block devices. <pre><code>    volumeDevices:\n      - name: data\n        devicePath: /dev/xvda\n</code></pre></p>"},{"location":"ODF/ODF/#logs","title":"Logs","text":"<p>Get listing of ODF pods: <pre><code>oc get pods -n openshift-storage\n</code></pre></p> <p>Monitor Logs <pre><code>oc logs rook-ceph-mon-a-blah -n openshift-storage -c mon\n</code></pre></p> <p>OSD Logs <pre><code>oc logs rook-ceph-osd-0-blah -n openshift-storage -c osd\n</code></pre></p> <p>Rook Operator Logs: <pre><code>oc logs rook-ceph-operator-blah -n openshift-storage\n</code></pre></p> <p>RBD Logs: <pre><code>oc logs csi-rbdplugin-blah -n openshift-storage -c csi-rbdplugin\n</code></pre></p> <p>CephFS Logs: <pre><code>oc logs csi-cephfsplugin-blah -n openshift-storage -c cephfsplugin\n</code></pre></p>"},{"location":"ODF/ODF/#cluster-config-tool","title":"Cluster Config Tool","text":"<p>Ceph CLI <pre><code>oc exec -it pod/rook-ceph-operator-blah -n openshift-storage -c rook-ceph-operator -- /bin/bash\n</code></pre></p> <p>Cluster Health <pre><code>ceph -c /var/lib/rook/openshift-storage/openshift-storage.config health\n</code></pre></p> <p>Cluster Status <pre><code>ceph -c /var/lib/rook/openshift-storage/openshift-storage.config -s\n</code></pre></p> <p>Storage Usage <pre><code>ceph -c /var/lib/rook/openshift-storage/openshift-storage.config df\n</code></pre></p> <p>Tool - must-gather <pre><code>oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.7 --dest-dir=must-gather\n</code></pre></p>"},{"location":"ODF/ODF/#rook-ceph-toolbox","title":"Rook-Ceph Toolbox","text":"<p>Before installing the Toolbox, consider using the rook-ceph-operator container in the openshift-storage namespace.  Access the terminal for the pod and export the CEPH_ARGS variable and you're all set.</p> <pre><code>export CEPH_ARGS='-c /var/lib/rook/openshift-storage/openshift-storage.config'\n</code></pre> <p>To deploy the toolbox, navigate to Administration --&gt; Custom Resource Definitions --&gt; OCSInitialization --&gt; Instances.  Select the <code>ocsinit</code> instance and upddate the YAML with the following:</p> <pre><code>spec:\n  enableCephTools: true \n</code></pre> <p>To access the toolbox, navigate to Workloads --&gt; Pods --&gt; openshift-storage namespace, scroll to bottom of list of pods and select the rook-ceph-toolbox pod.  From there, select Terminal from the menu bar.</p> <p>To access via the command line:</p> <pre><code>oc rsh $(oc get pods -l app=rook-ceph-tools -o name) \n</code></pre> <p>NOTE: Make sure you are in the openshift-storage project or use the -n parameter to pass the namespace  </p>"},{"location":"ODF/ODF/#debug","title":"Debug","text":"<p>Enable debug logging for rook-ceph-operator container: <pre><code>oc edit cm rook-ceph-operator-config\n...\ndata:\n...\n  ROOK_LOG_LEVEL: DEBUG\n</code></pre></p> <p>Get the list of devices available on worker node: <pre><code>oc debug node/worker01 -- lsblk --paths --nodeps\n</code></pre></p>"},{"location":"ODF/ODF/#adding-disks-to-an-internal-cluster","title":"Adding Disks to an Internal Cluster","text":"<p>Create <code>LocalVolume</code> <pre><code>apiVersion: local.storage.openshift.io/v1\nkind: LocalVolume\nmetadata:\n  name: expanded-local-block\n  namespace: openshift-local-storage\nspec:\n  nodeSelector:\n    nodeSelectorTerms:\n    - matchExpressions:\n        - key: cluster.ocs.openshift.io/openshift-storage\n          operator: In\n          values:\n          - \"\"\n  storageClassDevices:\n    - storageClassName: localblock\n      volumeMode: Block\n      devicePaths:\n        - /dev/disk/by-id/virtio-aaf40cdd-da3d-4d6d-9\n        - /dev/disk/by-id/virtio-fe40db23-a129-4dff-9\n        - /dev/disk/by-id/virtio-a0d9e043-c4b9-4a6d-8\n</code></pre></p> <p>Patch or edit the storage cluster to increase the <code>storageDeviceSets</code>. <pre><code>oc patch storagecluster/ocs-storagecluster -n openshift-storage --type json \\\n-p '[{op: replace, path: /spec/storageDeviceSets/0/count, value: 2}]'\n</code></pre></p> <p>NOTE: This triggers the creation of new OSD pods</p>"},{"location":"ODF/ODF/#adding-nodes-to-an-internal-cluster","title":"Adding Nodes to an Internal Cluster","text":"<p>Add the new node name to the <code>LocalVolumeDiscovery</code> and <code>LocalVolumeSet</code> objects or label the nodes with the <code>node-role.kubernetes.io/worker=</code> and <code>cluster.ocs.storage.io/openshift-storage=</code> labels to make the storage cluster use the nodes automatically. <pre><code>oc label nodes -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=\n</code></pre></p>"},{"location":"ODF/backup/","title":"Backup Restore","text":""},{"location":"ODF/backup/#oadp","title":"OADP","text":"<p>Features:</p> <ul> <li>Backup - all applications and can filter the resources by type, namespace, or label.  Kubernetes objects and internal images are saved as an archive to object storage.  PVs are backed up by creating a snapshot with the CSI or cloud-native snapshot API.</li> <li>Restore - All objects in a backup can be restored or filter the objects by namespace, PV or label.</li> <li>Schedule - Can schedule backups at specified intervals.</li> <li>Hooks - can be used both pre and post backup or restore.  Restore hooks can run in an init container or in the application container.</li> </ul> <p>Data Mover is now fully supported for both containerized and VM workloads.  This allows you to move CSI volume snapshots to remote object stores.  It uses Kopia as the uploader mechanism to read the snapshot data and to write to the Unified Repository.</p> <p>File system backup supports both the Restic and Kopia libraries.  This is specified during the installation through the uploader-type flag (restic/kopia and defaults to kopia).  This cannot be changed after installation.</p> <p>Supported S3 compatible object storage providers:</p> <ul> <li>MinIO</li> <li>MCG</li> <li>AWS S3</li> <li>IBM Cloud S3</li> </ul> <p>Supported object storage providers with their own plugins:</p> <ul> <li>GCP</li> <li>Azure</li> </ul>"},{"location":"ODF/backup/#velero-cli","title":"Velero CLI","text":"<pre><code>alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'\n</code></pre>"},{"location":"ODF/backup/#configure-oadp-dpa","title":"Configure OADP DPA","text":"<p>Create OBC in the <code>openshift-adp</code> or <code>default</code> namespace.</p> <p>Get the access/secret keys, bucket name, and bucket hostname/url <pre><code>oc get configmap oadp-bucket -n openshift-adp -o jsonpath='{.data.BUCKET_NAME}{\"\\n\"}'\n</code></pre> <pre><code>oc get configmap oadp-bucket -n openshift-adp -o jsonpath='{.data.BUCKET_HOST}{\"\\n\"}'\n</code></pre> <pre><code>oc get secret oadp-bucket -n openshift-adp -o jsonpath='{.data.AWS_ACCESS_KEY_ID}{\"\\n\"}' | base64 -d\n</code></pre> <pre><code>oc get secret oadp-bucket -n openshift-adp -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}{\"\\n\"}' | base64 -d\n</code></pre></p> <p>Install the OADP Operator; accept the default values.  </p> <p>Create the <code>cloud credentials</code> secret in the openshift-adp namespace.</p> <p>Create the Data Protection Application in the OADP Operator</p> <pre><code>kind: DataProtectionApplication  \napiVersion: oadp.openshift.io/v1alpha1\nmetadata:\n  name: backup-odf\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - velero:\n        config:\n          insecureSkipTLSVerify: 'true'\n          profile: default\n          region: local\n          s3ForcePathStyle: 'true'\n          s3Url: https://s3-openshift-storage.apps.blm-ocp.hexo.lab\n        credential:\n          key: cloud\n          name: cloud-credentials\n        default: true\n        objectStorage:\n          bucket: oadp-bucket-b4991e9c-99fa-48f1-ba12-aaf6940277f4\n          prefix: oadp-backups\n        provider: aws\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      featureFlags: \n        - EnableCSI\n      podConfig:\n        resourceAllocations:\n          limits:\n            cpu: \"1\"\n            memory: 1024Mi\n          requests:\n            cpu: 200m\n            memory: 256Mi\n      defaultPlugins:\n        - openshift\n        - aws\n        - kubevirt\n        - csi\n      resourceTimeout: 10m\n  snapshotLocations:\n    - velero:\n        config:\n          profile: default\n          region: local\n        provider: aws\n</code></pre> <p>Update the VolumeSnapShotClasses ocs-storagecluster-rbdplugin-snapclass; change the deletionPolicy to <code>Retain</code> and add the <code>label</code></p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\ndeletionPolicy: Retain\ndriver: openshift-storage.rbd.csi.ceph.com\nkind: VolumeSnapshotClass\nmetadata:\n  creationTimestamp: '2024-05-09T16:22:18Z'\n  generation: 1\n  labels:\n    velero.io/csi-volumesnapshot-class: 'true'  \n  managedFields:\n...\nparameters:\n  clusterID: openshift-storage\n  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/snapshotter-secret-namespace: openshift-storage\n</code></pre>"},{"location":"ODF/backup/#backup-application-resources","title":"Backup Application Resources","text":"<p>Backup of the application resource for stateless applications: <pre><code>oc get deployment/example -o json | jq '. | del(\n    .metadata.uid,\n    .metadata.selfLink,\n    .metadata.generation,\n    .metadata.resourceVersion,\n    .metadata.creationTimestamp,\n    .metadata.managedFields,\n    .metadata.annotations.\"deployment.kubernetes.io/revision\",\n    .metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\",\n    .status\n)' &gt; deployment-backup.json\n</code></pre></p>"},{"location":"ODF/backup/#backup-of-stateful-applications","title":"Backup of Stateful Applications","text":"<p>Requirements:  </p> <ul> <li>Application must be stopped to backup the data  </li> <li>Application has a specialized backup tool (i.e. mysqldump)  </li> </ul> <p>Scale the application to zero replicas if app cannot be paused <pre><code>oc scale deployment/myApp --replicas=0\n</code></pre></p> <p>Create a job to mounts the persistent volume and copies the data to a backup location <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: backup\n  namespace: application\n  labels:\n    app: backup\nspec:\n  backoffLimit: 1\n  template:\n    metadata:\n      labels:\n        app: backup\n    spec:\n      containers:\n      - name: backup\n        image: registry.access.redhat.com/ubi8/ubi:8.4-209\n        command: \n        - /bin/bash\n        - -vc\n        - 'dnf -qy install rsync &amp;&amp; rsync -avH /var/application /opt/backup'\n        resources: {}\n        volumeMounts: \n        - name: application-data\n          mountPath: /var/application\n        - name: backup\n          mountPath: /opt/backup\n      volumes: \n      - name: application-data\n        persistentVolumeClaim:\n          claimName: pvc-application\n      - name: backup\n        persistentVolumeClaim:\n          claimName: pvc-backup\n      restartPolicy: Never\n</code></pre></p> <p>NOTE: The command section is used to install and execute rsync.  The volumeMounts section mounts both the original PVC and the PVC to hold the copy of the data.  The volume section references the PVC names.</p> <pre><code>oc apply -f backup-job.yaml\n</code></pre> <p>Scale the application back to original state when complete <pre><code>oc scale deployment/myApp --replicas=1 \n</code></pre></p>"},{"location":"ODF/backup/#using-specialized-tool","title":"Using Specialized Tool","text":"<p>Example using MYSQL <pre><code>oc exec -it deployment/mariadb -- /bin/bash\n...\n$ mysql -u ${MYSQL_USER} -p \"${MYSQL_PASSWORD}\" ${MYSQL_DATABASE}\n...\nMariaDB [example]&gt; FLUSH TABLES WITH READ LOCK ;\n...\n</code></pre></p> <p>Create a job to dump the database to another PVC: <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mariadb-backup\n  namespace: application\n  labels:\n    app: mariadb-backup\nspec:\n  backoffLimit: 1\n  template:\n    metadata:\n      labels:\n        app: mariadb-backup\n    spec:\n      containers:\n      - name: mariadb-backup\n        image: mariadb:10.5\n        workingDir: /opt/backup\n        command: \n        - /bin/bash\n        - -vc\n        - 'mysqldump -v -h \"${MYSQL_HOST}\" -P \"${MYSQL_PORT}\" -u \"root\" -p\"${MYSQL_ROOT_PASSWORD}\" --databases \"${MYSQL_DATABASE}\" &gt; backup.sql'\n        resources: {}\n        env: \n        - name: MYSQL_HOST\n          value: mariadb.application\n        - name: MYSQL_PORT\n          value: \"3306\"\n        envFrom: \n        - configMapRef:\n            name: mariadb\n        - secretRef:\n            name: mariadb\n        volumeMounts: \n        - name: backup\n          mountPath: /opt/backup\n      volumes:  \n      - name: backup\n        persistentVolumeClaim:\n          claimName: pvc-backup\n      restartPolicy: Never\n</code></pre></p> <p>NOTE: Command section is used to run the mysqldump.  The env section is used to pass the <code>mariadb</code> service information.  The envFrom is used to pass the database name and credentials.  The volumeMounts indicate where to mount the backup PVC and the volumes section references the PVC to use.</p> <pre><code>oc apply -f backup-job.yaml\n</code></pre>"},{"location":"ODF/monitoring/","title":"Monitoring","text":""},{"location":"ODF/monitoring/#monitoring-stack","title":"Monitoring Stack","text":"<p>By default the monitoring data storage is set to ephemeral.  All metrics are lost when the pods are restarted or recreated.  Block (recommended) or file storage can be configured for persistent storage for the monitoring stack.</p> <p>To update the configuration to use persistent storage, create the PVC configMap:</p> <pre><code>prometheusK8s:\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: ocs-storagecluster-ceph-rbd\n      resources:\n        requests:\n          storage: 40Gi\nalertmanagerMain:\n  volumeClaimTemplate:\n    spec:\n      storageClassName: ocs-storagecluster-ceph-rbd\n      resources:\n        requests:\n          storage: 20Gi\n</code></pre> <p>NOTE: Prometheus metrics will be retained for 7 days in this example.</p> <pre><code>oc create -n openshift-monitoring configmap cluster-monitoring-config --from-file config.yaml=metrics-storage.yml\n</code></pre> <pre><code>oc get cm cluster-monitoring-config -n openshift-monitoring -o yaml\napiVersion: v1\nkind: ConfigMap\ndata:\n  config.yaml: |\n    prometheusOperator:\n      baseImage: quay.io/coreos/prometheus-operator\n      prometheusConfigReloaderBaseImage: quay.io/coreos/prometheus-config-reloader\n      configReloaderBaseImage: quay.io/coreos/configmap-reload\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\n    prometheusK8s:\n      retention: 7d &lt;-- 1\n      baseImage: openshift/prometheus\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\n      volumeClaimTemplate:\n        spec:\n          storageClassName: ocs-storagecluster-ceph-rbd &lt;-- 2\n          resources:\n            requests:\n              storage: 40Gi &lt;-- 3\n    alertmanagerMain:\n      baseImage: openshift/prometheus-alertmanager\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\n      volumeClaimTemplate:\n        spec:\n          storageClassName: ocs-storagecluster-ceph-rbd &lt;-- 2\n          resources:\n            requests:\n              storage: 20Gi &lt;-- 3\n    nodeExporter:\n      baseImage: openshift/prometheus-node-exporter\n    kubeRbacProxy:\n      baseImage: quay.io/coreos/kube-rbac-proxy\n    kubeStateMetrics:\n      baseImage: quay.io/coreos/kube-state-metrics\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\n    grafana:\n      baseImage: grafana/grafana\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\n    auth:\n      baseImage: openshift/oauth-proxy\n    k8sPrometheusAdapter:\n      nodeSelector:\n        node-role.kubernetes.io/infra: \"\"\nmetadata:\n  name: cluster-monitoring-config\nnamespace: openshift-monitoring\n</code></pre> <ol> <li>Time units are (s)econds, (m)inutes, (h)ours, or (d)ays</li> <li>The object storage class provided by ODF is <code>ocs-storagecluster-ceph-rbd</code></li> <li>Storage values, E,P,T,G,M,K or Ei,Pi,Ti,Gi,Mi,Ki</li> </ol> <p>Check the current configuration of <code>prometheus-k8s</code> or <code>alertmanager-main</code>: <pre><code>#!/bin/bash\nset -veuo pipefail\n\n# Prometheus container volume mount spec:\noc get statefulset/prometheus-k8s \\\n  -n openshift-monitoring \\\n-o jsonpath='{.spec.template.spec.containers}' | \\\njq '.[] | select(.name == \"prometheus\") | .volumeMounts[] | select(.name == \"prometheus-k8s-db\")'\n\n# Prometheus container volume mount spec:\noc get statefulset/prometheus-k8s \\\n  -n openshift-monitoring \\\n-o jsonpath='{.spec.template.spec.volumes}' | \\\njq '.[] | select(.name == \"prometheus-k8s-db\")'\n</code></pre></p>"},{"location":"ODF/multipath/","title":"Multipath","text":"<p>Multipath Configuration</p> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 99-worker-multipathd-config\n  labels:\n    machineconfiguration.openshift.io/role: worker\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    systemd:\n      units:\n        - enabled: true\n          name: multipathd.service\n    storage:\n      files:\n        - filesystem: root\n          mode: 420\n          path: /etc/multipath.conf\n          contents:\n            inline: |\n              defaults {\n                user_friendly_names yes\n                find_multipaths yes\n              }\n              # include devices for exclusion\n              # see \n              blacklist {\n              }\n</code></pre>"},{"location":"ODF/multus/","title":"Multus","text":"<p>Multus is an advanced OpenShift feature with broad capabilities Multus is not SDN Multus is not in the network data path Multus exposes Linux network interfaces directly into Ceph pods  </p> <ul> <li>Improved latency  </li> <li>Improved bandwidth  </li> <li>Improved security  </li> </ul> <p></p>"},{"location":"ODF/multus/#planning","title":"Planning","text":"<p>Homeogeneous network interfaces across all nodes  </p> <ul> <li>All storage and worker nodes need public newtork interfaces (if applicable)  </li> <li>Only nodes hosting ODF need the cluster network interfaces (if applicable)  </li> </ul> <p>Network Diagrams should include:  </p> <ul> <li>Node types  </li> <li>Interfaces with host-level IDs, speeds, bond info  </li> <li>Pod Network  </li> <li>ODF Public Network  </li> <li>ODF Cluster Network  </li> <li>CIDRs  </li> <li>Interfaces  </li> <li>Switches  </li> <li>VLANs  </li> </ul>"},{"location":"ODF/multus/#configuring-multus","title":"Configuring Multus","text":"<p>Define a Multus Network using a Newtork Attachment Definition (NAD)</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: odf-cluster    \n  namespace: openshift-storage\nspec:\n  config: |\n    {\n       \"cniVersion\": \"0.3.1\",  \n       \"type\": \"macvlan\",\n       \"master\": \"eth1\",   \n       \"mode\": \"bridge\",\n       \"ipam\": {\n          \"type\": \"whereabouts\",\n          \"range\": \"192.168.10.0/24\"   \n       }\n    }\n</code></pre> <ul> <li>namespace can be default (unspecified) or openshift-storage</li> <li>type - macvlan (recommended) or ipvlan</li> <li>master - parent interface or interface/bond on host</li> <li>ipam - IP Address Management; whereabouts, uses OCP/Kubernetes leases, or dhcp</li> <li>range - CIDR, unique for each network</li> </ul> <p>NOTE: If there is a DNCP server, ensure Multus won't give out the same range so that multiple MAC addresses on the network can't have the same IP.</p> <p>Driver Types:  </p> <ul> <li>macvlan  <ul> <li>each connection gets a sub-interface of the parent interface with its own MAC addr  </li> <li>each interface is isolated from the host network  </li> <li>uses less CPU and privdes better throughput than LINUX bridge or ipvlan  </li> <li>almost always want bridge mode  </li> <li>near-host performance when NIC supports virtual ports/VLANs in hardware  </li> </ul> </li> <li>ipvlan  <ul> <li>each connection gets its own IP address and shares the same MAC addr  </li> <li>L2 mode is analogous to macvlan bridge mode  </li> <li>L3 mode is analogous to a router existing on the parent interface  </li> <li>L3 mode is useful for BGP, otherwise use macvlan for reduced CPU and better throughput  </li> <li>if NIC doesn't support VLANs in hardware, might be better than macvlan (but unlikely)  </li> </ul> </li> </ul>"},{"location":"ODF/multus/#pre-flight-check","title":"Pre-flight Check","text":"<p>ODF Multus Validation Tool</p> <ul> <li>Run in OCP admin shell before ODF is installed</li> <li>Checks basic Multus network connectivity</li> <li>On failure, suggests known issues/items to check for troubleshooting</li> <li>Make sure to download latest version</li> <li>Will be part of ODF official release in the future</li> </ul> <pre><code>./rook multus validation run --public-newtork odf-publid --namespace openshift-storage\n</code></pre>"},{"location":"ODF/multus/#installing-odf","title":"Installing ODF","text":"<p>When installing ODF Operator, on the Security and network screen, select the NAD definition created for Multus for the Public Network Interface or the Cluster Network Interface (or both).</p>"},{"location":"ODF/multus/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If pods aren't starting<ul> <li>problem with NAD definition</li> <li><code>oc pod describe</code> usually contains the error as an even</li> <li>NIC may not support enough virtual ports/VLANs for the additional MAC addresses</li> </ul> </li> <li> <p>If pods are starting but not communicating</p> <ul> <li>Network design/configuration may contain errors</li> <li>Newtork switch may be blocking sub-interface MAC addresses/IPs</li> <li>Switch firewalling</li> <li>System or NAD LINCU networking configuration may be blocking traffic</li> <li>SOS report will have good info to look at next.<ul> <li><code>ip_netns_exec_*_address_*</code> and <code>ip_netns_exec_*_route_*</code> from container namespace</li> </ul> </li> </ul> <p>NOTE: Some NICs support a limited number of virtual ports/VLANs (64 per physical) via hardware/driver.  Also, switches with port security can block unknown MAC addresses, they must support promiscuous (promisc) traffic.</p> </li> </ul>"},{"location":"ODF/rbac/","title":"Quotas and Permisstions","text":""},{"location":"ODF/rbac/#rbac","title":"RBAC","text":"<p>Default Roles</p> Roles Description admin Can manage all project resources, including granting permissions basic-user Read access to the project cluster-admin Superuser access to the cluster resources; full control on all projects cluster-status Get cluster status info edit Can create, change, and delete common app resources from the project self-provisioner Can create new projects; cluster role, not project role view Can view project resources, cannot modify the resources <p>Managing RBAC using CLI</p> <p>Adding a cluster admin role to a user: <pre><code>oc adm policy add -cluster-role-to user `cluster-role` `username`\n</code></pre></p> <p>NOTE: A user with <code>cluster-admin</code> role can fully manage all storage pools, classes, and PVCs in all projects.</p> <p>Adding a role to a user: <pre><code>oc policy add-role-to-user `role-name` `user-name` -n project\n</code></pre></p> <p>Use the who-can command: <pre><code>oc adm policy who-can create persistentvolumeclaims -n `namespace`\n</code></pre></p>"},{"location":"ODF/rbac/#limits-and-quotas","title":"Limits and Quotas","text":"<p>Two types of quotes, single project or for mulitple projects</p> Resource Name Description requests.storage Sum of storage requests across all PVCs in any state persistentvolumeclaims Total number of PVCs that can exist in the project <code>storageclass</code>.storageclass.storage.k8s.io/requests.storage Same as requests.storage but for a given storage class <code>storageclass</code>.storageclass.storage.k8s.io/persistentvolumeclaims Same as persistentvolumeclaims but for a given storage class"},{"location":"ODF/registry/","title":"Internal Registry","text":"<p>Storage options for the OCP Internal Registry:  </p> Storage Type ROM RWM Registry Scaled Registry Block yes No Configurable Not Configurable File yes yes Configurable Configurable Object yes yes Recommended Recommended <p>NOTE: Red Hat does not recommend using NFS server in production environments; tests have shown issues using NFS server on RHEL as the storage back end for OCP core services.</p> <p>The <code>configs.imageregistry.operator.openshift.io</code> resource controls the configuration and desired status of the container image registry.</p> <p>To configure the storage, use the storage parameter: <pre><code>storage:\n  s3:\n    bucket: &lt;bucketName&gt;\n    region: &lt;regionName&gt;\n    regionEndpoint: &lt;regionEndpointName&gt;\n</code></pre></p> <p>NOTE: These parameters only apply if the <code>spec.storage.managementState</code> is set to <code>Managed</code></p> <p>Create the <code>image-registry-private-configuration-user</code> secret in the <code>openshift-image-registry</code> namespace.</p> <ul> <li>REGISTRY_STORAGE_S3_ACCESSKEY</li> <li>REGISTRY_STORAGE_S3_SECRETKEY</li> </ul> <pre><code>oc create secret generic image-registry-private-configuration-user \\\n--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=myaccesskey \\\n--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=mysecretkey \\\n--namespace openshift-image-registry\n</code></pre> <p>Patch the <code>configs.imageregistry.operator.openshift.io cluster</code> resource. <pre><code>cat imageregistry-patch.yaml\n---\napiVersion: imageregistry.operator.openshift.io/v1\nkind: Config\nmetadata:\n  name: cluster\nspec:\n  storage:\n    managementState: Managed\n    pvc: null\n    s3:\n      bucket: noobaa-review-f9911a42-3b8a-437d-a9f9-80898d97aa03\n      region: us-east-1\n      regionEndpoint: https://s3-openshift-storage.apps.ocp4.example.com\n</code></pre></p> <pre><code>oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch-file imageregistry-patch.yaml\n</code></pre>"},{"location":"ODF/snapshot/","title":"Snapshots and Clones","text":"<p>Volumes Clones are created by referencing the source PVC in the dataSource parameter in the yaml file of a PVC: <pre><code>---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata: 1\n  name: postgresql-data-clone\n  namespace: backup-volume\n  labels:\n    app: postgresql-data-clone\nspec:\n  accessModes: 2\n    - ReadWriteOnce\n  storageClassName: ocs-storagecluster-ceph-rbd 3\n  dataSource: 4\n    kind: PersistentVolumeClaim\n    name: postgresql-data\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre></p> <p>Snapshots are created by referencing the source PVC in the persistentVolumeClaimName parameter and specifying the volume snapshot class for Ceph RBD or CephFS. <pre><code>---\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: postgresql-data \nspec:\n  volumeSnapshotClassName: ocs-storagecluster-rbdplugin-snapclass \n  source:\n    persistentVolumeClaimName: postgresql-data \n</code></pre></p> <p>Once the snapshot is created, a new PVC can be created. <pre><code>---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata: 1\n  name: postgresql-data-restore\n  namespace: backup-volume\n  labels:\n    app: postgresql-data-snapshot\nspec:\n  accessModes: 2\n  - ReadWriteOnce\n  storageClassName: ocs-storagecluster-ceph-rbd 3\n  dataSource: 4\n    apiGroup: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: postgresql-data\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre></p>"},{"location":"Openshift/Containers/","title":"OCP","text":""},{"location":"Openshift/Containers/#download-link","title":"Download Link","text":"<p>RHOCP Installers Product Documentation</p>"},{"location":"Openshift/Containers/#control-plane-components","title":"Control Plane Components","text":"Component Description etcd Distributed key-value database that stores cluster config details kube-apiserver Front-end server that exposes the Kubernates API kube-scheduled Watcher service that determins an available compute node for new pod requests"},{"location":"Openshift/Containers/#compute-plane-components","title":"Compute Plane Components","text":"Component Description kubelet Main agent on each cluster compute node, responsible for executing pod requests that come from API and scheduler kube-proxy Provides network configuration adn communication for pods on a node. cri-o CRI-O Engine, represents a small OCI-compliant runtime engine. &gt; NOTE: CRI (Container Runtime Interface) is a plug-in interface that provides configurable communication betwween kubelet and pod config requests"},{"location":"Openshift/Containers/#ocp-cli","title":"OCP CLI","text":"<p>Login <pre><code># oc login -u user -p password https://api.ocp4.example.com:6443\n</code></pre></p> <p>Retrieve the Web-Console Link: <pre><code># oc whoami --show-console\n</code></pre></p> <p>Get Kubernates control plane URL: <pre><code># oc cluster-info\n</code></pre></p> <p>Get nodes in the cluster: <pre><code># oc get nodes\n</code></pre></p> <p>List of installed operators: <pre><code># oc get clusteroperator\n</code></pre></p> <p>Get resources: <pre><code># oc get [all|*resource_type*] [ -n namespace ]\n</code></pre></p> <p>Describe a resource: <pre><code># oc describe *resouce_type* *resource_name*\n</code></pre></p> <p>Get API-Resources: <pre><code># oc api-resources [ --api-group='' ]\n</code></pre></p> <p>Deployments: <pre><code># oc get deploy\n# oc get deploy *name* -o wide\n# oc describe deployment *name*\n</code></pre></p> <p>Describe a POD: <pre><code># oc get pods\nNAME                    READY   STATUS      REASTARTS   AGE\nmyapp-77fb5cd997-xr889  1/1     Running     0           13m\n# oc describe pod myapp-77fb5cd997-xr889 \nName:               myapp-77fb5cd997-xr889 \nNamespace:          cli-resources\nPriority:           0\nService Account:    default\n...\n</code></pre></p> <p>NOTE: Use the oc get pod with the -o yaml to format the output in YAML format; this yields more details about a resource than the <code>describe</code> option.</p> <p>Explain the fields of an API resource: <pre><code># oc explain .pods.spec.containers.resources [--recursive]\n</code></pre></p> <p>Delete a resource: <pre><code># oc delete pod quotes-ui\n</code></pre></p> <p>NOTE: When deleting managed resources, such as pods, results in the automatic creation of new instances of those resources.  When a project is deleted, it deletes all the resources and applications within it.</p>"},{"location":"Openshift/Containers/#containers","title":"Containers","text":"Options Description -t --tty meaning pseudo-tty -i --interactive; standard input is kep open into the container -d --detach; run the container in the background -e environment variables <p>List images that have been downloaded locally: <pre><code># podman images\n</code></pre></p> <p>Run the container in the background: <pre><code># podman run -d -p 8080 registry.redhat.io/rhel8/httpd-24\n</code></pre></p>"},{"location":"Openshift/Containers/#appendix","title":"Appendix","text":"Acronym Description S2I Source-to-image feature uses a BuildConfig to build a container image from application source code that is stored in a Git Repo."},{"location":"Openshift/Containers/#json-formatting","title":"JSON Formatting","text":"<p>Similar to JSON styled queries, use the -o custom-columns option: <pre><code>$ oc get pods \\\n-o custom-columns=PodName:\".metadata.name\",\\\nContainerName:\"spec.containers[].name\",\\\nPhase:\"status.phase\",\\\nIP:\"status.podIP\",\\\nPorts:\"spec.containers[].ports[].containerPort\"\nPodName                  ContainerName   Phase     IP          Ports\nmyapp-77fb5cd997-xplhz   myapp           Running   10.8.0.60   &lt;none&gt;\n</code></pre></p> <p>JSONPath expression: <pre><code>$ oc get pods  \\\n-o jsonpath='{range .items[]}{\"Pod Name: \"}{.metadata.name}\n{\"Container Names:\"}{.spec.containers[].name}\n{\"Phase: \"}{.status.phase}\n{\"IP: \"}{.status.podIP}\n{\"Ports: \"}{.spec.containers[].ports[].containerPort}\n{\"Pod Start Time: \"}{.status.startTime}{\"\\n\"}{end}'\nPod Name: myapp-77fb5cd997-xplhz\nContainer Names:myapp\nPhase: Running\nIP: 10.8.0.60\nPorts:\nPod Start Time: 2023-03-15T18:45:40Z\n</code></pre></p>"},{"location":"Openshift/OCPBaremetalPI/","title":"OCP Baremetal IPI Installation","text":""},{"location":"Openshift/OCPBaremetalPI/#introduction","title":"Introduction","text":"<p>Goals:</p> <ul> <li>Enable field on OpenShift Baremetal 4.14+</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features    </li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs\u00a0    </li> <li>Send valuable feedback to Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OpenShift Baremetal</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Openshift/OCPBaremetalPI/#lab-access","title":"Lab Access","text":""},{"location":"Openshift/OCPBaremetalPI/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.1.</p> <p>We have limited resources available, but there should be enough room for about 10 virtual environments.  </p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snap-shotting, adding more cinder volumes to OCS nodes or even adding more networks via either OpenStack CLI, Horizon, or Ansible Tower.</p> Role vRAM vCPU vNIC Disk Bootstrap 20G 6 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 100GB Master 16GB 4 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 25 GB Worker 24GB 12 <ul><li>1x pxe</li><li>1x baremetal</li> 50GB 100GB OSD Custom (optional)"},{"location":"Openshift/OCPBaremetalPI/#building-your-kni-lab","title":"Building Your KNI Lab:","text":""},{"location":"Openshift/OCPBaremetalPI/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.</p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy OpenShift Baremetal Environment</p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.</p> <p></p> </li> <li> <p>The jobs can be monitored under Jobs in the left pane.\u00a0 Additional jobs will be initiated to create the project, network, instances, and bare metal bootstrap.\u00a0 Wait the deployment to finish which can take ~10-15 minutes.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p>NOTE:  Ensure the quota_vcpus value is set to at least 128 and the quota_ram value is set to at least 256000.  These the requirements needed to deploy an OCP cluster with ODF Essentials using the kni.worker.xlarge flavor.</p> <pre><code>quota_vcpus: 128\nquota_ram: 256000\nquota_instances: 30\nquota_ports: 200\n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>\"ext_network\": \"vlan1117\",\nnetworks:\n- { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"baremetal0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"storage0\", cidr: \"10.30.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"storagemgmt0\", cidr: \"10.40.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"migration0\", cidr: \"10.50.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"virtualipmi\", cidr: \"10.60.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }\n</code></pre> </li> <li> <p>Set user/project and password using the same project and password used previously when creating the project.  Submit the job.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 controllers 2 computes and 3 ceph nodes.  </p> <p>NOTE: Don't forget to include the az parameter!</p> <pre><code>\"az\": \"leaf1\"\n</code></pre> <p>With OCS (xlarge workers):  </p> <pre><code>instances:\n  - { name: \"bootstrap\", image: \"rhel-9.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master.odf\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master.odf\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master.odf\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"storage0\", net_name4: \"storagemgmt0\", net_name5: \"migration0\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"storage0\", net_name4: \"storagemgmt0\", net_name5: \"migration0\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"storage0\", net_name4: \"storagemgmt0\", net_name5: \"migration0\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  az: \"leaf1\"\n  cloud_cert: |\n    -----BEGIN CERTIFICATE-----\n    MIIEKzCCAxOgAwIBAgIUKrMQeLIpyjDKKWgtaWfyBIq9auMwDQYJKoZIhvcNAQEL\n...\n</code></pre> <p>Without ODF (normal workers): <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-9.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\naz: 'leaf1'\ncloud_cert: |\n    -----BEGIN CERTIFICATE-----\n    MIIEKzCCAxOgAwIBAgIUKrMQeLIpyjDKKWgtaWfyBIq9auMwDQYJKoZIhvcNAQEL\n...\n</code></pre></p> <p>NOTE: The <code>cloud_cert</code> value is that of the openinfraCA.pem root certificate. This is needed for the <code>ospbmc</code> service that runs in the IPMI VMs.  If this root certificate is updated, this key value must be updated or this service will not start.  </p> </li> <li> <p>Set user/project and password and submit the job using the same project and password used previously when creating the project.  Submit the job. </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to - HextupleO - configure OCP BM Bootstrap.  This playbook will configure the credentials on the bootstrap node, set the DNS and NTP servers, and copy some goodies into the /home/kni/GoodieBag directory.  Update the project name, password, and submit the job.</p> </li> <li> <p>At the end you will be getting a screen similar to this one:</p> <p></p> </li> </ol> <p>You can ssh to this IP as the user kni using the password you set in the playbook.</p>"},{"location":"Openshift/OCPBaremetalPI/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Openshift/OCPBaremetalPI/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0</p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.</p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address. You will use that IP to access your undercloud node. This will match the above output from the Ansible job (even though it does not in this document).\u00a0 You can SSH to this IP as kni using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0   </p> </li> <li> <p>Go to Routers, select the existing router (projectName_router); click the Interfaces tab and then click the Add Interface icon on the right. \u00a0 Add the baremetal0 interface in the Subnet dropdown.\u00a0 Click Submit.  </p> <p></p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#bootstrap","title":"Bootstrap","text":"<ol> <li> <p>Access the bootstrap server via ssh as the <code>kni</code> user using the IP address obtained in step 3 of the Accessing Your Project\u2019s OpenStack Environment section above and the password specified in Tower when deploying the KNI environment.</p> </li> <li> <p>You can now start deploying Openshift Baremetal (KNI) based on the standard instructions below or feel free to deploy using any other documented process.  </p> <p>INFO: Repos Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN that have been already enabled for you. If you dont want to use our local repos, delete this file:</p> <p>[kni@bootstrap ~]$ rm /etc/yum.repos.d/rhel9.repo </p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-vanilla-openshift-baremetal","title":"Deploying Vanilla Openshift Baremetal","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, we highly encourage everyone to get at least one vanilla deployment done and get familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation found here.\u00a0 Please also review the official documentation for accuracy and open any Bugzilla\u2019s against it.</p>"},{"location":"Openshift/OCPBaremetalPI/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<ol> <li> <p>Log into Bootstrap VM.  </p> <pre><code>ssh kni@&lt;bootstrapInstance&gt;  / password specified in Tower\n</code></pre> </li> <li> <p>Install the KNI Packages.  </p> <pre><code>sudo dnf install -y libvirt qemu-kvm mkisofs python3-devel jq ipmitool\n</code></pre> </li> <li> <p>Modify the user to add the libvirt group to the newly created kni user.  </p> <pre><code>sudo usermod --append --groups libvirt kni\n</code></pre> <p>NOTE: If you receive the error <code>DB version too old [0.21], expected [0.23] for domain implicit_files!</code> stop sssd with the <code>systemctl stop sssd</code> command, remove the cache files in /var/lib/sss/db directory, and restart sssd with the <code>systemctl start sssd</code> command</p> </li> <li> <p>Start and enable libvirtd; verify the daemon started successfully.</p> <pre><code>[kni@bootstrap ~]$ sudo systemctl enable libvirtd --now\n[kni@bootstrap ~]$ systemctl status libvirtd\n\u25cf libvirtd.service - Virtualization daemon\n      Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; preset: disabled)\n      Active: active (running) since Thu 2024-02-29 09:44:36 EST; 6s ago\n TriggeredBy: \u25cf libvirtd.socket\n           \u25cb libvirtd-tls.socket\n           \u25cb libvirtd-tcp.socket\n           \u25cf libvirtd-ro.socket\n           \u25cf libvirtd-admin.socket\n       Docs: man:libvirtd(8)\n         https://libvirt.org\n   Main PID: 6459 (libvirtd)\n      Tasks: 21 (limit: 32768)\n     Memory: 18.2M\n        CPU: 493ms\n     CGroup: /system.slice/libvirtd.service\n            \u251c\u25006459 /usr/sbin/libvirtd --timeout 120\n            \u251c\u25006559 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefil&gt;\n            \u2514\u25006560 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefil&gt;\n</code></pre> </li> <li> <p>Create the default storage pool and start it.</p> <pre><code>[kni@bootstrap ~]$ sudo virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images\nPool default defined\n\n[kni@bootstrap ~]$ sudo virsh pool-start default \nPool default started\n\n[kni@bootstrap ~]$ sudo virsh pool-autostart default\nPool default marked as autostarted\n</code></pre> </li> <li> <p>Configure the network using the Ansible <code>reconfig-net.yml playbook</code>.  This playbook will add the <code>provisioning</code> and <code>baremetal</code> network bridges and add eth1 and eth2 respectively to the bridges. Use the <code>nmcli con show</code> command to see the configuration before and after the playbook.</p> <pre><code>[kni@bootstrap]$ nmcli con show\nNAME         UUID                                  TYPE      DEVICE \nSystem eth0  5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0   \nSystem eth1  9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04  ethernet  eth1   \nSystem eth2  3a73717e-65ab-93e8-b518-24f5af32dc0d  ethernet  eth2   \nlo           579ab600-7690-4439-84c6-1332da9a68df  loopback  lo     \nvirbr0       691c87ce-bbcc-436f-b936-f9dfd340d9b1  bridge    virbr0 \n</code></pre> <pre><code>[kni@bootstrap]$ ansible-playbook GoodieBag/reconfig-net.yml\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match\n'all'\n\nPLAY [Configure Networks] *********************************************************************************************\n\nTASK [Gathering Facts] ************************************************************************************************\nok: [localhost]\n\nTASK [Delete eth1 and eth2 connections] *******************************************************************************\nchanged: [localhost] =&gt; (item={'conn_name': 'System eth1'})\nchanged: [localhost] =&gt; (item={'conn_name': 'System eth2'})\n\nTASK [Add Linux Bridge for provisioning network on eth1] **************************************************************\nchanged: [localhost]\n\nTASK [Add eth1 to provisioning bridge] ********************************************************************************\nchanged: [localhost]\n\nTASK [Add Linux Bridge for baremetal network on eth2] *****************************************************************\nchanged: [localhost]\n\nTASK [Add eth2 to baremetal bridge] ***********************************************************************************\nchanged: [localhost]\n\nPLAY RECAP ************************************************************************************************************\nlocalhost                  : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <pre><code>[kni@bootstrap GoodieBag]$ nmcli con show\nNAME               UUID                                  TYPE      DEVICE       \nSystem eth0        5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0         \nlo                 579ab600-7690-4439-84c6-1332da9a68df  loopback  lo           \nbaremetal          5aab580a-6db3-4f57-9fe6-7f3978d76c13  bridge    baremetal    \nprovisioning       3f8c4f9d-f391-4270-8b7c-944932f0271a  bridge    provisioning \nvirbr0             691c87ce-bbcc-436f-b936-f9dfd340d9b1  bridge    virbr0       \nbridge-slave-eth1  7079db87-436b-42a3-be35-04a11dd07135  ethernet  eth1         \nbridge-slave-eth2  e016724f-cccc-473a-a11d-76d721865ad1  ethernet  eth2         \n</code></pre> </li> <li> <p>Create a pull-secret.txt file.  In a web browser, navigate to  Install OpenShift on Bare Metal with user-provisioned infrastructure, in the Pull Secret section, click the Copy pull secret link.  </p> <p></p> </li> <li> <p>Create a pull-secret.txt file in the kni user\u2019s home directory by pasting the data just copied.  </p> <pre><code>[kni@bootstrap ~]$ vi pull-secret.txt\n{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"OTE3Nb3lcmOTU6SDA3UENGVhc2Ut9hY2obNfNViA1NWIDZGaWZMWU4ZWQxN2ExYjhTQxN2Q2NzKSK90LXJly2JMk20klD3Kk\n\u2026\nNLl2czNjckTJWdkXRQTFScwmeUxFVGNpVQkTVhBZdaVypeHcFVRZnVuRVX1j2xO==\",\"email\":\"user@redhat.com\"}}}\n:wq\n[kni@bootstrap ~]$\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#openshift-installation","title":"OpenShift Installation","text":"<p>The installation is based on the stable-4.15 version.  This will need to be updated as new versions are released.</p> <ol> <li> <p>Run the <code>get-ocp-installer.sh</code> script in the kni user\u2019s ~/GoodieBag directory.  This script will download the openshift-client installer, creates the install-config.yaml file, generates the ssh keys, updates the install-config.yaml file with the public ssh key and pull-secret, and configures DHCP and DNS.  </p> <pre><code>[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh -h \n/tmp/get-ocp-installer.sh usage:\n-v  Specify version, default is \"stable-4.15\".\n-s  Specify full path of pull-secret.txt file, default is \"~/pull-secret.txt\".\n-d  Specify directory to extract the release image in, default is current directory.\n-h  Display help/usage information.\n\n[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh\nGetting the release image name.\nDownloading the openshift-client-linux.tar.gz file.\nExtracting the openshift-client installer.\nThe openshift-baremetal-installer installed successfully.\n\nGenerating the install-config.yaml file using the ~/GoodieBag/generate-configs.yml playbook.\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match\n'all'\n\nPLAY [Generate configs] ***********************************************************************************************\n\nTASK [Gathering Facts] ************************************************************************************************\nok: [localhost]\n\nTASK [Learn kni instances in the project] *****************************************************************************\nok: [localhost]\n\nTASK [Learn ipmi instances in the project] ****************************************************************************\nok: [localhost]\n\nTASK [Show kni instances] *********************************************************************************************\nok: [localhost] =&gt; {\n   \"msg\": [\n       {\n            \"access_ipv4\": \"\",\n            \"access_ipv6\": \"\",\n            \"addresses\": {\n                \"baremetal0\": [\n                    {\n                        \"OS-EXT-IPS-MAC:mac_addr\": \"fa:16:3e:4c:0a:d5\",\n                        \"OS-EXT-IPS:type\": \"fixed\",\n                        \"addr\": \"10.20.0.16\",\n                        \"version\": 4\n                    }\n                ],\n...\nTASK [Create /home/kni/clusterconfigs/] *******************************************************************************\nok: [localhost]\n\nTASK [Copy install-config] ********************************************************************************************\nchanged: [localhost]\n\nPLAY RECAP ************************************************************************************************************\nlocalhost                  : ok=9    changed=8    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\ndone.\nIf you have a failed attempt at the installation, use the ~/GoodieBag/cleanup-ocp.sh script to reset the environment.\nGo forth and deploy openshift-baremetal-installation.\n</code></pre> </li> <li> <p>Review the contents of the ~/GoodieBag/install-configs.yaml file.  All IP addresses for the server instances should be updated to the correct IPMI IP addresses, and the pullSecrets and sshKey variables should be updated with the correct information.  </p> <pre><code>[kni@bootstrap ~]$ more GoodieBag/install-config.yaml\n\u2026\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:a7:80:64\"\n        role: master\n        rootDeviceHints:\n          deviceName: \"/dev/vda\"\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://10.30.0.129\"\n          username: \"deployodf\"\n          password: \"Passw0rd\"\n\u2026\n</code></pre> </li> <li> <p>To ensure the installation doesn\u2019t get interrupted if there is a disconnect in your ssh session, run the installation in a tmux window.  The tmux utility was installed by the get-ocp-installer.sh script.  </p> <pre><code>[kni@bootstrap ~]$ tmux\n</code></pre> </li> <li> <p>Deploy OpenShift using the openshift-baremetal-install command.  </p> <pre><code>[kni@bootstrap ~]$ openshift-baremetal-install --dir ~/clusterconfigs --log-level debug create cluster\n</code></pre> </li> <li> <p>You can monitor your deployment in another window.  </p> <pre><code>[kni@bootstrap ~]$ sudo virsh list\n Id   Name                        State\n -------------------------------------------\n 1    kni-test2-ntpgv-bootstrap   running\n</code></pre> <p>To display the console output of the boostrap-vm (ctrl + <code>]</code> to exit): <pre><code>[kni@bootstrap ~]$ sudo virsh console &lt;bootstrap-vm&gt;\n</code></pre></p> <p>View the status of the servers deployed through OpenStack: <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc\n(kni-test2) [kni@bootstrap ~]$ openstack server list --insecure\n+--------------------------------------+------------------+---------+----------\n| ID                                   | Name             | Status  | Networks                                                                        | Image          | Flavor |\n+--------------------------------------+------------------+---------+----------\n| d339db1a-4e50-4c69-8063-d7d92bfcc190 | ipmi_kni-master1 | ACTIVE  | virtualipmi=10.30.0.132                                                         | virtualipmi    |        |\n| ccfaf78c-80f5-4b01-b3ee-ab0fd82a07c0 | ipmi_kni-master2 | ACTIVE  | virtualipmi=10.30.0.85                                                          | virtualipmi    |        |\n| 14a636dd-3709-48e9-a6e2-be689d434ac7 | ipmi_kni-master3 | ACTIVE  | virtualipmi=10.30.0.49                                                          | virtualipmi    |        |\n| 77fda0c4-8a88-4970-806e-540e3946dd3a | ipmi_kni-worker1 | ACTIVE  | virtualipmi=10.30.0.45                                                          | virtualipmi    |        | \n| 80c13e57-2869-4473-99e2-c4748daaf84c | ipmi_kni-worker2 | ACTIVE  | virtualipmi=10.30.0.147                                                         | virtualipmi    |        |\n| 32f1de18-8bdb-4bdf-bdc3-57ddb0e522d8 | ipmi_kni-worker3 | ACTIVE  | virtualipmi=10.30.0.210                                                         | virtualipmi    |        |\n| 85369613-44ec-47fa-9a9b-2476ca9278e8 | kni-master1      | ACTIVE  | baremetal0=10.20.0.8; provisioning0=10.10.0.200                                 | pxeboot        |        |\n| 438088a3-0182-4f9f-bcce-485bce5973d3 | kni-master2      | ACTIVE  | baremetal0=10.20.0.119; provisioning0=10.10.0.160                               | pxeboot        |        |\n| 0239711f-e661-47f3-bc71-eea60eec8763 | kni-master3      | ACTIVE  | baremetal0=10.20.0.100; provisioning0=10.10.0.209                               | pxeboot        |        |\n| 26798b59-1d6c-4d93-b51a-26b59c725a33 | kni-worker1      | SHUTOFF | baremetal0=10.20.0.77; provisioning0=10.10.0.91                                 | pxeboot        |        |\n| 3f11240b-5806-40f9-92d4-82a58df4053b | kni-worker2      | SHUTOFF | baremetal0=10.20.0.236; provisioning0=10.10.0.94                                | pxeboot        |        |\n| 75015e47-7210-4bb8-8f45-8da5f3f78829 | kni-worker3      | SHUTOFF | baremetal0=10.20.0.124; provisioning0=10.10.0.109                               | pxeboot        |        || 27c33f7a-ea95-4cd6-aec9-691fbcfe27c1 | bootstrap        | ACTIVE  | baremetal0=10.20.0.122; vlan1117=10.9.65.140; provisioning0=10.10.0.45 | rhel82-update1 |        |\n+--------------------------------------+------------------+---------+----------\n</code></pre></p> <p>Other commands that will come in handy in the later state of the deployment:</p> <pre><code>[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n[kni@bootstrap ~]$ oc get clusteroperators\n[kni@bootstrap ~]$ oc get nodes\n</code></pre> <p>We hope it works! If it has, then at the end of the deployment you will see something like this:</p> <pre><code>INFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.kni-test2.hexo.lab\nINFO Login to the console with user: \"kubeadmin\", and password: \"mv22Z-Tv2Zb-pTFgq-xAvki\"\nDEBUG Time elapsed per stage:\nDEBUG     Infrastructure: 29m15s\nDEBUG Bootstrap Complete: 9m54s\nDEBUG  Bootstrap Destroy: 13s\nDEBUG  Cluster Operators: 34m31s\nINFO Time elapsed: 1h13m54s\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#accessing-your-environment","title":"Accessing Your Environment","text":"<p>There should be a few ways to access your environment including using sshuttle or a jumphost.</p>"},{"location":"Openshift/OCPBaremetalPI/#using-sshuttle","title":"Using sshuttle","text":"<p>The OpenShift Console is not available on the VPN network the lab environment is deployed on but rather the private network of your project.  To access the console, you can use the sshuttle utility which allows you to create a VPN connection using an ssh connection to the bootstrap server.  You need root access on the client machine but not on the bootstrap server.  The sshuttle utility requirements are python 2.3 or higher which are already installed on the bootstrap server. </p>"},{"location":"Openshift/OCPBaremetalPI/#linux-client-installation","title":"Linux Client Installation","text":"<ol> <li> <p>Install using the dnf command from the EPEL repository.  </p> <pre><code>$ dnf install -y sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required to use sshuttle.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.  </p> <pre><code>$ sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 --dns --to-ns 10.20.0.10 [ -vv ]\n</code></pre> </li> <li> <p>Access your OpenShift Console in your browser using the link:  </p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#macos-client-installation","title":"MacOS Client Installation","text":"<ol> <li> <p>If not installed already, install homebrew.  Open a terminal window and grab the install.sh script from GitHub and run it.  Wait for the command to finish.  If you are prompted to enter a password, enter your Mac user\u2019s login password and press ENTER.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Make the brew command available inside the terminal window.</p> <pre><code>\"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\n. ~/.zprofile\n</code></pre> </li> <li> <p>Install sshuttle.</p> <pre><code>$ brew install sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.</p> <pre><code>~ % sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n[local sudo] Password: \nkni@172.20.17.124's password: \nc : Connected to server.\n</code></pre> <p>NOTE: As traffic is generated on the VPN connection, warning messages will be displayed in the terminal.</p> <p>Example:</p> <p>s: warning: closed channel 7 got cmd=TCP_DATA len=517  c : warning: closed channel 11 got cmd=TCP_STOP_SENDING len=0   s: warning: closed channel 11 got cmd=TCP_DATA len=517   s: warning: closed channel 11 got cmd=TCP_EOF len=0</p> </li> <li> <p>Access your OpenShift Console in your browser using the link:</p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> <p>You will also need to add an entry in your /etc/hosts file for the DNS resolution.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#using-a-jumphost-from-openstack","title":"Using a JumpHost from OpenStack","text":"<ol> <li> <p>Access your OpenStack environment, select Compute-&gt;Instances.  Click the Launch Instance icon on the right.  Enter the Instance Name, Description, AZ, and Count.  Click Next.  </p> <p>NOTE: At this time, the procedures below will yield issues with accessing the jumphost due to a known issue with injecting the ssh keys. Please use the procedures under Using sshuttle above.</p> </li> <li> <p>On the Source screen, make sure Select Boot Source is Image and Create New Volume is No.  In the Available list of images, find fedora-cloud37 and click the up arrow to the right of the entry to move it up to the Allocated section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Flavor screen, find t2.medium in the list of Available flavors, click the up arrow to the right of the entry to move it to Allocated.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Networks screen, add the vlan1117 and baremetal0 entries in the Available list to the Allocated list.  Scroll to the bottom, click Next.</p> </li> <li> <p>Click Next in the lower right on the Network Ports screen.  On the Security Groups screen, click the down arrow next to the default security group in the Allocated section to move it to the Available section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Key Pair screen, import your ssh keys using the Import Key Pair icon.  Remove the hextupleo_pub_key key from Allocated by clicking the down arrow to the far right.  Once complete, click the Launch Instance in the lower right as the remaining sections are not required or needing to be updated.</p> </li> <li> <p>Ssh to environment and enable x11 and maybe firefox/chrome if you\u2019d like.  </p> <p>``` [fedora@fedora-jumpbox ~]$ sudo dnf -y groupinstall gnome [fedora@fedora-jumpbox ~]$ sudo dnf -y group install \"Basic Desktop\" GNOME [fedora@fedora-jumpbox ~]$ sudo systemctl set-default graphical.target Removed /etc/systemd/system/default.target. Created symlink /etc/systemd/system/default.target \u2192 /usr/lib/systemd/system/graphical.target. [fedora@fedora-jumpbox ~]$ sudo -i [root@fedora-jumpbox ~]# passwd fedora Changing password for user fedora. New password:  BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word Retype new password:  passwd: all authentication tokens updated successfully. [root@fedora-jumpbox ~]# reboot</p> </li> <li> <p>Adding DNS is optional and not a requirement.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-odf-storage","title":"Deploying ODF (storage)","text":"<p>Official docs -&gt; https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/index </p> <ol> <li> <p>Install local storage operator; click Operators-&gt;OperatorHub.  Search for local.  Select the Local Storage Operator, in the pop up window, click the Install icon.  </p> <p></p> </li> <li> <p>Keep the defaults; scroll to the bottom of the screen and click the Install icon.  </p> <p></p> </li> <li> <p>Install ODF Operator; click Operators-&gt;OperatorHub, search for Data Foundation.  Select the OpenShift Data Foundation Operator, in the pop-up window keep the defaults, scroll to the bottom and click Install.  </p> <p></p> </li> <li> <p>Once the operator has successfully been installed, the GUI will indicate that a change has occurred and to refresh.  Create the StorageSystem; select Storage-&gt;Data Foundation.  Click the Storage System tab; click the Create StorageSystem icon on the right.  </p> <p></p> </li> <li> <p>For the Backing storage type, select Create a new StorageClass using local storage devices.  Click Next.  </p> <p></p> </li> <li> <p>The worker nodes have been preconfigured with 100GB secondary drives.  </p> <pre><code>[kni@bootstrap ~]$ ssh core@kni-worker1\nRed Hat Enterprise Linux CoreOS 412.86.202301191053-0\nPart of OpenShift 4.12, RHCOS is a Kubernetes native operating system\nmanaged by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\nhttps://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html\n\n[core@kni-worker1 ~]$ sudo fdisk -l | grep vdb\nDisk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors\n</code></pre> </li> <li> <p>On the Create local volume set screen, enter a name for the volume set.  In the example below, localvolumes is used.  Verify the Disks on all nodes (3 node) is selected in the Filter disks by section and All is selected for the Disk type.  Click the Next icon.  </p> <p></p> </li> <li> <p>A pop up confirmation window will display providing additional information to consider if this is a stretched cluster.   Click the Yes icon if the settings are correct to create the LocalVolumeSet.  </p> </li> <li> <p>On the Capacity and nodes screen, accept the defaults and click Next.  </p> <p></p> </li> <li> <p>On the Security and network screen, accept the default of SDN.  Click Next.</p> </li> <li> <p>On the Review and create screen, review the configuration and click Next if correct to create the Storage System.</p> </li> <li> <p>It will take several minutes to create the local volumes and configure the storage.  You can monitor the progress on the Storage-&gt;Data Foundation-&gt;StorageSystems screen.  Click the ocs-storage-cluster-storagesystem link to access the Overview dashboard.  As the system configures the storages, messages will be logged in the Activity section.  Once complete, the Status for Storage Cluster and Data Resiliency should be green.  </p> <p></p> </li> <li> <p>To verify local storage has been create, click the BlockPools menu item along the top; click the ocs-storagecluster-cephblockpool link.  In the Inventory widget, click the links to view the available Storage Classes or the Persistent Volume Claims.  </p> <p></p> <p>Storage Class:</p> <p></p> <p>PersistentVolumeClaims:</p> <p></p> </li> <li> <p>The local storage and volume claims can be viewed with the CLI.  </p> <pre><code>[kni@bootstrap ~]$ oc get all -n openshift-local-storage\nNAME                                         READY   STATUS    RESTARTS   AGE\npod/diskmaker-discovery-2xnxv                2/2     Running   0          3m46s\npod/diskmaker-discovery-9vjbf                2/2     Running   0          3m49s\npod/diskmaker-discovery-ff2cc                2/2     Running   0          4m2s\npod/diskmaker-manager-c54j8                  2/2     Running   0          2m10s\npod/diskmaker-manager-prgbh                  2/2     Running   0          2m10s\npod/diskmaker-manager-xtw58                  2/2     Running   0          2m10s\npod/local-storage-operator-9bc77c9cf-rzqjt   1/1     Running   0          32m\n\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/local-storage-discovery-metrics   ClusterIP   172.30.23.42    &lt;none&gt;        8383/TCP   11m\nservice/local-storage-diskmaker-metrics   ClusterIP   172.30.248.10   &lt;none&gt;        8383/TCP   2m10s\n\nNAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/diskmaker-discovery   3         3         3       3            3           &lt;none&gt;          11m\ndaemonset.apps/diskmaker-manager     3         3         3       3            3           &lt;none&gt;          2m10s\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/local-storage-operator   1/1     1            1           32m\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/local-storage-operator-9bc77c9cf   1         1         1       32m\n\n[kni@bootstrap ~]$ oc get pv\nNAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\nlocal-pv-2cc6fb0    100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-3faa9a90   100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-e2c0ad97   100Gi      RWO            Delete           Available           localvolumes            114s     3m45s\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-ocpv","title":"Deploying OCPv","text":""},{"location":"Openshift/OCPBaremetalPI/#deleting-your-project","title":"Deleting Your Project","text":"<ol> <li> <p>To remove your entire project, use the Template in Ansible Automation Platform.</p> <p>Ansible Automation Platform</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Hextupleo - delete my project.</p> <p></p> </li> <li> <p>Update the project_name: and project_password: fields.  Click the Next icon in the lower left.</p> </li> <li> <p>Review the information on the Preview screen.  When ready to move forward, click the Launch icon in the lower left corner.  The Jobs Output screen for the new job will display ongoing output as the system progresses through the Ansible playbook.  You can monitor to completion or just check back to ensure your Job finishes successfully.</p> <p></p> </li> <li> <p>Verify your job finished successfully in the Jobs listing.</p> <p></p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#troubleshooting","title":"Troubleshooting","text":"<p>Getting NTP applied for workers and masters.  </p> <pre><code>[kni@bootstrap ~]$ wget \n[kni@bootstrap ~]$ wget \n\n[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n\n[kni@bootstrap ~]$ oc apply -f 99_workers-chrony-configuration.yaml\n[kni@bootstrap ~]$ oc apply -f 99_masters-chrony-configuration.yaml\n</code></pre> <p>Check status of IPMI.  </p> <pre><code>kni@bootstrap ~]$ ipmitool -I lanplus -H 10.30.0.129 -U deployodf -P Passw0rd chassis power status\n</code></pre>"},{"location":"Openshift/OCPBaremetalPI/#appendix","title":"Appendix","text":"<p>The get-ocp-installer.sh script is provided as a quick method to complete the prep work before installing OpenShift.  If you want to execute manually, the commands are provided below for your reference.  </p> <p>Set the environment variables and download the openshift-client-linux.tar.gz file.  </p> <pre><code>[kni@bootstrap ~]$ export VERSION=latest-4.12\n[kni@bootstrap ~]$ export RELEASE_IMAGE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/release.txt | grep 'Pull From: quay.io' | awk -F ' ' '{print $3}')\n[kni@bootstrap ~]$ export cmd=openshift-baremetal-install\n[kni@bootstrap ~]$ export pullsecret_file=~/pull-secret.txt\n[kni@bootstrap ~]$ export extract_dir=$(pwd)\n[kni@bootstrap ~]$ curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/openshift-client-linux.tar.gz | tar zxvf - oc\noc\n[kni@bootstrap ~]$ sudo cp oc /usr/local/bin\n[kni@bootstrap ~]$ oc adm release extract --registry-config \"${pullsecret_file}\" --command=$cmd --to \"${extract_dir}\" ${RELEASE_IMAGE}\n[kni@bootstrap ~]$ sudo cp openshift-baremetal-install /usr/local/bin\n</code></pre> <p>Verify the installer file has been downloaded.  </p> <pre><code>[kni@bootstrap ~]$ ls\nGoodieBag  nohup.out  oc  openshift-baremetal-install  pull-secret.txt\n</code></pre> <p>Create or generate install-config.yaml.</p> <p>For your convenience, there is an ansible playbook inside a Goodiebag directory that helps gather mac information for the OCP nodes. It uses openstack APIs to gather the information. Install the required packages first Note: This step would typically not be performed on different hardware.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf -y install ansible python3-shade python3-openstackclient\n</code></pre> <p>Generate the install-config.yaml  </p> <pre><code>[kni@bootstrap ~]$ ansible-playbook GoodieBag/generate-configs.yml\n</code></pre> <p>The file will look similar to this:  </p> <pre><code>[kni@bootstrap ~]$ cat GoodieBag/install-config.yaml \napiVersion: v1\nbasedomain: hexo4.lab\nmetadata:\n  name: \"kni-test\"\nnetworking:\n  machineCIDR: 10.20.0.0/24\n  networkType: OVNKubernetes\ncompute:\n  - name: worker\n     replicas: 3\ncontrolPlane:\n  name: master\n  replicas: 3\n  platform:\n    baremetal: {}\nplatform:\n  baremetal:\n    apiVIP: &lt;api-ip&gt;\n    ingressVIP: &lt;wildcard-ip&gt;\n    provisioningNetworkCIDR: &lt;CIDR&gt;\n    hosts:\n      - name: \"kni_worker3\"\n        bootMACAddress: \"fa:16:3e:0f:21:4d\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker2\"\n        bootMACAddress: \"fa:16:3e:5c:94:dd\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker1\"\n        bootMACAddress: \"fa:16:3e:86:ce:e8\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master3\"\n        bootMACAddress: \"fa:16:3e:2e:7a:86\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master2\"\n        bootMACAddress: \"fa:16:3e:23:03:f5\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:43:75:71\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\npullSecret: '&lt;pull_secret&gt;'\nsshKey: '&lt;ssh_pub_key&gt;'\n</code></pre> <p>You must update the ipmi IP addresses for each of the server instances, the pullSecret, and sshKey variables.  Get the ipmi addresses using the openstack server command.  First, you need to source the variables in the ~/GoodieBag/rc file.   <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc   &lt; - replace with your projectname\n(kni-test) [kni@bootstrap ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"      \" $8 }'\nipmi_kni_worker3      virtualipmi=10.30.0.106\nipmi_kni_worker2      virtualipmi=10.30.0.122\nipmi_kni_worker1      virtualipmi=10.30.0.139\nipmi_kni-master3      virtualipmi=10.30.0.37\nipmi_kni-master2      virtualipmi=10.30.0.150\nipmi_kni-master1      virtualipmi=10.30.0.132\n</code></pre> <p>Update each server\u2019s host section replacing the X.X.X.X with the correct IP address.</p> <pre><code>hosts:\n      - name: \"kni_worker3\"\n      bootMACAddress: \"fa:16:3e:0f:21:4d\"\n      role: worker\n      hardwareProfile: unknown\n      bmc:\n        address: \"ipmi://X.X.X.X\"  \u2190 replace with 10.30.0.106\n        username: \"kni-test\"\n        password: \"changeme\"\n</code></pre> <p>Update the pullSecret variable at the bottom of the file with the pull-secret.txt file contents you created in the Openshift Installation.  </p> <pre><code>(kni-test) [kni@bootstrap ~]$ cat pull-secret.txt \n{\"auths\":{\"cloud.openshift.com\":{\"auth\":.....\n\npullSecret:'{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3BlbnNoaWHVXQkdaVy1jTVhZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NMkRRN1RTWQxN2ExYjhmOTU6TQxN2Q2NzViOTESDA3UEFGOFNA4QzJTQVOTDNDUFU5SlJY8UV0pSWl\n\u2026\nnZnVzNfN3NDA1NWIyMWU4Zjckx2ZGJHVVzFJd1FNLXRmeUxFcFVRZnVuRTJWdkNpo3MEw3MlJGl2czNkRzLUdpeBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}'\n</code></pre> <p>Update the sshKey variable with your ssh public key.  If you have not generated it, use the ssh-keygen command.  </p> <pre><code>[kni@bootstrap ~]$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/kni/.ssh/id_rsa): &lt;enter&gt;\nEnter passphrase (empty for no passphrase): &lt;enter&gt; \nEnter same passphrase again: \nYour identification has been saved in /home/kni/.ssh/crapid_rsa.\nYour public key has been saved in /home/kni/.ssh/crapid_rsa.pub.\nThe key fingerprint is:\nSHA256:CVnMl7uiOYLFK1jNkWQZ7M29cbsNhdP13Hfvbsy5Yfg kni@bootstrap\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   ..o o.  .     |\n|    =  oo o   .  |\n|   + +o. . + . o.|\n|    + o.o.= o   *|\n|   + .  S+ =    +|\n|  . =   o +   . .|\n| o o . o . + . *.|\n|. o o +   . . o.*|\n|   . . .       Eo|\n+----[SHA256]-----+\n[kni@bootstrap ~]$ cat .ssh/id_rsa.pub \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3PtY3V32PbEbXpuVaaPV\n\u2026\nfionHKM8gdFbXo8yWqCTdT3HuMs9gjjjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap\n</code></pre> <p>Copy the id_rsa.pub contents and paste as the sshKey.</p> <pre><code>sshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3Pt  \n\u2026\nfjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap'\n</code></pre> <p>Openshift Baremetal IPI also requires DHCP and DNS to be configured.  Copy the respective files from the GoodieBag directory to /etc.  Enable and start the dnsmasq daemon.  </p> <pre><code>[kni@bootstrap ~]$ sudo cp GoodieBag/hosts /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/kni.dns /etc/dnsmasq.d/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf.upstream /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf /etc/\n[kni@bootstrap ~]$ sudo systemctl enable dnsmasq\nCreated symlink /etc/systemd/system/multi-user.target.wants/dnsmasq.service \u2192 /usr/lib/systemd/system/dnsmasq.service.\n[kni@bootstrap ~]$ sudo systemctl start dnsmasq\n</code></pre> <p>If this is not your first attempt at installing OpenShift, cleanup the remnants of the first attempt. </p> <pre><code>[kni@bootstrap ~]$ for i in $(sudo virsh list | tail -n +3 | grep bootstrap | awk {'print $2'})\ndo\n    sudo virsh destroy $i;\n    sudo virsh undefine $i;\n    sudo virsh vol-delete $i --pool default;\n    sudo virsh vol-delete $i.ign --pool default;\ndone\n[kni@bootstrap ~]$ rm -rf ~/clusterconfigs/auth ~/clusterconfigs/terraform* ~/clusterconfigs/tls ~/clusterconfigs/metadata.json\n</code></pre> <p>Create the clusterconfigs working directory in the kni home directory.</p> <pre><code>[kni@bootstrap ~]$ mkdir ~/clusterconfigs\n</code></pre>"},{"location":"Openshift/cloudinstallation/","title":"AWS OCP 4 Cloud Installation","text":"<p>This page will guide you through the IPI installation of OCP 4 on AWS.</p>"},{"location":"Openshift/cloudinstallation/#pre-requisites","title":"Pre-requisites","text":"<p>Download the OpenShift installer from the Install on Bare Metal with user-provisioned infrastructure web-site.  Select the appropriate OS and architecture.  Once you have the file locally, decompress and extract the tar file. Place the ocp-installer command in a location that is in your PATH for easy access.</p> <p>When running the ocp-install command, the secret json data is used when connecting to AWS.  Save this data in the user\u2019s home directory so it can be copied and pasted when prompted.  To pull your secret json structure, click the <code>Copy pull secret</code> link.  Paste the contents into the pull-secret.txt file and save the contents in the user\u2019s home directory.</p> <p>Download the <code>oc</code> CLI binary to use after the installation is complete.  Once you have the file locally, decompress and extract the tar file. Place the <code>oc</code> command in a location that is in your PATH for easy access.</p>"},{"location":"Openshift/cloudinstallation/#demo-environment","title":"Demo Environment","text":"<p>Request the AWS Blank Open Environment from the Red Hat Demo Platform.  Once the environment is built, you will receive all the pertinent information needed to configure the AWS CLI and access the AWS Console.</p> <p>Once the demo environment has been provisioned, configure the AWS environment.</p> <pre><code>aws configure\nAWS Access Key ID []: &lt;accessKey from demo.redhat.com&gt;\nAWS Secret Access Key []: &lt;secretKey from demo.redhat.com&gt;\nDefault region name [us-east-1]: &lt;region from demo.redhat.com&gt;\nDefault output format [json]: \n</code></pre>"},{"location":"Openshift/cloudinstallation/#create-install-configyaml","title":"Create install-config.yaml","text":"<p>Use the <code>openshift-install</code> command to create a default yaml file.  Use the up/down arrow keys to select an option when prompted.  When prompted for <code>Pull Secret</code>, copy and paste your pull secret json data that was obtained in the pre-requesite section.  The <code>install-config.yaml</code> file will be created in the <code>ocp_install</code> directory, or the directory specified with the \u2013dir option.  You can modify this file if you need to change the architecture type, number of replicas, or networking information.  </p> <pre><code>./openshift-install --dir ocp_install --log-level debug create install-config\n</code></pre>"},{"location":"Openshift/cloudinstallation/#install-ocp-4","title":"Install OCP 4","text":"<p>Ensure that you are not in the <code>ocp_install</code> directory and that the <code>openshift-install</code> command is in your path.  Run the installation using the <code>openshift-install</code> command.  You can specify the log-level as info, warn, debug, or error.</p> <pre><code>openshift-install --dir ocp_install --log-level debug create cluster\n</code></pre> <p>Once the installation is complete, generally around 35-40 minutes, the link to the OpenShift console is provided along with the KUBECONFIG file to export when using the <code>oc</code> command.</p> <pre><code>...\nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console, image-registry, monitoring, openshift-samples are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console, monitoring, openshift-samples are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console, monitoring are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console, monitoring are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operators authentication, console are not available \nDEBUG Still waiting for the cluster to initialize: Cluster operator authentication is not available \nDEBUG Cluster is initialized                       \nINFO Checking to see if there is a route at openshift-console/console... \nDEBUG Route found in openshift-console namespace: console \nDEBUG OpenShift console route is admitted          \nINFO Install complete!                            \nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/Users/bmclaren/ocp.0124/ocp_install/auth/kubeconfig' \nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.blm-aws.sandbox2258.opentlc.com \nINFO Login to the console with user: \"kubeadmin\", and password: \"gqYnD-yXtEL-yiq6f-xJofS\" \nDEBUG Time elapsed per stage:                      \nDEBUG            cluster: 4m52s                    \nDEBUG          bootstrap: 38s                      \nDEBUG Bootstrap Complete: 10m57s                   \nDEBUG                API: 1m48s                    \nDEBUG  Bootstrap Destroy: 2m31s                    \nDEBUG  Cluster Operators: 10m17s                   \nINFO Time elapsed: 29m27s                         \n</code></pre>"},{"location":"Openshift/cloudinstallation/#create-dedicated-odf-nodes","title":"Create Dedicated ODF Nodes","text":""},{"location":"Openshift/cloudinstallation/#create-machine-sets-for-odf","title":"Create Machine Sets for ODF","text":"<p>Login to the ODF Console GUI.  Navigate to Compute, Machine Sets.  Download each of the existing machine sets.  These will be used as templates for the new machine sets.  Click the machine name, select YAML from the top menu bar and then click the Download link in the lower right corner.  Alternatively, you can use the <code>oc get machineset/&lt;machineSetName&gt; -n openshift-machine-api -o yaml</code> command and save the output to a file.  </p> <p>For each machine YAML files, make the following edits:</p> <ul> <li> <p>Delete the highlighted line: <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  annotations:\n    capacity.cluster-autoscaler.kubernetes.io/labels: kubernetes.io/arch=amd64\n    machine.openshift.io/GPU: \"0\"\n    machine.openshift.io/memoryMb: \"16384\"\n    machine.openshift.io/vCPU: \"4\"\n  creationTimestamp: \"2024-01-11T18:40:17Z\"\n  generation: 1\n  labels:\n    machine.openshift.io/cluster-api-cluster: blm-aws-grv6f\n  name: blm-aws-grv6f-worker-us-east-2a\n  namespace: openshift-machine-api\n  resourceVersion: \"25011\"\n  uid: 3ba3aa6e-f751-414c-bc8a-5c2e44b04bac\n</code></pre></p> </li> <li> <p>Modify the replicas from 1 to 0 <pre><code>spec:\n  replicas: 0\n</code></pre></p> </li> <li> <p>Modify the name from worker to storage: <pre><code>name: blm-aws-grv6f-storage-us-east-2a\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: blm-aws-grv6f\n      machine.openshift.io/cluster-api-machineset: blm-aws-grv6f-storage-us-east-2a\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: blm-aws-grv6f\n        machine.openshift.io/cluster-api-machineset: blm-aws-grv6f-storage-us-east-2a\n</code></pre></p> </li> <li> <p>Update the metadata section to add the labels and the taints: <pre><code>spec:\n  replicas: 0\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: blm-aws-grv6f\n      machine.openshift.io/cluster-api-machineset: blm-aws-grv6f-storage-us-east-2a\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: blm-aws-grv6f\n        machine.openshift.io/cluster-api-machineset: blm-aws-grv6f-storage-us-east-2a\n        node-role.kubernetes.io/infra: ''\n        cluster.ocs.openshift.io/openshift-storage: ''\n      taints:\n        - effect: NoSchedule\n          key: node.ocs.openshift.io/storage\n          value: true\n</code></pre></p> </li> <li> <p>Update the instanceType to m5a.4xlarge; this satisfies the minimum requirements for ODF and is the lowest cost EC2 instance. <pre><code>          iamInstanceProfile:\n            id: blm-aws-grv6f-worker-profile\n          instanceType: m6i.4xlarge\n</code></pre></p> </li> <li> <p>Remove the status section from the bottom of the file: <pre><code>status:\n  availableReplicas: 1\n  fullyLabeledReplicas: 1\n  observedGeneration: 1\n  readyReplicas: 1\n  replicas: 1\n</code></pre></p> </li> </ul> <p>Once all the edits are complete to all downloaded yaml files, access the OpenShift console, navigate to Compute, Machine Sets, click the <code>Create MachineSet</code> icon.  Copy/paste the contents of the first file and click <code>Create</code>.  Rinse and repeat for each of the yaml files.</p> <p>After all three new machine sets are created, (one per AWS zone), modify the Machine Set Count to 1 to create the new EC2 instance in each zone.  Click the elipses on the far right of the machine set, select Edit Machine Count.  In the dialog box, set the count to 1, click the <code>Save</code> icon.</p> <p>NOTE: Monitor the Compute , Machines section to get the status of the builds.  Once they are complete, the status will be Provisioned as node.</p>"},{"location":"Openshift/cloudinstallation/#install-odf-and-create-storage-cluster","title":"Install ODF and Create Storage Cluster","text":"<p>To install the ODF Operator, select Operator Hub, Operators from the left pane.  In the search text box, input ODF.  Select OpenShift Data Foundation.  On the pop-up Window, click <code>Install</code>.  On the install Operator window, scroll to the bottom and click the <code>Install</code> icon.</p> <p>After a few minutes, the installation will complete and a message will be displayed with the <code>Create StoraegSystem</code> icon; click it to create the ODF Storage Cluster.</p> <p>On the Backing Storage screen, accept the defaults and click <code>Next</code>.  </p> <p>On the Capacity and nodes screen, you can modify the size of the volumes using the drop-down menu.  Select the nodes that are labeled as Infra.  There should be three.  Scroll to the bottom of the screen and click teh Taint Nodes checkbox.  Click the <code>Next</code> icon.</p> <p>On the Security and network screen, take the defaults and click <code>Next</code>.</p> <p>Review the final configuration on the Review and create screen.  Click <code>Create StorageSystem</code> ot create or <code>Back</code> to make modifications.</p> <p>NOTE: It will take time to create the StorageSystem.  Once it is complete, the status on the Overview page will show green for Storage.</p>"},{"location":"Openshift/cloudinstallation/#post-install-configuration","title":"Post Install Configuration","text":""},{"location":"Openshift/cloudinstallation/#default-storageclass","title":"Default StorageClass","text":"<p>Update the default storage class to ocs-storagecluster-ceph-rbd.  Use the <code>oc patch</code> command or edit the StorageClass YAML in the GUI.</p> <pre><code>oc patch storageclass gp3 -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}'\n</code></pre> <pre><code>oc patch storageclass ocs-storagecluster-ceph-rbd -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'\n</code></pre> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: ocs-storagecluster-ceph-rbd\n  uid: ce6ff323-544b-44ae-b88b-fed208aa901e\n  resourceVersion: '74999'\n  creationTimestamp: '2024-01-11T20:32:29Z'\n  annotations:\n    description: 'Provides RWO Filesystem volumes, and RWO and RWX Block volumes'\n    storageclass.kubernetes.io/is-default-class: 'true'\n</code></pre>"},{"location":"Openshift/cloudinstallation/#health-checks","title":"Health Checks","text":"<p>Using the yaml file below, add health checks to the worker and storage nodes.  Update the <code>name</code> and the <code>matchLabels</code> parameters to match the node/machine name, execute the <code>oc apply</code> command to apply.  Rinse and repeat for each node/machine.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineHealthCheck\nmetadata:\n  name: worker-2c \n  namespace: openshift-machine-api\nspec:\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-machineset: blm-aws-grv6f-worker-us-east-2c\n  unhealthyConditions:\n  - type:    \"Ready\"\n    timeout: \"300s\" \n    status: \"False\"\n  - type:    \"Ready\"\n    timeout: \"300s\" \n    status: \"Unknown\"\n  maxUnhealthy: \"40%\" \n</code></pre>"},{"location":"Openshift/hcp/","title":"OCP Hosted Control Plane","text":""},{"location":"Openshift/hcp/#hosted-control-plane","title":"Hosted Control Plane","text":""},{"location":"Openshift/hcp/#standalone-control-plane","title":"Standalone Control Plane","text":""},{"location":"Openshift/ocpCertificates/","title":"OCP Certificates","text":""},{"location":"Openshift/ocpCertificates/#service-certificates","title":"Service Certificates","text":"<p>Use the <code>service-ca</code> controller to generate certificates for internal traffic.  To create a cert/key pair, use the <code>oc annotate service</code> command.</p> <pre><code># oc annotate service serviceName service.beta.openshift.io/serving-cert-secret-name=serviceName-secret\n</code></pre> <p>Mount the secret in the app deployment.  The location is app dependent.</p> <p>Sample yaml specification: <pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: deploymentName\n          volumeMounts:\n            - name: volumeName\n              mountPath: /location/to/mount\n      volumes:\n        - name: volumeName\n          secret:\n            defaultMode: 420\n            secretName: serviceName-secret\n            items:\n            - key: tls.crt\n              path: server.crt\n            - key: tls.key\n              path: private/server.key\n</code></pre></p> <p>To rotate a certifcate from the <code>service-ca</code>, delete the existing secret and the <code>service-ca</code> will automatically generate a new one.</p> <pre><code># oc delete secret/serviceName-secret\n</code></pre>"},{"location":"Openshift/ocpCertificates/#ca-bundle-certs","title":"CA Bundle Certs","text":"<p>The <code>service-ca</code> will inject the CA certs when you apply the service.beta.openshift.io/inject-cabundle=true annotation using a configmap.</p> <pre><code># oc annotate configmap ca-bundle service.beta.openshift.io/inject-bundle=true\n</code></pre> <p>NOTE: The CA certificate is valide for 26 months by default and is automatically rotated after 13 months.  After a rotation, there is a 13 month grace period where the original CA certificate is valid to give each pod using the CA cert time to be restarted.  The new CA certificate is injected at restart.</p> <p>To manually rotate the CA certificate, use the <code>oc delete secret</code> command.  This will automatically generate a new CA cert.</p> <pre><code># oc delete secret/signing-key -n openshift-service-ca \n</code></pre> <p>WARNING: This process immediately invalidates the former service CA certificate.  All pods must be restarted that use it for TLS to function.</p> <p>Create a standard configmap of the service-ca CA certificate; sample to mount the config map to a deployment:</p> <pre><code># oc create configmap service-ca\n# oc annotate configmap/service-ca service.beta.openshift.io/inject-cabundle=true\n</code></pre> <p>Update the deployment yaml to include the following:  </p> <pre><code>spec:\n  containers: \n    ...\n    volumeMounts:\n      - name: trusted-ca\n        path: /etc/pki/ca-trust/extracted/pem\n  volumes:\n    - name: trusted-ca\n      configMap:\n        defaultMode: 420\n        name: service-ca\n        items:\n          key: service-ca.crt\n          path: tls-ca-bundle.pem\n</code></pre>"},{"location":"Openshift/ocpGeneral/","title":"OCP General Info","text":""},{"location":"Openshift/ocpGeneral/#terminology","title":"Terminology","text":"Term Definition Operator cluster component that simplifies the management of another application for function Resource any configureable or consumable compenent managed by the OCP Cluster Control Plane cluster layer, responsible for constainer lifecycle mgmt through API Data Plane cluster layer, responsible for providing resources required to run containers (stg, CPU, RAM) Pod group of running containers that provide a single application, service or function Container small executable image that defines the libraries and dependencies for an application"},{"location":"Openshift/ocpGeneral/#subscription-guide","title":"Subscription Guide","text":"<p>ODF Subscription Guide KCS</p>"},{"location":"Openshift/ocpGeneral/#whats-new-slide-deck","title":"What's New Slide Deck","text":"<p>ODF 4.13</p>"},{"location":"Openshift/ocpGeneral/#operator-hub-operatorhubio","title":"Operator Hub (operatorhub.io)","text":"<p>Operator Hub</p>"},{"location":"Openshift/ocpGeneral/#odf-dr-offerings","title":"ODF DR Offerings","text":"<p>OpenShift Data Foundation Disaster Recovery Offerings</p>"},{"location":"Openshift/ocpGeneral/#request-an-evaluation-subscription","title":"Request an evaluation subscription","text":"<p>Sales Assistence Product Trials</p>"},{"location":"Openshift/ocpGeneral/#rhel-coreos-versions","title":"RHEL CoreOS Versions","text":"<p>KCS Article</p> <p>Internal Support Streams Spreadsheet</p>"},{"location":"Openshift/ocpGeneral/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Allow the node to control which pods should or should not be schedule on them. </p> <p>A taint allows a node to reguse a pod to be scheduled unless that pod has a matching toleration.</p> <p>Taints are applied to nodes (NodeSpec) and tolerations are applied to a pod (PodSpec)</p> <p>Taint: <pre><code>apiVersion: v1\nkind: Node\nmetadata:\n  name: my-node\n#...\nspec:\n  taints:\n  - effect: NoExecute\n    key: key1\n    value: value1\n#...\n</code></pre></p> <p>Toleration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n#...\nspec:\n  tolerations:\n  - key: \"key1\"\n    operator: \"Equal\"\n    value: \"value1\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 3600\n#...\n</code></pre></p>"},{"location":"Openshift/ocpGeneral/#the-virtctl-tool","title":"The <code>virtctl</code> Tool","text":"<p>To download the virtctl tool:</p> <pre><code>oc get ConsoleCLIDownload virtctl-clidownloads-kubevirt-hyperconverged -o yaml\n</code></pre> <p>Use the appropriate link to the get the latest version. <pre><code>curl https://hyperconverged-cluster-cli-download-openshift-cnv.apps.blm-ocp.hexo.lab/amd64/linux/virtctl.tar.gz --insecure --output virtctl.tar.gz\n</code></pre></p> <p>Access the serial or VNC consoles of a VM with the virtctl CLI.  First login to the OCP cluster with the <code>oc login</code> command.  Select the VMs project with <code>oc project vm-project-name</code> command.  Access the serial console with the <code>virtctl console vm-name</code> command.  </p> <p>Note: The escape character for the serial console is Ctlr-]</p> <p>Access the VNC or graphical interface of a VM using <code>virtctl vnc vm-name</code>.</p>"},{"location":"Openshift/ocpGeneral/#commands","title":"Commands","text":""},{"location":"Openshift/ocpGeneral/#login","title":"Login:","text":"<pre><code>oc login -u user -p password https://api.blah.example.com:6443\n</code></pre>"},{"location":"Openshift/ocpGeneral/#get-the-console","title":"Get the console:","text":"<pre><code>oc whoami --show-console\n</code></pre>"},{"location":"Openshift/ocpGeneral/#status-of-all-vms-in-the-cluster","title":"Status of all VMs in the cluster:","text":"<pre><code>oc get vm -A\n</code></pre>"},{"location":"Openshift/ocpGeneral/#restrict-to-specific-namespace","title":"Restrict to specific namespace","text":"<pre><code>oc get vm -n namespace\n</code></pre>"},{"location":"Openshift/ocpGeneral/#get-details-on-vm","title":"Get details on VM","text":"<pre><code>oc describe vm vm-name -n namespace\n</code></pre>"},{"location":"Openshift/ocpGeneral/#list-of-filesystems-in-the-vmi","title":"List of filesystems in the VMI","text":"<pre><code>virtctl fslist vmi-name\n</code></pre>"},{"location":"Openshift/ocpGeneral/#view-guest-info-avou-the-vmi-os","title":"View guest info avou the VMI OS","text":"<pre><code>virtctl guestosinfo vmi-name\n</code></pre>"},{"location":"Openshift/ocpGeneral/#ssh-to-master-or-worker-nodes","title":"SSH to master or worker nodes:","text":"<pre><code>ssh -i ./id_rsa core@kni-master1\n</code></pre>"},{"location":"Openshift/ocpGeneral/#extract-secrets","title":"Extract Secrets","text":"<p><code>oc extract secret/&lt;secretName&gt; -n &lt;nameSpName&gt;</code>%                                           </p>"},{"location":"Openstack/OSP16.2Instructions/","title":"OSP 16.2 Installation Instructions","text":""},{"location":"Openstack/OSP16.2Instructions/#introduction","title":"Introduction","text":""},{"location":"Openstack/OSP16.2Instructions/#goals","title":"Goals:","text":"<ul> <li>Enable field on OSP 16.2</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to OSP Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OSP and director </li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Openstack/OSP16.2Instructions/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Undercloud 16G 4 <ul><li>1x pxe</li><li>1x external</li> 100GB Controller 12GB 2 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li><li>1x external</li> 60 GB Compute 4GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li> 60GB Ceph 4GB 2 <ul><li>1x pxe</li><li>1x storage</li><li>1x storagemgmt</li><li>1x storage</li> <ul><li>50GB OSD</li><li>100GB (osd)</li> HCI (optional) 8GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li> <ul><li>60GB OSD</li><li>100GB (osd)</li> Custom (optional)"},{"location":"Openstack/OSP16.2Instructions/#building-your-hextupleo-lab","title":"Building your HextupleO Lab:","text":"<p>HextupleO is an upstream project built with ansible playbooks talking directly to OpenStack APIs via python-shade libraries. It also nicely integrates with Ansible Automation Platform for ease of deployment and manageability (you can learn more about it here).</p>"},{"location":"Openstack/OSP16.2Instructions/#default-deployment","title":"Default Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Deploy OpenStack Environment. </p> </li> <li> <p>Provide a unique project_name and password. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Wait for the deployment to finish.  You can monitor via the Jobs Output screen or you can monitor each individual job by selecting Jobs from the left pane and selecting the appropriate jobs.  There are four jobs, create project, create networks, create instances, and configure OSP undercloud.</p> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates Tab and hit the \u201crocket\u201d icon next to - \u201cHextupleol - create  project\u201d, after you hit \u201cNext\u201d, you will get a survey prompting for user and password</p> </li> <li> <p>Set user/project and password and submit the job. The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create networks.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>networks:  \n  - { name: \"external0\", cidr: \"10.1.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"internalAPI0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"tenant0\", cidr: \"10.30.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storage0\", cidr: \"10.40.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storagemgmt0\", cidr: \"10.50.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provider0\", cidr: \"10.60.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"virtualipmi\", cidr: \"10.70.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create instances.  Below is a good starting config with controllers, 2 compute, and 3 Ceph nodes.  </p> <pre><code>instances:  \n  - { name: \"undercloud\", image: \"rhel-8.2\", flavor: \"undercloud\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"provider1-vlan217, net_name2: \"provisioning0, net_name3: \"external0\", net_name4: \"provider0\", net_name5: \"virtualipmi\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_controller1\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller2\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller3\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_compute1\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_compute2\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph1\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph2\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph3\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - configure OSP undercloud.  </p> </li> <li> <p>Follow the survey and submit the job.</p> </li> <li> <p>At the end you will be getting a screen similar to this:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user stack using the password you set in the playbook.</p>"},{"location":"Openstack/OSP16.2Instructions/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":"<p>This is accessing the Openstack that your environment is deployed to.</p>"},{"location":"Openstack/OSP16.2Instructions/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link: </p> <p>Horizon Login</p> <p>NOTE: You must be connected to the NA-SSA VPN</p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  </p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address that is associated with a provider vlan. You will use that IP to access your undercloud node. This should match the above output from the Ansible job.  You can SSH to this IP as the stack user using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected. </p> </li> </ol> <p>NOTE: We have created Tenant (over GENEVE/VXLAN) networks to satisfy all the non-routable networks. Only External Network is connected to one of the provider routable networks and accessible from outside of your deployed environment.</p> <p></p>"},{"location":"Openstack/OSP16.2Instructions/#undercloud","title":"Undercloud","text":"<ol> <li> <p>Access undercloud via ssh using stack@ip-learned-from-horizon  / password specified in Tower.  </p> </li> <li> <p>You can now start deploying OSP16.2 based on standard instructions (or whatever version you have staged).  See below notes for helpful information in the deployment process.</p> <p>NOTE:  Repos:  Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the osp repo file from here:</p> <p>[stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p>~/GoodieBag/deploy.sh - pre-configured deploy script that can be used as a template.</p> <p>~/templates - Directory that has multiple templates that have been pre-configured for making the vanilla deployment easier to perform. Undercloud.conf - sample undercloud.conf file with pre-configured known working settings like IP pools, interface settings, and more.  The egrep command is used to remove blank and comment lines.</p> <pre><code>[stack@undercloud ~]$ egrep -v '(^$|^#)' undercloud.conf\n[DEFAULT]\ncertificate_generation_ca = local\nclean_nodes = true\ncontainer_images_file = /home/stack/templates/containers-prepare-parameter.yaml\ngenerate_service_certificate = true\nhieradata_override = /home/stack/templates/undercloud_hiera.yaml\nlocal_interface = eth1\nlocal_ip = 10.10.0.10/24\nlocal_mtu = 8946\nlocal_subnet = ctlplane-subnet\novercloud_domain_name = hextupleo.lab\nsubnets = ctlplane-subnet\nundercloud_admin_host = 10.10.0.11\nundercloud_debug = false\nundercloud_hostname = osp-blm-undercloud.hextupleo.lab\nundercloud_nameservers = 172.20.129.10\nundercloud_ntp_servers = 172.20.129.10\nundercloud_public_host = 10.1.0.11\n[ctlplane-subnet]\ncidr = 10.10.0.0/24\ndhcp_end = 10.10.0.149\ndhcp_start =  10.10.0.100\ndns_nameservers = 10.10.0.10\ngateway = 10.10.0.10\ninspection_iprange = 10.10.0.200,10.10.0.249\nmasquerade = true\nmasquerade_network = 10.10.0.0/16\n[stack@undercloud ~]$  \n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#deploy-vanilla-osp-162","title":"Deploy Vanilla OSP 16.2","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, It is highly encouraged for everyone to get at least one vanilla deployment done to get themselves familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation. Please review the official documentation for accuracy and open any Bugzilla\u2019s against it!</p>"},{"location":"Openstack/OSP16.2Instructions/#undercloud-installation","title":"Undercloud Installation","text":"<ol> <li> <p>Log into the undercloud VM as the stack user using the IP address that was obtained during the VM deployments in Horizon.</p> <p>NOTE: You must be connected to the NA-SSA VPN to access the environment.</p> </li> <li> <p>Complete the RHEL upgrade requirements.</p> <p>Make sure you grabbed the repo configuration from the DNS server. [stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p><pre><code>[stack@undercloud ~]$ sudo dnf module reset container-tools\n[stack@undercloud ~]$ sudo dnf module enable -y container-tools:3.0\n[stack@undercloud ~]$ sudo yum update -y\n[stack@undercloud ~]$ sudo reboot \n</code></pre> 3. Install the TripleO Director, Ceph Ansible, tmux (optional) packages, and prepare the container images.</p> <pre><code>[stack@undercloud ~]$ sudo yum install -y python3-tripleoclient ceph-ansible tmux\n[stack@undercloud ~]$ openstack tripleo container image prepare default --local-push-destination --output-env-file ~/templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\nOutput env file exists, moving it to backup.\n</code></pre> </li> <li> <p>Starting with OSP15, Red Hat is moving to the Container Registry that requires an active Red Hat account.  In order to continue the installation, you need to include your credentials into the container-prepare-parameter.yaml file.  Service credentials can be created for the deployment using the Customer Portal Terms-Based-Registry site.</p> <p>Click the New Service Account icon in the upper right corner.  Enter a name for the service account and a description.  Click the Create icon.</p> <p></p> <p>Click the Copy icon on the far right to copy the new token to your local clipboard.</p> <p></p> </li> <li> <p>Update the containers-prepare-parameter.yaml file with the new service account name and token.  Open the file and jump to the bottom.  Add the highlighted lines and paste the user name and token generated from Step 4.  The ContainerImageRegistryCredentials space aligns with the ContainerImagePrepare line at the top of the file.</p> <pre><code>[stack@undercloud ~]$ vi templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-08T09:20:45.446840\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n...\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4N\n      ...\n      j1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>Update the undercloud.conf file if necessary.  This should not be required, but feel free to review and understand the parameters.</p> </li> <li> <p>Install the undercloud.</p> <pre><code>[stack@undercloud ~]$ openstack undercloud install\nundercloud_admin_host or undercloud_public_host is not in the same cidr as local_ip.\nConfig option undercloud_public_host \"10.1.0.11\" not in defined CIDR \"10.10.0.0/24\"\nRunning: sudo --preserve-env openstack tripleo deploy --standalone --standalone-role Undercloud --stack undercloud --local-domain=hextupleo.lab --local-ip=10.10.0.10/24 --templates=/usr/share/openstack-tripleo-heat-templates/ --networks-file=network_data_undercloud.yaml --heat-native -e /usr/share/openstack-tripleo-heat-templates/environments/undercloud.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/use-dns-for-vips.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/podman.yaml -e /home/stack/templates/containers-prepare-parameter.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/masquerade-networks.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic-inspector.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/mistral.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/zaqar-swift-backend.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/tempest.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/public-tls-undercloud.yaml --public-virtual-ip 10.1.0.11 --control-virtual-ip 10.10.0.11 -e /usr/share/openstack-tripleo-heat-templates/environments/ssl/tls-endpoints-public-ip.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/undercloud-haproxy.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/undercloud-keepalived.yaml --deployment-user stack --output-dir=/home/stack --cleanup -e /home/stack/tripleo-config-generated-env-files/undercloud_parameters.yaml --hieradata-override=/home/stack/templates/undercloud_hiera.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/tripleo-validations.yaml --log-file=install-undercloud.log -e /usr/share/openstack-tripleo-heat-templates/undercloud-stack-vstate-dropin.yaml\nThe heat stack undercloud action is CREATE\n2023-04-10 13:54:09.767 11371 INFO migrate.versioning.api [-] 70 -&gt; 71... \n2023-04-10 13:54:09.804 11371 INFO migrate.versioning.api [-] done\n...\n</code></pre> </li> <li> <p>Once the installation is complete, verify the containers are up and source the stackrc file to set the environment for the undercloud environment.</p> <pre><code>[stack@undercloud ~]$ sudo podman ps\n[stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ \n</code></pre> </li> <li> <p>Obtain the images for the overcloud nodes.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo dnf -y install rhosp-director-images rhosp-director-images-ipa-x86_64\n</code></pre> </li> <li> <p>Extract the tar balls for the overcloud and ironic images.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/images\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/overcloud-full-latest-16.2.tar\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/ironic-python-agent-latest-16.2.tar\n</code></pre> </li> <li> <p>The root password can be changed in the overcloud image using the procedures below.  This is optional and is not required.</p> <pre><code>(undercloud) [stack@undercloud ~]$ export LIBGUESTFS_BACKEND=direct\n(undercloud) [stack@undercloud ~]$ virt-customize -a overcloud-full.qcow2 --root-password password:changeme\n</code></pre> <p>NOTE: the libguestfs-tools package may need to be installed:        sudo dnf install -y libguestfs-tools</p> </li> <li> <p>Upload the overcloud images and verify they are available to the undercloud.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud image upload --image-path /home/stack/images\n(undercloud) [stack@undercloud ~]$ openstack image list\n+--------------------------------------+------------------------+--------+\n| ID                                   | Name                   | Status |\n+--------------------------------------+------------------------+--------+\n| f3e73e81-acca-45d7-a848-7d6de40933ed | overcloud-full         | active |\n| f8162188-4ec9-4312-9519-2e6421bd52a8 | overcloud-full-initrd  | active |\n| da2195d1-d75f-4134-b906-1302ff9943af | overcloud-full-vmlinuz | active |\n+--------------------------------------+------------------------+--------+\n</code></pre> </li> <li> <p>Verify the DNS server has been set for the cltplane subnet.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack subnet show ctlplane-subnet\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field             | Value                                                                                                                                                   |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| allocation_pools  | 10.10.0.100-10.10.0.149                                                                                                                                 |\n| cidr              | 10.10.0.0/24                                                                                                                                            |\n| created_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n| description       |                                                                                                                                                         |\n| dns_nameservers   | 10.10.0.10                                                                                                                                              |\n| enable_dhcp       | True                                                                                                                                                    |\n| gateway_ip        | 10.10.0.10                                                                                                                                              |\n| host_routes       |                                                                                                                                                         |\n| id                | 2096d5b9-7516-4146-ae4b-919a73f82a8f                                                                                                                    |\n| ip_version        | 4                                                                                                                                                       |\n| ipv6_address_mode | None                                                                                                                                                    |\n| ipv6_ra_mode      | None                                                                                                                                                    |\n| location          | cloud='', project.domain_id=, project.domain_name='Default', project.id='cd6a92e810154ab882d290a70e8c6afc', project.name='admin', region_name='', zone= |\n| name              | ctlplane-subnet                                                                                                                                         |\n| network_id        | 316ccfd5-f1b3-455c-9856-cbbed5b65ba7                                                                                                                    |\n| prefix_length     | None                                                                                                                                                    |\n| project_id        | cd6a92e810154ab882d290a70e8c6afc                                                                                                                        |\n| revision_number   | 0                                                                                                                                                       |\n| segment_id        | None                                                                                                                                                    |\n| service_types     |                                                                                                                                                         |\n| subnetpool_id     | None                                                                                                                                                    |\n| tags              |                                                                                                                                                         |\n| updated_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#overcloud-installation","title":"Overcloud Installation","text":"<p>The first step in deploying the overcloud is to generate the instackenv.yaml file.  Once this is complete, the file needs to be updated with the IP addresses of the VMs that were deployed by HextupleO.</p> <ol> <li> <p>Generate the instackenv.yaml file using the ansible-playbook.  Once the file is generated, source the projectNamerc file for the overcloud environment.  This will allow you to get the list of servers and their IP addresses.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/GoodieBag\n(undercloud) [stack@undercloud ~]$ ansible-playbook generate_instackenv.yml\n(undercloud) [stack@undercloud ~]$ source *projectName*rc\n(myproject) [stack@undercloud ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"    \" $8 }'\nipmi_overcloud_ceph3    virtualipmi=10.70.0.240\nipmi_overcloud_ceph2    virtualipmi=10.70.0.236\nipmi_overcloud_ceph1    virtualipmi=10.70.0.24\nipmi_overcloud_compute2    virtualipmi=10.70.0.11\nipmi_overcloud_compute1    virtualipmi=10.70.0.22\nipmi_overcloud_controller3    virtualipmi=10.70.0.70\nipmi_overcloud_controller2    virtualipmi=10.70.0.233\nipmi_overcloud_controller1    virtualipmi=10.70.0.227\n</code></pre> </li> <li> <p>Update the instackenv.yaml file with the IP addresses.  You can do this manually, or you can use this code:</p> <pre><code>openstack server list --insecure | awk '/ipmi_/ {print $4 \"    \" $8}' &gt; /tmp/ipmi_addresses.txt\ncp ~/GoodieBag/instackenv.yaml ~/\nsed -i '/pm_addr/d' ~/instackenv.yaml\nfor NODE in $(grep 'name: ' ~/instackenv.yaml | awk '{print $NF}' | sed 's/\"//g')\ndo\n  IP=$(egrep ${NODE} /tmp/ipmi_addresses.txt | awk -F= '{print $NF}')\n  sed -i \"/name: \\\"${NODE}\\\"/a \\    pm_addr: \\\"${IP}\\\"\" ~/instackenv.yaml\ndone\n</code></pre> <p>NOTE: Make sure you view the ~/instackenv.yaml file to ensure it is correct before continuing with the installation.</p> </li> <li> <p>Register the nodes for the overcloud.  Make sure you source the stackrc file for the undercloud environment.  Once the import is complete, list the baremetal nodes and ensure all information is correct for the deployment.  All nodes will be in a power off or manageable state.</p> <pre><code>(undercloud) [stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ openstack overcloud node import ~/instackenv.yaml\n(undercloud) [stack@undercloud ~]$ openstack baremetal node list\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name                  | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | 59118ee3-785a-45ef-bef7-8ce4739e34f6 | power off    | manageable             | False       |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | 55eefb7f-e5ad-4987-89ae-fe9ee2fb10dd | power off    | manageable             | False       |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | de0b39b9-b69e-4316-a2b7-743fa95acb65 | power off    | manageable             | False       |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | 5c61b62a-5324-4c20-9dc0-e3cfa866ffbb | power off    | manageable             | False       |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | d9bc6142-5780-45a2-899e-e5dc9d806ab9 | power off    | manageable             | False       |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | 2bb2ca3e-819a-46ce-aa6e-7a4d900875e3 | power off    | manageable             | False       |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | 5a55912a-9c7c-4e35-bde2-c582bbfa1c28 | power off    | manageable             | False       |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | 7f573f3c-8389-4973-aba7-99f055071632 | power off    | manageable             | False       |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n</code></pre> <p>NOTE: Continue to monitor until the nodes are all in an manageable state.</p> </li> <li> <p>Run the introspect to assign the profiles and configure the nodes successfully.  After all nodes are in an available state, verify the proper profiles were assigned.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud node introspect --all-manageable --provide\n(undercloud) [stack@undercloud ~]$ openstack overcloud profiles list\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| Node UUID                            | Node Name             | Provision State | Current Profile | Possible Profiles |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | active          | ceph-storage    |                   |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | active          | ceph-storage    |                   |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | active          | ceph-storage    |                   |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | active          | compute         |                   |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | active          | compute         |                   |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | active          | control         |                   |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | active          | control         |                   |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | active          | control         |                   |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n</code></pre> </li> <li> <p>The undercloud public endpoints have been most likely encrypted with self-signed certificates.  Make sure to inject the cert into the deployment of the overcloud.  Copy the inject-trust-anchor-hiera.yaml file to the templates directory, copy the cert from the cm-local-ca.pem file and paste into the new file in the templates directory.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ cp /usr/share/openstack-tripleo-heat-templates/environments/ssl/inject-trust-anchor-hiera.yaml ~/templates\n(undercloud) [stack@undercloud ~]$ cat /etc/pki/ca-trust/source/anchors/cm-local-ca.pem\nBag Attributes\n    localKeyID: 39 D8 6C E9 C8 7F 65 01 20 80 25 09 E7 A4 41 EB 5D 5E 4E E9 \n    friendlyName: Local Signing Authority\nsubject=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\nissuer=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\n-----BEGIN CERTIFICATE-----\nMIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n...\nLO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\nfaM=\n-----END CERTIFICATE-----\n(undercloud) [stack@undercloud ~]$ vi ~/templates/inject-trust-anchor-hiera.yaml\n# *******************************************************************\n# This file was created automatically by the sample environment\n# generator. Developers should use `tox -e genconfig` to update it.\n# Users are recommended to make changes to a copy of the file instead\n# of the original, if any customizations are needed.\n# *******************************************************************\n# title: Inject SSL Trust Anchor on Overcloud Nodes\n# description: |\n#   When using an SSL certificate signed by a CA that is not in the default\n#   list of CAs, this environment allows adding a custom CA certificate to\n#   the overcloud nodes.\nparameter_defaults:\n  # Map containing the CA certs and information needed for deploying them.\n  # Type: json\n  CAMap:\n    undercloud:\n      content: |\n        -----BEGIN CERTIFICATE-----\n        MIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n        ...\n        LO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\n        faM=\n        -----END CERTIFICATE-----\n</code></pre> <p>NOTE: Make sure that you line up the indentation for the certificate data.  It must be indented two spaces under the content tag.  </p> </li> <li> <p>Copy the deploy.sh template from the GoodieBag directory to the stack user's home directory.  Edit the /home/stack/deploy.sh file and add the inject-trust-anchor-hiera.yaml file.  Ensure you have an understanding of each of the yaml files included.  </p> <p>NOTE: If you want to deploy the Ceph Dashboard on the external network, you need to deploy it at the time of the initial overcloud deployment.  Create the ~/templates/ceph-dashboard-network-override.yaml file and include it in the ~/deploy.sh script.</p> <pre><code>(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi templates/ceph-dashboard-newtork-override.yaml\nparameter_defaults:\n  ServiceNetworkMap: \n    CephDashboardNetwork: external\n:wq\n</code></pre> <p>NOTE: If you are going to enable the RGW service in Ceph, make sure to include the ceph-rgw.yaml file in the initial deployment.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ cat deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack myproject \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml \\\n     -e templates/ceph-dashboard-network-override.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     -e templates/ceph_dashboard_network_override.yaml \\\n     --log-file myproject_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>In HextupleO 4, we are relying on the undercloud to provide NTP services.  By default, OSP v16 doesn't allow time sync from it's chrony service.  As a workaround, execute the following which opens the port via iptables and then allows a sync via chrony.  Restart the chrony service once complete.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo iptables -I INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT\n(undercloud) [stack@undercloud ~]$ echo \"echo allow 0.0.0.0/0 &gt;&gt; /etc/chrony.conf\" | sudo /bin/bash\n(undercloud) [stack@undercloud ~]$ cat /etc/chrony.conf\n# Do not manually edit this file.\n# Managed by ansible-role-chrony\nserver 172.20.129.10 iburst minpoll 6 maxpoll 10\nbindcmdaddress 127.0.0.1\nbindcmdaddress ::1\nallow 10.10.0.0/24\ndriftfile /var/lib/chrony/drift\nlogdir /var/log/chrony\nrtcsync\nmakestep 1.0 3 \nallow 0.0.0.0/0\n(undercloud) [stack@undercloud ~]$ sudo systemctl restart chronyd\n</code></pre> </li> <li> <p>Execute the deploy.sh script to deploy the overcloud.  This takes a very long time to deploy so make sure you run the script in a tmux session.</p> <pre><code>(undercloud) [stack@undercloud ~]$ tmux  \n(undercloud) [stack@undercloud ~]$ cd   \n(undercloud) [stack@undercloud ~]$ ./deploy.sh \n...\nPLAY RECAP *********************************************************************\nmyproject-cephstorage-0 : ok=294  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-1 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-2 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-controller-0 : ok=380  changed=217  unreachable=0    failed=0    skipped=193  rescued=0    ignored=0   \nmyproject-controller-1 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-controller-2 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-novacompute-0 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0   \nmyproject-novacompute-1 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0    \nundercloud                 : ok=172  changed=56   unreachable=0    failed=0    skipped=44   rescued=0    ignored=2   \n\n2023-03-27 16:11:25.971578 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.971826 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Total Tasks: 2517       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972000 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Elapsed Time: 1:22:22.733271 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972191 |                                 UUID |       Info |       Host |   Task Name |   Run Time\n2023-03-27 16:11:25.972394 | fa163e91-32a1-f7b4-b63f-000000007b9f |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 3 using paunch | 1087.73s\n2023-03-27 16:11:25.972600 | fa163e91-32a1-f7b4-b63f-00000000707e |    SUMMARY | undercloud | tripleo-ceph-run-ansible : run ceph-ansible | 479.52s\n2023-03-27 16:11:25.972762 | fa163e91-32a1-f7b4-b63f-000000006641 |    SUMMARY | myproject-controller-2 | Wait for container-puppet tasks (generate config) to finish | 390.19s\n2023-03-27 16:11:25.972917 | fa163e91-32a1-f7b4-b63f-00000000660c |    SUMMARY | myproject-controller-1 | Wait for container-puppet tasks (generate config) to finish | 390.03s\n2023-03-27 16:11:25.973060 | fa163e91-32a1-f7b4-b63f-00000000667b |    SUMMARY | myproject-controller-0 | Wait for container-puppet tasks (generate config) to finish | 379.89s\n2023-03-27 16:11:25.973195 | fa163e91-32a1-f7b4-b63f-000000007235 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for haproxy | 250.51s\n2023-03-27 16:11:25.973368 | fa163e91-32a1-f7b4-b63f-0000000076d6 |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 2 using paunch | 215.77s\n2023-03-27 16:11:25.973582 | fa163e91-32a1-f7b4-b63f-000000007609 |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 2 using paunch | 195.36s\n2023-03-27 16:11:25.973734 | fa163e91-32a1-f7b4-b63f-00000000763f |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 2 using paunch | 195.29s\n2023-03-27 16:11:25.973875 | fa163e91-32a1-f7b4-b63f-00000000659a |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 154.03s\n2023-03-27 16:11:25.974022 | fa163e91-32a1-f7b4-b63f-000000007803 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for ovn_dbs | 150.17s\n2023-03-27 16:11:25.974162 | fa163e91-32a1-f7b4-b63f-000000006516 |    SUMMARY | myproject-controller-1 | Wait for puppet host configuration to finish | 143.97s\n2023-03-27 16:11:25.974322 | fa163e91-32a1-f7b4-b63f-00000000655d |    SUMMARY | myproject-controller-2 | Wait for puppet host configuration to finish | 143.71s\n2023-03-27 16:11:25.974517 | fa163e91-32a1-f7b4-b63f-000000007268 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for redis | 125.43s\n2023-03-27 16:11:25.974698 | fa163e91-32a1-f7b4-b63f-000000007245 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for mysql | 125.22s\n2023-03-27 16:11:25.974839 | fa163e91-32a1-f7b4-b63f-000000007b1a |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 3 using paunch | 124.18s\n2023-03-27 16:11:25.974974 | fa163e91-32a1-f7b4-b63f-000000007b50 |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 3 using paunch | 123.86s\n2023-03-27 16:11:25.975119 | fa163e91-32a1-f7b4-b63f-000000007258 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for oslo_messaging_rpc | 123.83s\n2023-03-27 16:11:25.975276 | fa163e91-32a1-f7b4-b63f-000000008a1e |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 123.51s\n2023-03-27 16:11:25.975468 | fa163e91-32a1-f7b4-b63f-000000008e17 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for cinder_volume | 122.44s\n2023-03-27 16:11:25.975619 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ End Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nAnsible passed.Overcloud configuration completed.\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    101m32.022s\nuser    0m13.412s\nsys     0m1.491s\n</code></pre> <p>NOTE: In a separate session, you can monitor the deployment with the openstack commands.  </p> <p>(undercloud) [stack@undercloud ~]$ openstack server list (undercloud) [stack@undercloud ~]$ openstack baremetal node list  </p> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#post-deployment-validations","title":"Post Deployment Validations","text":"<ol> <li> <p>Check the health of the cluster and the avialability zones.  Source the \\&lt;projectName&gt;rc file first.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ source myprojectrc\n(myproject) [stack@myproject-undercloud ~]$ openstack service list\n+----------------------------------+-----------+----------------+\n| ID                               | Name      | Type           |\n+----------------------------------+-----------+----------------+\n| 2cf5a8efe59e429f913ed11f0fe29d58 | glance    | image          |\n| 3a3aaac39577402ca2d91ae8ca70f359 | heat      | orchestration  |\n| 5879421c8cda4f2fa1b98a1ff159b10a | placement | placement      |\n| 9ce51c3e27f9448182a52c733a4cda2d | cinderv3  | volumev3       |\n| a3ddde9d1c0b48c19bf9005680716d38 | keystone  | identity       |\n| b22f9ce59c404fdbab27dd9fdd819149 | swift     | object-store   |\n| bed225f252b74b6d96dcb23249d20da0 | heat-cfn  | cloudformation |\n| e3c472eb17f545ebb7278bbcf475f322 | nova      | compute        |\n| e460149135bd4f2dabe03d8a8f3eedc1 | cinderv2  | volumev2       |\n| e7c30296b07f4147bb843723d10a7859 | neutron   | network        |\n+----------------------------------+-----------+----------------+\n</code></pre> <pre><code>(myproject) [stack@myproject-undercloud ~]$ openstack network agent list\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| ID                                   | Agent Type                   | Host                                     | Availability Zone | Alive | State | Binary         |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| 9e202644-48b9-4d40-a484-4ad6420bc752 | OVN Controller agent         | myproject-novacompute-0.localdomain |                   | :-)   | UP    | ovn-controller |\n| a43a0c19-669b-40d0-84d4-48ad9d5f514e | OVN Controller Gateway agent | myproject-controller-2.localdomain  |                   | :-)   | UP    | ovn-controller |\n| f26f3981-1e67-4063-b1ce-848deff49d18 | OVN Controller Gateway agent | myproject-controller-0.localdomain  |                   | :-)   | UP    | ovn-controller |\n| e1538367-7b98-49a0-a9cb-98273b01938a | OVN Controller agent         | myproject-novacompute-1.localdomain |                   | :-)   | UP    | ovn-controller |\n| bf65bc54-80d5-4614-b28c-9b42aaf08bc5 | OVN Controller Gateway agent | myproject-controller-1.localdomain  |                   | :-)   | UP    | ovn-controller |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n</code></pre> </li> <li> <p>Create an image with central and remote/dcn Glance service.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ curl http://10.9.71.7/cirros-0.4.0-x86_64-disk.img -o ~/cirros-0.4.0-x86_64-disk.img  \n(myproject) [stack@myproject-undercloud ~]$ qemu-img convert -f qcow2 -O raw cirros-0.4.0-x86_64-disk.img cirros-0.4.0-x86_64-disk.raw  \n(myproject) [stack@myproject-undercloud ~]$ glance image-create --disk-format raw --container-format bare --name cirros --file cirros-0.4.0-x86_64-disk.raw --visibility public  \n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#ceph-alias","title":"Ceph Alias","text":"<p>There isn't a Ceph client installed (i.e. ceph-common) on the controller nodes.  To access the Ceph cluster, all commands are run in the ceph-mon containers.  To make for less typing, I like to set up an alias for ceph on the controller nodes.</p> <pre><code>[heat-admin@osp-blm-controller-0 ~]$ vi .bash_profile\n# .bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n# User specific environment and startup programs\nhn=$(hostname)\nalias ceph=\"sudo podman exec ceph-mon-$hn ceph\"\n</code></pre> <p>NOTE: Don't forget to update the <code>.bash_profile</code> on controller-[12] nodes.</p>"},{"location":"Openstack/OSP16.2Instructions/#installation-of-ceph-dashboard","title":"Installation of Ceph Dashboard","text":"<p>The Ceph dashboard is disabled by default but can easily be enabled in the overcloud using Director.  Full documentation can be found here.</p> <p>NOTE: If deploying the Ceph Dashboard on the external network or any network other than the provisioning or ctlplane networks, it must be deployed at initial deployment of the OSP overcloud.  This is due to Puppet and HAProxy.  </p> <p>For quick reference, follow these procedures:</p> <ol> <li> <p>Source the stackrc file; reivew the templates/containers-prepare-parameter.yaml file.  This was generated in the overcloud deployment procedures and includes the containers required for Ceph and the dashboard.</p> <pre><code>[stack@blm-ospinstall-undercloud ~]$ source ./stackrc\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ cat templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4NyJ9.\n      ...\n      edIJ9DVJCI8MzWcouwKXIfGiMzjbj1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>The Ceph dashboard network is set by default to the provisioning network.  If you want to access through the ctlplane network, create an environment file and set the CephDashboardNetwork parameter to ctlplane.  Include this file in the deploy.sh script.</p> <pre><code>(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi templates/ceph-dashboard-newtork-override.yaml\nparameter_defaults:\n  CephDashboardNetwork: ctlplane\n:wq\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\n\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack blm-ospinstall \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml \\\n     -e templates/ceph-dashboard-network-override.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     --log-file blm-ospinstall_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>Run the deploy.sh script to install the dashboard stack.  This will deploy grafana, prometheus, alertmanager, and the node-exporter containers on the same nodes as the manager containers.</p> <pre><code>(undercloud) [stack@undercloud ~]$ ./deploy.sh\n...\nAnsible passed.Overcloud configuration completed.\nThe output file /home/stack/overcloud-deploy/myproject/myproject-deployment_status.yaml will be overriden\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    92m45.065s\nuser     0m12.506s\nsys       0m1.494s\n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#accessing-the-ceph-dashboard","title":"Accessing the Ceph Dashboard","text":"<p>The dashboard is read only by default.  You can change the permissions, see the full documentation for the procedures keeping in mind that changes made could be overwritten by the Director.</p> <ol> <li> <p>The VIP address and the Ceph admin credentials are contained within the all.yml file on the Undercloud Director.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo grep -e dashboard_admin_password -e dashboard_frontend_vip /var/lib/mistral/myproject/ceph-ansible/group_vars/all.yml\ndashboard_admin_password: ********************\ndashboard_frontend_vip: 10.10.0.137\n(undercloud) [stack@undercloud ~]$ \n</code></pre> <p>NOTE: If the Dashboard was deployed on the external network, the dashboard_frontend_vip will still contain an IP from the ctlplane network.  To get the external network IP address and view the HAProxy configuration, login to a control node and execute the following:</p> <p><pre><code>$ sudo grep -A13 dashboard /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg \nlisten ceph_dashboard\nbind 10.1.0.95:8444 transparent\nmode http\nbalance source\nhttp-check expect rstatus 2[0-9][0-9]\nhttp-request set-header X-Forwarded-Proto https if { ssl_fc }\nhttp-request set-header X-Forwarded-Proto http if !{ ssl_fc }\nhttp-request set-header X-Forwarded-Port %[dst_port]\noption httpchk HEAD /\noption httplog\noption forwardfor\nserver osp-blm-controller-0.storage.localdomain 10.40.0.108:8444 check fall 5 inter 2000 rise 2\nserver osp-blm-controller-1.storage.localdomain 10.40.0.178:8444 check fall 5 inter 2000 rise 2\nserver osp-blm-controller-2.storage.localdomain 10.40.0.160:8444 check fall 5 inter 2000 rise 2\n</code></pre> 2. If the VIP is on a private network, open a sshuttle VPN connection using the undercloud director.  </p> <p>NOTE: In Hextupleo, the external network still requires the use of sshuttle.</p> <pre><code>bmclaren@bmclaren-mac ~ % sshuttle -r stack@blm-ospinstall 10.10.0.0/24\n[local sudo] Password: \nstack@blm-ospinstall's password: \nc : Connected to server.\n s: warning: closed channel 1 got cmd=TCP_DATA len=432\n</code></pre> </li> <li> <p>In your favorite browswer, access the dashboard at the VIP on port 8444.  </p> </li> </ol> <p></p> <p>NOTE: An error indicating you don't have permission to view this page will display, just click the Go to Dashboard icon.  </p>"},{"location":"Openstack/OSP16.2Instructions/#appendix","title":"Appendix","text":""},{"location":"Openstack/OSP16.2Instructions/#osp-and-ceph-alignment","title":"OSP and Ceph Alignment","text":"RH-OSP Director External OPS 10 Ceph 2.x Ceph 2.x or 3.x OPS 13 Ceph 3.x Ceph 3.x or 4.x OPS 16.1 Ceph 4.x Ceph 4.x or 5.x (16.1.8) OPS 16.2 Ceph 4.x Ceph 4.x or 5.x (16.2.1) OPS 17.0 Ceph 5.2+ Ceph 5.2 or TBD OSP 17.1 (when released) Ceph 6 Ceph 5.2 or 6"},{"location":"Openstack/OSP16.2Instructions/#openstack-resource-and-event-list","title":"OpenStack Resource and Event List","text":"<pre><code>watch -n 30 \"openstack stack event list blm-ospinstall --nested-depth 5 | grep -v COMPLETE|  tail -n 10; openstack stack resource list blm-ospinstall -n 5 | grep -v COMPLETE | tail -n 10\"\n\n2023-03-27 18:46:26Z [blm-ospinstall.AllNodesDeploySteps.BootstrapServerId]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:27Z [blm-ospinstall.AllNodesDeploySteps.ExternalPostDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:28Z [blm-ospinstall.AllNodesDeploySteps.ExternalUpdateTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:29Z [blm-ospinstall.AllNodesDeploySteps.ControllerExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:30Z [blm-ospinstall.AllNodesDeploySteps.CephStorageExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:31Z [blm-ospinstall.AllNodesDeploySteps.ComputeExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:32Z [blm-ospinstall.AllNodesDeploySteps.ExternalDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ControllerPostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.CephStoragePostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ComputePostConfig]: CREATE_IN_PROGRESS  state changed\n+--------------------------------------------+-------------+-----------------------------------------\n---------------------------------------------------------------------------------------------------------+-----------------+----------------------+-------------------------------------------------------------------------------------------------\n--------------------------------------------------------+\n| resource_name                              | physical_resource_id                                                                                                                                       | resource_type\n                                                                                                         | resource_status | updated_time         | stack_name\n                                                        |\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#logs","title":"Logs","text":"<p>During the deployment, as the Ansible playbooks are executing, the output is written to the ansible.log under the mistral service.  The logs is located in /var/lib/mistral/stackName</p> <pre><code>tail -f ansible.log\n2023-03-27 15:19:33,748 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.748105 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-1\n2023-03-27 15:19:33,817 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.817014 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,338 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.337763 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-2\n2023-03-27 15:19:34,340 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.340029 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-1\n2023-03-27 15:19:34,401 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.401495 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:34,423 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.422923 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,550 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.550269 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n2023-03-27 15:19:34,865 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.865084 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-2\n2023-03-27 15:19:34,928 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.928633 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:35,050 p=89502 u=mistral n=ansible | 2023-03-27 15:19:35.049924 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n...\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#ceph-network-interface-bonds","title":"Ceph Network Interface Bonds","text":"<pre><code>  OsNetConfigImpl:\n    type: OS::Heat::SoftwareConfig\n    properties:\n      group: script\n      config:\n        str_replace:\n          template:\n            get_file: ../../scripts/run-os-net-config.sh\n          params:\n            $network_config:\n              network_config:\n              - type: interface\n                name: nic1  &lt;-- Change to the provisioning network NIC (i.e. eno01)\n                mtu:\n                  get_param: ControlPlaneMtu\n                use_dhcp: false\n                addresses:\n                - ip_netmask:\n                    list_join:\n                    - /\n                    - - get_param: ControlPlaneIp\n                      - get_param: ControlPlaneSubnetCidr\n                routes:\n                  list_concat_unique:\n                    - get_param: ControlPlaneStaticRoutes\n                    - - default: true\n                        next_hop:\n                          get_param: ControlPlaneDefaultRoute\n\n              - type: ovs_bridge\n                name: br_bond0\n                - type: linux_bond\n                  name: bond0  &lt;-- StorageMgmt Network\n                  mtu:\n                    get_attr: [MinViableMtuBondApi, value]\n                  use_dhcp: false\n                  bonding_options:\n                    get_param: BondInterfaceOvsOptions\n                  dns_servers:\n                    get_param: DnsServers\n                  domain:\n                    get_param: DnsSearchDomains\n                  members:\n                    - type: interface\n                      name: nic2   &lt;-- Change this to the port for storage network (i.e. eno49)\n                      mtu:\n                        get_attr: [MinViableMtuBondApi, value]\n                      primary: true\n                    - type: interface\n                      name: nic3   &lt;-- Change this to the port for storage network (i.e. ens1f1)\n                      mtu:\n                        get_attr: [MinViableMtuBondApi, value]\n                - type: vlan\n                  device: bond0\n                  mtu:\n                    get_param: StorageMgmtMtu\n                  vlan_id:\n                    get_param: StorageMgmtNetworkVlanID\n                  addresses:\n                  - ip_netmask:\n                      get_param: StorageMgmtIpSubnet\n                  routes:\n                    list_concat_unique:\n                     - get_param: StorageMgmtInterfaceRoutes\n              - type: ovs_bridge\n                name: br_bond1\n                - type: linux_bond\n                  name: bond1    &lt;--Storage Network\n                  mtu:\n                    get_attr: [MinViableMtuBondApi, value]\n                  use_dhcp: false\n                  bonding_options:  \n                    get_param: BondInterfaceOvsOptions\n                  dns_servers:\n                    get_param: DnsServers\n                  domain:\n                    get_param: DnsSearchDomains\n                  members:\n                    - type: interface\n                      name: nic4    &lt;-- Change this to the port for storage network (i.e. eno50)\n                      mtu:\n                        get_attr: [MinViableMtuBondData, value]\n                      primary: true\n                    - type: interface\n                      name: nic5    &lt;-- Change this to the port for storage network (i.e. ens1f0)\n                      mtu:\n                        get_attr: [MinViableMtuBondData, value]\n                - type: vlan\n                  device: bond1\n                  mtu:\n                    get_param: StorageMgmtMtu\n                  vlan_id:\n                    get_param: StorageMgmtNetworkVlanID\n                  addresses:\n                  - ip_netmask:\n                      get_param: StorageMgmtIpSubnet\n                  routes:\n                    list_concat_unique:\n                     - get_param: StorageMgmtInterfaceRoutes\noutputs:\n  OS::stack_id:\n    description: The OsNetConfigImpl resource.\n    value:\n      get_resource: OsNetConfigImpl\n</code></pre> <pre><code>```\n#This file is an example of an environment file for defining the isolated\n#networks and related parameters.\nresource_registry:\n  # Network Interface templates to use (these files must exist)\n  OS::TripleO::Compute::Net::SoftwareConfig:\n    ./nic-config/compute.yaml\n  # if you are configuring HCI (Hyperconverged nodes) comment 2 lines above and uncomment 2 lines below\n  #OS::TripleO::Compute::Net::SoftwareConfig:\n  #  ./nic-config/compute-hci.yaml\n  OS::TripleO::Controller::Net::SoftwareConfig:\n    ./nic-config/controller.yaml\n  OS::TripleO::CephStorage::Net::SoftwareConfig:\n    ./nic-config/ceph-storage.yaml   &lt;-- Update the reference the proper location of this file.\n\n...\n  DnsServers: [\"172.20.129.10\",\"8.8.8.8\"]\n  # List of Neutron network types for tenant networks (will be used in order)\n  NeutronNetworkType: 'geneve,vlan'\n  # The tunnel type for the tenant network (vxlan or gre). Set to '' to disable tunneling.\n  NeutronTunnelTypes: 'geneve'\n  # Neutron VLAN ranges per network, for example 'datacentre:1:499,tenant:500:1000':\n  NeutronNetworkVLANRanges: 'datacentre:1:1000'\n  #NeutronBridgeMappings: 'datacentre:br-bond0, tenant;br-tenant  &lt;--Example update \n  NeutronBridgeMappings: 'datacentre:br-ex,provider:br-provider'\n  NeutronFlatNetworks: 'datacentre,provider'\n  # Customize bonding options, e.g. \"mode=4 lacp_rate=1 updelay=1000 miimon=100\"\n  # for Linux bonds w/LACP, or \"bond_mode=active-backup\" for OVS active/backup.\n  # BondInterfaceOvsOptions: \"mode=802.3ad lacp_rate=fast updelay=1000 miimon=100 xmit_hash_policy=layer3+4   &lt;--Example update \n  BondInterfaceOvsOptions: \"bond_mode=active-backup\"\n  TimeZone: 'US/Eastern'\n  NtpServer: 10.10.0.10    &lt;-- Update with NTP servers is they are available outside the network, otherwise make sure the Chrony update is made.\n  # TimeZone: \"America/Chicago\"  &lt;-- Add the timezone\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#ceph-specific-info","title":"Ceph Specific Info","text":"<pre><code>tail -f /var/lib/mistral/blm-ospinstall/ceph-ansible/ceph_ansible_command.log\n...\n2023-03-27 15:18:02,889 p=683304 u=root n=ansible | INSTALLER STATUS ***************************************************************\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Monitor           : Complete (0:01:10)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Manager           : Complete (0:01:03)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph OSD               : Complete (0:02:05)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph RGW               : Complete (0:00:33)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Client            : Complete (0:00:27)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Install Ceph Crash             : Complete (0:00:41)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Monday 27 March 2023  15:18:02 -0400 (0:00:00.044)       0:07:56.045 ********** \n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | =============================================================================== \n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-container-common : pulling blm-ospinstall-undercloud.ctlplane.hextupleo.lab:8787/rhceph/rhceph-4-rhel8:latest image -- 27.71s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mon : waiting for the monitor(s) to form the quorum... ------------ 17.18s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mgr : create ceph mgr keyring(s) on a mon node -------------------- 12.77s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-handler : restart the ceph-crash service -------------------------- 12.47s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-osd : wait for all osd to be up ----------------------------------- 12.34s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mon : fetch ceph initial keys ------------------------------------- 12.06s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : generate keys ------------------------------------------------ 8.93s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : use ceph-volume lvm batch to create bluestore osds ----------- 8.36s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mgr : wait for all mgr to be up ------------------------------------ 7.46s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : assign application to pool(s) -------------------------------- 7.25s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : create openstack pool(s) ------------------------------------- 6.75s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | gather and delegate facts ----------------------------------------------- 5.94s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool crush_rule ------------------------------------ 5.88s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool min_size -------------------------------------- 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool size ------------------------------------------ 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : set pg_autoscale_mode value on pool(s) ----------------------- 5.69s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-crash : create client.crash keyring -------------------------------- 4.56s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.27s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.24s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.01s\n</code></pre> <p>openstack stack resource list  openstack stack event list  --nested-depth 5 <p>openstack stack list openstack stack event list  <p>openstack stack failures list  <p>openstack baremetal node set --property capabilities</p> <p>Manually tagging nodes for the profile: openstack baremetal node set --property capabilities='profile:control,boot_option:local' 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0</p> <p>openstack baremetal node set --property capabilities='profile:compute,boot_option:local' 484587b2-b3b3-40d5-925b-a26a2fa3036f</p> <p>openstack baremetal node set --property capabilities='profile:ceph-storage,boot_option:local' d930e613-3e14-44b9-8240-4f3559801ea6</p>"}]}