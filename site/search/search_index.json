{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Red Hat NA-SSA Lab's Documentation Site.","text":"<p>This site is created using MKDocs.  For full documentation visit mkdocs.org.</p>"},{"location":"#update-the-site-or-add-documentation","title":"Update the Site or Add Documentation","text":"<p>The documentation is hosted in GitHub at https://github.com/redhat-openinfra-lab/openinfra-docs.</p> <p>Please refer to the Documentation Contribution for more information on contributing to this documentation.</p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages.\n    /images   # images that are used in docs pages\n</code></pre>"},{"location":"#helpful-commands","title":"Helpful commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#lab-environment","title":"Lab Environment","text":"<p>If you'd like to see the physical layout of the hardware in the lab, check out this diagram in  Lucid Charts  or if you don't have access, here's a static diagram (click to open a larger version in a new tab):</p> <p></p>"},{"location":"#lab-ssa-links","title":"Lab SSA Links","text":"<p>SSA Links</p>"},{"location":"#lab-vpn-access","title":"Lab VPN Access","text":"<ul> <li>Connect to the Red Hat VPN</li> <li>Access the Fortigate Administration Page.</li> <li>You will need to login as the <code>user_mgr</code> account. These credentials are in the BitWarden Vault under Fortigate User Management FW.</li> <li> <p>After logging in, navigate to the User &amp; Device -&gt; User Definition on the left:</p> <p></p> </li> <li> <p>Click Create New </p> <p>NOTE: If you are changing your password, click Edit User and change it there.</p> <ul> <li>User Type: Local User</li> <li>Login Credentials: Input user name and password, use your Kerberos name for consistency.</li> <li>Contact Information: Input your Red Hat e-mail address</li> <li>Extra Information: Select the following</li> </ul> <p></p> </li> </ul> <p>NOTE&gt; : If you accidentally create a user and you want to delete it, first edit the user, remove the group membership from it, disable group membership and then you will be able to delete it.</p> <p>Once you have the new credentials configured, log out in the top right corner.  Test your new credentials by connecting to the  Fortinet VPN Portal</p> <p>Connecting validated the credentials.  Download the Fortinet VPN Client from their Product Downloads page or in the portal, from the Download FortiClient dropdown list.</p> <p></p> <p>Select the appropriate client and download it from where you\u2019re redirected to.</p> <p>Install the client - this screenshot was taken from a forticlient running on a Mac. When you first start it, you will be prompted to configure the VPN.</p> <p></p> <p>Click Save.  You can then connect to the NA SSA Lab VPN.</p> <p>You can also use OpenConnect (version 8+) to connect to the NA SSA Lab VPN from a Linux desktop running GNOME. First install OpenConnect (sudo dnf install openconnect - or debian or whatever Linux flavor you are running equivalent to), then install NetworkManager-openconnect-gnome (sudo dnf install NetworkManager-openconnect-gnome).</p> <p>Open GNOME Settings, navigate to Network and select the plus button to add a new VPN profile:</p> <p></p> <p>Select \u201copenconnect\u201d.  In the properties type the following:</p> <p>Name: (Whatever you like but \u201cNA SSL Lab VPN\u201d sounds good to me!) Gateway: 209.132.179.151:20443</p> <p></p> <p>Save the new VPN profile and connect to the VPN. Please note that the VPN gateway uses a self signed certificate - you might be warned about that. Accept the certificate and proceed.</p> <p>You can also use an alternative client that works well on headless systems. It does not have any dependencies on GNOME or dbus etc - like the network-manager method described above.</p> <ul> <li>Install openforticlient (sudo dnf install openforticlient)  </li> <li>Connect to the VPN from the command line:  </li> <li>sudo openfortivpn 209.132.179.151:20443 --username=mlecki --trusted-cert 646fd76ad8c617bfd94f3318f25e592a88fd2735949dfde0281df19de43b47ce</li> </ul>"},{"location":"#private-bin","title":"Private Bin","text":"<p>https://privatebin.corp.redhat.com</p>"},{"location":"MiscellaneousNotes/","title":"Miscellaneous Notes","text":""},{"location":"MiscellaneousNotes/#keys-to-the-kingdom","title":"Keys to the Kingdom","text":"<p>For security purposes, all administrative and root logins are kept in BitWarden for safe keeping.  This is an enterprise approved password management application.  BitWarden is integrated with Red Hat Rover Groups for access management.</p>"},{"location":"MiscellaneousNotes/#red-hat-rover-groups","title":"Red Hat Rover Groups","text":"<p>You can find Rover Groups in the  Rover application.  In the Search bar, enter groups.  </p> <p>Access  Rover Groups ; the main page will display your group membership.  The <code>na-ssa-infrastructure</code> group is the group used for BitWarden membership.  Current owners of the group are Ken, Chris, and Peter.  They can help with any membership additions and deletions.</p>"},{"location":"MiscellaneousNotes/#join-the-group-e-mail","title":"Join the Group E-Mail","text":"<p>Once a user is added to the group, they will recieve an e-mail to accept the invitation.  Until they join, they won't be in the group.</p>"},{"location":"MiscellaneousNotes/#bitwarden","title":"BitWarden","text":"<p>BitWarden  can be access via your favorite browser.  If necessary create an account using your @redhat.com email address.  </p> <p>The Na Ssa Infrastructure Collection has all of the lab's passwords, keys, and secrets.  You can create your own Folders to organize the secrets.  Keep in mind, the Folders are only available to you.  They are not shared folders with the na-ssa-infrastructure group, just the collection is shared.  Feel free to organize however you want. </p>"},{"location":"MiscellaneousNotes/#weblinks-isos","title":"Weblinks &amp; ISOs","text":"<p>The rhel8 repo server (172.20.129.19) has an alias created for it for www.openinfra.lab and has a number of useful links on it.</p> <p>Additionally, if you host your ISO files on this server in <code>/var/www/html</code>, they will be auto-populated on this page to make copying of the links to them useful for pasting into IMM or any other UI where you are prompted for the URL to an ISO.</p>"},{"location":"MiscellaneousNotes/#gitea","title":"Gitea","text":"<p>This git installation is running as an Operator within the Openshift Baremetal Cluster.  In order to customize the deployment to enable SSH access, the default configuration needed to be modified.  This was accomplished by:</p> <ol> <li> <p>Under Workloads-&gt;ConfigMaps in the gitea namespace, examine the config ini that was being used.  In our case it was the cloud-infra-git-config ConfigMap.</p> </li> <li> <p>Create a new ConfigMap with the contents of the previous config and any changes that are neeeded.  We called this nassa-gitea-static-config.</p> </li> <li> <p>Under Operators-&gt;Installed Operators select the Gitea Operator and go to the Gitea tab.</p> </li> <li> <p>Select cloud-infra-git, go to YAML and added the following line to the spec:</p> </li> </ol> <pre><code>  giteaConfigMapName: nassa-gitea-static-config\n</code></pre>"},{"location":"Ceph/CephAuthorization/","title":"Ceph Authorization","text":""},{"location":"Ceph/CephAuthorization/#introduction","title":"Introduction","text":"<p>Ceph uses user accounts for internal communication between Ceph daemons, client applications accessing the cluster through librados, and for cluster administration.  Administrative and all client access have accounts that begin with the client. prefix.</p> <p>The Ceph super user account is client.admin with capabilities that allow the account to access everything and to modify the cluster configuration.  This is the default account that is used unless you specify a user name with the --name or --id options.</p> <p>NOTE: When using the --name option, use the client. prefix.  But when using the --id option, the client. prefix is assumed.  Alternately you can also export CEPH_ARGA='--id cephuser\".</p> <p>The keyring files are maintained in the /etc/ceph directory.  If your keyring is not in this directory, you must pass it the --keyring option along with the file name that contains the secret key.</p>"},{"location":"Ceph/CephAuthorization/#capabilities","title":"Capabilities","text":"<ul> <li>r grants read access</li> <li>w grants write access</li> <li>x grants authorization to execute extended object classes</li> <li>* grants full access</li> </ul>"},{"location":"Ceph/CephAuthorization/#profiles","title":"Profiles","text":"<ul> <li>profile rbd - grants read/write access to any RBD pool</li> <li>profile rbd-read-only - grants read only access to any RBD pool</li> </ul>"},{"location":"Ceph/CephAuthorization/#create-account","title":"Create Account","text":"<p>NOTE: to save the key to a file use the -o /etc/ceph/ceph.client.name.keyring option</p> <p>To create a client account to read and write to any RBD pool:</p> <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw'\n</code></pre> <p>Limit the user to a specific pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool'\n</code></pre></p> <p>Limit the user to specific objects in a specific pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool object_prefix my_prefix'\n</code></pre></p> <p>Limit the user to a specific namespace with a pool: <pre><code># ceph auth get-or-create client.myappuser mon 'allow r' osd 'allow rw pool=myapppool namespace=myappspace'\n</code></pre></p> <p>Limit the user read/write access to a specific path (used by CephFS): <pre><code># ceph fs authorize cephfs client.myappuser /myapppath rw\n# ceph auth get client.myappuser\nexported keyring for client.myappuser\n[client.myappuser]\n        key = ABE9asvIhDW7FJE#kii982qavejJKEJjEji==\n        caps mds = 'allow rw path=/myapppath\n        caps mon = 'allow r'\n        caps osd = 'allow rw pool=cephfs_data'\n</code></pre></p> <p>Limit the user to a specific administraive commands against a mon: <pre><code># ceph auth get-or-create client.operator mon 'allow r allow command \"auth get-or-create\", allow command \"auth list\"' \n</code></pre></p> <p>List Users: <pre><code># ceph auth list\n</code></pre></p> <p>Get current key and permissions of a user: <pre><code># ceph auth print-key client.myappuser\n# ceph auth get client.myappuser\n</code></pre></p>"},{"location":"Ceph/CephAuthorization/#update-ceph-dashboard-admin-credentials","title":"Update Ceph Dashboard admin Credentials","text":"<pre><code>echo 'thepassword' &gt; dash.pass\nceph dashboard ac-user-set-password admin -i dash.pass\n</code></pre>"},{"location":"Ceph/CephAuthorization/#update-capabilities","title":"Update Capabilities","text":"<p>Use the ceph auth caps command and the same options as creating an account.  Keeping in mind that any update to the user will overwrite the existing capabilities, therefore make sure you include any existing capabilites that need to be retained along with any additional.</p>"},{"location":"Ceph/CephAuthorization/#delete-account","title":"Delete Account","text":"<pre><code># ceph auth del client.myappuser\n</code></pre> <p>NOTE: You should also remove the /etc/ceph/ceph.client.name.keyring file.</p>"},{"location":"Ceph/CephCrushMap/","title":"Ceph CRUSH Map","text":""},{"location":"Ceph/CephCrushMap/#introduction","title":"Introduction","text":"<p>Ceph calculates which OSDs should hold the objects by using a placement algorithm called CRUSH, Controlled Replication Under Scalable Hashing.  Objects are assigned to placement groups (PGs) and CRUSH determines which OSDs those placement groups should use to store their objects.</p> <p>Crush uses the concept of buckets.  Bucket types include root, region, datacenter, room, pod, pdu, row, rack chassis and host.  You can also had your own types.</p>"},{"location":"Ceph/CephCrushMap/#crush-map-commands","title":"CRUSH Map Commands","text":"<p>Dump the crush map into a binary file and then convert to text file: <pre><code># ceph osd getcrushmap -o /tmp/map.bin\n# crushtool -d /tmp/map.bin -o /tmp/map.txt\n</code></pre></p> <p>Compile a CRUSH map from text file, test it, and then import it: <pre><code># crushtool -c /tmp/map.txt -o /tmp/map.new.bin\n# crushtool -i /tmp/map.new.bin --test\n# ceph osd setcrushmap -i /tmp/map.new.bin\n</code></pre> Sample CRUSH rule to select SSD for primary copy and HDDs for replicas:</p> <p>rule ssd-first {       id 5       type replicated       min_size 1       max_size 10       step take default class ssd       step chooseleaf firstn 1 type host       step emit       step take default class hdd       step chooseleaf firstn -1 type host       step emit   }   </p> <p>Set the OSD Device Class: <pre><code># ceph osd crush set-device-class osd.1 ssd\n</code></pre></p> <p>Remove a device class: <pre><code># ceph osd crush rm-device-class hdd\n</code></pre></p> <p>List configured device classes: <pre><code># ceph osd crush class ls\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#customizing-crush-map","title":"Customizing CRUSH Map","text":"<p>Create buckets: <pre><code># ceph osd crush add-bucket *name* *type* \n</code></pre></p> <p>Organize the buckets <pre><code># ceph osd crush move *bucket* type=*parent*\n</code></pre></p> <p>Place the OSDs as leaves in the correct location: <pre><code># ceph osd crush set osd.1 1.0 root=default rack=rack1 host=hosta\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#adding-crush-rules","title":"Adding CRUSH Rules","text":"<p>Create a replicated rule to start from root, replicate across buckets of type, using devices of class: <pre><code># ceph osd crush rule create-replicated *name* root *type* *class*\n</code></pre></p> <p>Create an erasure-coded pool to start from crush-root, using crush-failure-domain and crush-device-class: <pre><code># ceph osd erasure-code-profile set *myprofile* k=4 m=2 crush-root=root crush-failure-domain=rack crush-device-class=ssd\n</code></pre></p> <p>List EC profiles and CRUSH Rules: <pre><code># ceph osd erasure-code-profile ls\n# ceph osd crush rule ls\n</code></pre></p>"},{"location":"Ceph/CephCrushMap/#calculating-pgs","title":"Calculating PGs","text":"<p>Single pool:</p> <p>totalPGs = (OSDs * 100 ) / numberOfReplicas</p> <p>Red Hat recommends the use of the Ceph Placement Groups per Pool Calculator, https://access.redhat.com/labs/cephpgc/, from the Red Hat Customer Portal Labs.</p>"},{"location":"Ceph/CephCrushMap/#osd-map-commands","title":"OSD Map Commands","text":"<pre><code># ceph osd dump\n# ceph osd getmap -o osd.map.bin\n# osdmaptool --print osd.map.bin\n# osdmaptool --export-crush crush.bin osd.map.bin\n# crushtool -d crush.bin crush.txt\n# crushtool -c crush.txt -o crush.new.bin\n# osdmaptool --import-crush crush.new.bin osd.map.bin\n# osdmaptool --test-map-pgs-dump /tmp/osd.map.bin\n</code></pre>"},{"location":"Ceph/CephFS/","title":"CephFS","text":""},{"location":"Ceph/CephFS/#introduction","title":"Introduction","text":"<p>A single CephFS filesystem can be created with multiple subvolumes defined within the CephFS for fine grained authorization and access control.</p> Option Description r Read access to the specified folder w Write access to the specified folder p Required to use layouts and quotas s Required to create snapshots <p>By default the Fuse client mounts the root directory; use the <code>ceph-fuse -r /directory</code> command to mount a subdirectory.</p>"},{"location":"Ceph/CephFS/#configuration","title":"Configuration","text":"<p>Create the CephFS volume; this creates the pools, the volumes, and starts the MDS service on the hosts. <pre><code># ceph fs volume create fs-name --placement=\"2 ceph01,ceph02\"\n</code></pre></p> <p>Create CephFS Manually: <pre><code># ceph osd pool create cephfs_data [erasure]\n# ceph osd pool create cephfs_metadata \n# ceph osd pool application enable cephfs_data cephfs\n# ceph osd pool application enable cephfs_metadata cephfs\n# ceph fs new fs-name cephfs_metadata cephfs_data\n</code></pre></p> <p>Add erasure coded pool to your CephFS: <pre><code># ceph fs add_data_pool fs-name data-pool\n</code></pre></p> <p>Deploy the CephFS service and verify the service is up: <pre><code># ceph orch apply mds fs-name --placement=\"2 ceph01 ceph02\"\n# ceph mds stat\n</code></pre></p>"},{"location":"Ceph/CephFS/#authorization","title":"Authorization","text":"<p>All clients must be authorized to access the CephFS filesystem.</p> <p>Authorize a client to use extended attributes and snapshots for files in the /mydirectory: <pre><code># ceph fs authorize mycephfs client.user / r /mydirectory rwps\n</code></pre></p> <p>NOTE: When authorizing both extended attributes and snapshots, they must be in alpa order. NOTE: Prevent the deletion of files from UID/GID=0 by using the <code>root_squash</code> option.</p>"},{"location":"Ceph/CephFS/#mounting-with-kernel-client","title":"Mounting with kernel client","text":"<p>Use the kernel client with LINX kernel versions of 4.0 and newer.  You can mount the root filesystem or subdirectories.  </p> <ul> <li>Verify the ceph-common package is installed.</li> </ul> <p>Mount: <pre><code># mount -t ceph mon1,mon2,etc:/path mount-point -o name=client.user1,fs=,mycephfs\n</code></pre></p> <p>NOTE: Subdirectories in the CephFS can be mounted as well. NOTE: You can also leave off the <code>mon1,mon2,etc</code> and Ceph will use the monitors in the /etc/ceph/ceph.conf file.</p> Option Name Description name=name Cephx client ID to use; default is guest fs=fs-name Name of CephFS filesystem to mount secret=secret-value Value of the secret key for the client (no longer needed) secretfile=filename Path to file that contains client secret (no longer needed) rsize=bytes Maximum read size in bytes wsize=bytes Maximum write size in bytes; default none <p>Persistent Mount /etc/fstab Entry: <pre><code>mon1,mon2:/ /mnt/cephfs ceph name=user1,_netdev 0 0\n</code></pre></p>"},{"location":"Ceph/CephFS/#mounting-with-fuse-client","title":"Mounting with Fuse client","text":"<p>Use the FUSE client when the LINUX kernel version is 4.0 or earlier.  Limitation is that you have to mount the root filesystem.  </p> <p>NOTE: This was stated in the online course.  The documentation indicates otherwise.  Use the -r option to instruct the client to treat the path as its root.</p> <ul> <li>Verify the ceph-common and ceph-fuse packages are installed.</li> <li>Verify the /etc/ceph/ceph.conf exists.</li> <li>When using the FUSE client as a non-root user, add user_allow_other in the /etc/fuse.conf file.</li> </ul> <p>Authorize the client; providing read/write access and ability to create snapshots: <pre><code># ceph fs authorize mycephfs client.fsuser / r /fsuser-directory rws\n</code></pre></p> <p>Mount: <pre><code># ceph-fuse /mnt/mycephfs -n client.fsuser --client-fs=mycephfs\n</code></pre></p> <p>Persistent Mount /etc/fstab Entry: <pre><code>mon-hostname:/ /mnt/cephfuse fuse.ceph ceph.id=myuser,_netdev 0 0\n</code></pre> NOTE:  If you don't use the default name and location of the user's keyring file, then use the --keyring option to specify the path to the file.</p>"},{"location":"Ceph/CephFS/#remove-cephfs-filesystem","title":"Remove CephFS Filesystem","text":"<p>Mark CephFS down, then remove it: <pre><code># ceph fs set fs-name down true\n# ceph fs rm fs-name --yes-i-really-mean-it\n</code></pre></p> <p>NOTE: the mon_allow_pool_delete option must be set to true for the above to succeed.  </p>"},{"location":"Ceph/CephFS/#mds-autoscaler","title":"MDS Autoscaler","text":"<p>CephFS requires at least one active MDS server and one standby for HA.  The MDS Autoscaler ensures the availability of enough MDS daemons.  The modules monitors the number of ranks and number of standby daemons and adjusts the number of MDS daemons that the orchestrator spawns.</p> <p>To enable: <pre><code>ceph mgr module enable mds_autoscaler\n</code></pre></p> <p>To set the max number of active MDS daemons: <pre><code>ceph fs set mycephfs max_mds 2  \n</code></pre></p> <p>To set the max number of standby MDS daemons: <pre><code>ceph fs set mycephfs standby_count_wanted 2  \n</code></pre></p>"},{"location":"Ceph/CephFS/#mdf-failover","title":"MDF Failover","text":"<p>When the active MDS becomes unresponsive, a MON daemon waits for the number of seconds specified by the mds_beacon_grace parameter.  If unresponsive after this number of seconds, the MON marks the MDS daemon as laggy and one of the standby daemons becomes active.</p> <pre><code># ceph config get mds mds_beacon_grace\n15.000000\n</code></pre>"},{"location":"Ceph/CephFS/#cephfs-mirror","title":"CephFS Mirror","text":"<p>To enable and deploy the feature: <pre><code># ceph mgr module enable mirroring\n# ceph orch apply cephfs-mirror node-name\n</code></pre></p> <p>For each CephFS peer, you must create a user on the target cluster: <pre><code># ceph fs authorize fs-name client_ / rwps\n</code></pre></p> <p>Enable mirroring on the source cluster: <pre><code># ceph fs snapshot mirror enable fs-name\n</code></pre></p> <p>Prepare the target peer: <pre><code># ceph fs snapshot mirror peer_bootstrap create fs-name peer-name site-name\n</code></pre></p> <p>Import the bootstrap token on the source cluster: <pre><code># ceph fs snapshot mirror peer_bootstrap import fs-name boostrap-token\n</code></pre></p> <p>Configure a directory for snapshot mirroring on the source clsuter: <pre><code>ceph fs snapshot mirror add fs-name path\n</code></pre></p>"},{"location":"Ceph/CephFS/#file-layout","title":"File Layout","text":"Attribute Name Description ceph.[file|dir].layout.pool The pool where Ceph stores the file's or directory's data objects ceph.[file|dir].layout.stripe_unit The size in bytes of a block of data that is used in RAID 0 distribution of a file/directory ceph.[file|dir].layout.stripe_count The number of consecutive stripe units that constitute a RAID 0 stripe ceph.[file|dir].layout.object_size File or directory data is split into RADOS objects of this size in bytes; 4MiB by default ceph.[file|dir].layout.pool_namespace The namespace in the pool that is used; if exists <p>Use the <code>getfattr</code> and <code>setfattr</code> commands to retrieve and set the attributes.  Keeping in mind, a file's attributes cannot be changed once the file is written.  A file's attributes are inherited by the parent.</p> <p>To change the pool from the replicated pool to an erasure coded pool, create the new pool, update allow_ec_overwrites to true, add it to the cephfs, and then set the layout on a new directory.  </p> <p>```</p>"},{"location":"Ceph/CephFS/#ceph-fs-add_data_pool-mycephfs-cephfs_data_ec","title":"ceph fs add_data_pool mycephfs cephfs_data_ec","text":""},{"location":"Ceph/CephFS/#setfattr-n-cephdirlayoutpool-v-cephfs_data_ec-mntmycephfsmynewdirectory","title":"setfattr -n ceph.dir.layout.pool -v cephfs_data_ec /mnt/mycephfs/mynewdirectory","text":""},{"location":"Ceph/CephInstallation/","title":"Ceph v5.x Installation Instructions","text":""},{"location":"Ceph/CephInstallation/#introduction","title":"Introduction","text":""},{"location":"Ceph/CephInstallation/#goals","title":"Goals:","text":"<ul> <li>Enable field on Ceph v5.0</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to Ceph Product management and engineering teams at IBM</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with Ceph</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Ceph/CephInstallation/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Ceph 32G 8 <ul><li>1x pxe</li><li>1x ceph-frontend</li><li>1x ceph-backend</li> <ul><li>64GB</li><li>100GB</li>"},{"location":"Ceph/CephInstallation/#building-your-kni-lab","title":"Building Your KNI Lab","text":""},{"location":"Ceph/CephInstallation/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy Ceph Storage Cluster.  </p> </li> <li> <p>Update the project_name and project_password parameters. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p></p> </li> <li> <p>Wait the deployment to finish which can take up to ~10-15 minutes.  </p> </li> </ol>"},{"location":"Ceph/CephInstallation/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  Update the project_name and project_password along with the networks to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output. </p> <pre><code>external_network: vlan1117\nnetworks:  \n  - { name: \"ceph-frontend\", cidr: \"10.20.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n\u00a0\u00a0- { name: \"ceph-backend\", cidr: \"10.20.1.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 ceph nodes.  Update the project_name and project_password along with the instances to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output.  </p> <pre><code>instances:  \n  - { name: \"ceph1\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n\u00a0\u00a0- { name: \"ceph2\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n  - { name: \"ceph3\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n</code></pre> </li> </ol>"},{"location":"Ceph/CephInstallation/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Ceph/CephInstallation/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0 </p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  Take note of the IP addresses for the VLAN1117 network.  You will use the 172.20.17.X addresses to access the servers.  </p> <p> </p> </li> <li> <p>Start each instance; In the Actions colume, select Start Instance for each node in the cluster.  </p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0  </p> </li> </ol> <p>To access your instance, ssh as the cloud-user using the VLAN IP addresses.  Make sure you are connected to the NA-SSA VPN.</p>"},{"location":"Ceph/CephInstallation/#ceph-v5-installation","title":"Ceph v5. Installation","text":"<p>The full Red Hat documentation for the Ceph installation is available here.  The below precedures are for the OpenInfra Lab environment and have been scaled down to only include the required steps.  </p>"},{"location":"Ceph/CephInstallation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Red Hat Enterprise Linux 8.4 EUS or later.  </li> <li>Ansible 2.9 or later.  </li> <li>Valid Red Hat subsription with the appropriate entitlements.  </li> <li>Root-level access to all nodes.  </li> <li>An active Red Hat Network or service account to access the Red Hat Registry.  </li> </ul> <p>NOTE: Ensure that you are connected to the NA-SSA VPN</p> <ol> <li> <p>Login to ceph1.  Update the /etc/hosts files with the IP and names.</p> <p>NOTE: Your IP address will be different.  </p> <pre><code>ssh cloud-user@172.20.17.117\n$ vi /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n172.20.17.40     ceph1 \n172.20.17.120    ceph2 \n172.20.17.191    ceph3 \n\n10.20.1.190      ceph1-stg\n10.20.1.125      ceph2-stg\n10.20.1.245      ceph3-stg\n</code></pre> </li> <li> <p>Grab the repo from the DNS Utility server</p> <pre><code>$ sudo curl http://172.20.129.10/hextupleo-repo/rhel8.repo -o /etc/yum.repos.d/rhel8.repo\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1379  100  1379    0     0   269k      0 --:--:-- --:--:-- --:--:--  448k\n[cloud-user@ceph1 ~]$ cat /etc/yum.repos.d/rhel8.repo\n[ansible-2.9-for-rhel-8-x86_64-rpms]\nname=ansible-2.9-for-rhel-8-x86_64-rpms\nbaseurl=http://172.20.129.10/repos/ansible-2.9-for-rhel-8-x86_64-rpms/\nenabled=1\ngpgcheck=0\n...\n[rhel-8-for-x86_64-highavailability-rpms]\nname=rhel-8-for-x86_64-highavailability-rpms\nbaseurl=http://172.20.129.10/repos/rhel-8-for-x86_64-highavailability-rpms/\nenabled=1\ngpgcheck=0\n</code></pre> </li> <li> <p>Update all packages using dnf on all servers.</p> <pre><code>$ cat /etc/redhat-release \nRed Hat Enterprise Linux release 8.7 (Ootpa)\n$ uname -a\nLinux ceph1 4.18.0-425.3.1.el8.x86_64 #1 SMP Fri Sep 30 11:45:06 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n$ sudo dnf update -y\n...\nUpgraded:\n  NetworkManager-1:1.40.0-5.el8_7.x86_64                          NetworkManager-libnm-1:1.40.0-5.el8_7.x86_64                 NetworkManager-team-1:1.40.0-5.el8_7.x86_64                                      \n  NetworkManager-tui-1:1.40.0-5.el8_7.x86_64                      authselect-1.2.5-2.el8_7.x86_64                              authselect-compat-1.2.5-2.el8_7.x86_64                                           \n  authselect-libs-1.2.5-2.el8_7.x86_64                            curl-7.61.1-25.el8_7.1.x86_64                                dbus-1:1.12.8-23.el8_7.1.x86_64                                                  \n  dbus-common-1:1.12.8-23.el8_7.1.noarch                          dbus-daemon-1:1.12.8-23.el8_7.1.x86_64                       dbus-libs-1:1.12.8-23.el8_7.1.x86_64                                             \n...\nInstalled:\n  bubblewrap-0.4.0-1.el8.x86_64               fwupd-1.7.8-1.el8.x86_64                  grub2-tools-efi-1:2.02-142.el8_7.1.x86_64 kernel-4.18.0-425.10.1.el8_7.x86_64   kernel-core-4.18.0-425.10.1.el8_7.x86_64\n  kernel-modules-4.18.0-425.10.1.el8_7.x86_64 libatasmart-0.19-14.el8.x86_64            libblockdev-2.24-11.el8.x86_64            libblockdev-crypto-2.24-11.el8.x86_64 libblockdev-fs-2.24-11.el8.x86_64       \n  libblockdev-loop-2.24-11.el8.x86_64         libblockdev-mdraid-2.24-11.el8.x86_64     libblockdev-part-2.24-11.el8.x86_64       libblockdev-swap-2.24-11.el8.x86_64   libblockdev-utils-2.24-11.el8.x86_64    \n  libbytesize-1.4-3.el8.x86_64                libgcab1-1.1-1.el8.x86_64                 libgudev-232-4.el8.x86_64                 libgusb-0.3.0-1.el8.x86_64            libsmbios-2.4.1-2.el8.x86_64            \n...\nComplete!\n$ sudo reboot\nConnection to 172.20.17.117 closed by remote host.\nConnection to 172.20.17.117 closed.\n</code></pre> <p>NOTE: Don't forget to do all servers in the cluster.  </p> </li> <li> <p>Generate the ssh key files for the root user on ceph1.  Update the authorized_keys file on all nodes and append the contents of the id_rsa.pub file.  </p> </li> <li> <p>Install the cephadm-ansible package on ceph1 (or the first node in the cluster).  </p> <pre><code>$ sudo dnf install -y cephadm-ansible\n...\nInstalled:\n  ansible-2.9.27-1.el8ae.noarch                    cephadm-ansible-1.8.0-1.el8cp.noarch                    python3-jmespath-0.9.0-11.el8.noarch                    sshpass-1.09-4.el8.x86_64                   \n\nComplete!\n</code></pre> </li> <li> <p>Create the inventory hosts and registry-login.json files on ceph1.  Change the permissions on the registry-login.json file.</p> <pre><code>$ cd /usr/share/cephadm-ansible \n$ vi hosts\nceph1\nceph2\nceph3\n\n[admin]\nceph1\n$ sudo mkdir /root/ceph\n$ sudo vi /root/ceph/registry.json\n{\n \"url\":\"registry.redhat.io\",\n \"username\":\"myuser1\",\n \"password\":\"mypassword1\"\n}\n$ sudo chmod 600 registry.json     \n</code></pre> <p>NOTE: The user name is the user name that you use to login to registry.redhat.io.  This is used to download the ceph containers.</p> </li> </ol>"},{"location":"Ceph/CephInstallation/#installation","title":"Installation","text":"<ol> <li> <p>Run the Ceph ansible preflight playbook.  </p> <pre><code># sudo -i\n# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars \"ceph_origin=\"\n</code></pre> </li> <li> <p>Create the bootstrap configuration file on ceph1 (or first node in the cluster).</p> <pre><code>service_type: host\naddr: ceph1\nhostname: ceph1\n---\nservice_type: host\naddr: ceph2\nhostname: ceph2\n---\nservice_type: host\naddr: ceph3\nhostname: ceph3\n---\nservice_type: mon\nplacement:\n  host_pattern: \"ceph[1-3]\"\n---\nservice_type: osd\nservice_id: initial_osds\nplacement:\n  host_pattern: \"ceph[1-3]\"\ndata_devices:\n  paths:\n   - /dev/vdb\n</code></pre> </li> <li> <p>Run the cephadm bootstrap command.  </p> <pre><code># cephadm bootstrap --mon-ip 172.20.17.40 --apply-spec /root/ceph/initial-cluster-config.yaml --initial-dashboard-password changeme --dashboard-password-noupdate --registry-json /root/ceph/registry-login.json --cluster-network 10.20.1.0/24\n</code></pre> </li> <li> <p>Once the bootstrap is complete, check the status of the cluster with the <code>ceph status</code> command.  </p> </li> <li> <p>If firewalld is enabled, ensure the following ports are opened on all nodes that run the <code>MON</code> and/or <code>OSD</code> service:  </p> <p>MON:</p> <pre><code># firewall-cmd --zone-public --add-port=6789/tcp\n# firewall-cmd --zone-public --add-port=6789/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-public --add-service=ceph-mon\n# firewall-cmd --zone-public --add-service=ceph-mon --permanent\n</code></pre> <p>OSD:</p> <pre><code># firewall-cmd --zone-public --add-port=6800-7300/tcp\n# firewall-cmd --zone-public --add-port=6800-7300/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-[public|cluster] --add-service=ceph\n# firewall-cmd --zone-[public|cluster] --add-service=ceph --permanent\n</code></pre> </li> <li> <p>Ensure the MTU size is set to 9000 on the network interfaces.</p> <pre><code># nmcli conn modify 'eth0' 802-3-ethernet.mtu 9000\n# nmcli conn down 'eth0'\n# nmcli conn up 'eth0'\n# ip link show 'eth0\n</code></pre> </li> <li> <p>Set the labels for the servers.</p> </li> </ol> <pre><code># ceph orch host ls\nHOST           ADDR         LABELS                  STATUS  \ncephstorage01  172.20.0.11  _admin mon mgr grafana          \ncephstorage02  172.20.0.12  mon mgr rgw _admin              \ncephstorage03  172.20.0.13  mon mgr rgw                     \ncephstorage04  172.20.0.14  mon mgr rgw                     \ncephstorage05  172.20.0.15                                  \ncephstorage06  172.20.0.16                                  \n6 hosts in cluster\n\n# ceph orch host label add cephstorage01 mgr\n</code></pre> <p>Note: Labels can be mon, mgr, rgw, admin, or whatever you choose.</p>"},{"location":"Ceph/CephInstallation/#appendix","title":"Appendix","text":""},{"location":"Ceph/CephInstallation/#export-service-specification","title":"Export Service Specification","text":"<pre><code>ceph orch ls --service_type type --service_name name --export\n</code></pre>"},{"location":"Ceph/CephInstallation/#create-floating-ip-address","title":"Create floating IP address","text":"<pre><code>[stack@bgp-undercloud ~] openstack floating ip create --subnet 372459e8-25f9-4885-b71a-6889ffff02bf --project ceph-blm 18743df0-57aa-4571-9d62-439e0570b059\n</code></pre>"},{"location":"Ceph/CephLogging/","title":"Ceph Logging","text":"<p>For container-based deployments, by default, Ceph daemons logs to stderr and logs are captured by the container runtime environment.  These logs are sent to journald and are accessible through the <code>journalctl</code> command.  </p> <p>Ceph logging levels operate on a scale of 1 (terse) to 20 (verbose).  Use a single value for the log level and memory level to set them both to the same value or use different values for output log level and memory level separating the values with a forward slash (i.e. debug_mon = 1/5 sets log level to 1 and memory level to 5).  </p> <p>Get list of Ceph daemons, view logs:  <pre><code># systemctl list-units \"ceph*\"\n# journalctl -eu ceph-dd6eede2-e3a6-11ed-9d9d-fa163ee4fccc@mgr.ceph1.exaerd.service\n</code></pre></p> <p>To enable logging to files instead of journald, set <code>log_to_file</code> to true.  The will create the Ceph log files in /var/log/ceph/fsid.  </p> <p>By default, Cephadm sets up log rotation on each host to rotate these files. You can configure the logging retention schedule by modifying /etc/logrotate.d/ceph.CLUSTER_FSID.</p> <pre><code># ceph config set global log_to_file true \n# ceph config set log_to_stderr false\n</code></pre> <p>NOTE: Make sure to set log_to_stderr false to avoid double logging.</p>"},{"location":"Ceph/CephLogging/#running-configuration-vs-configuration-database-vs-configuration-file","title":"Running Configuration vs. Configuration Database vs. Configuration File","text":"<p>Ceph manages the configuration database of options which centralizes management by storing options in key value pairs.  But there are a handful of Ceph options that can be defined in the local Ceph configuration file, <code>/etc/ceph/ceph.conf</code>.  These few config options control how other Ceph components connect to the monitors to authenticate and fetch the configuration information from the database.  </p> <p>When the same option exists in the config database and the config file, the config database option has a lower priority; the config file takes precedence.  </p> <p>To get the current running configuration use the <code>ceph config show</code> command.  </p> <pre><code># ceph config show mon.osp-blm-controller-1  \n</code></pre>"},{"location":"Ceph/CephLogging/#configuration-checks","title":"Configuration Checks","text":"<p>Cephadm periodically scans each of the hosts in the storage cluster, to understand the state of the OS, disks, and NICs . These facts are analyzed for consistency across the hosts in the storage cluster to identify any configuration anomalies. The configuration checks are an optional feature.  </p> Config Check Description CEPHADM_CHECK_KERNEL_LSM Check to ensure all hosts are running SELINUX CEPHADM_CHECK_SUBSCRIPTION Verifies valid subscriptions CEPHADM_CHECK_PUBLIC_MEMBERSHIP At least one NIC is configured on public network CEPHADM_CHECK_MTU Ensure all OSD nodes are running the same MTU size CEPHADM_CHECK_LINKSPEED Ensure all nodes are running the same link speeds CEPHADM_CHECK_NETWORK_MISSING Ensure all nodes are running configured for the same subnets CEPHADM_CHECK_CEPH_RELEASE Ensure all nodes are running the same Ceph version CEPHADM_CHECK_KERNEL_VERSION Ensure the OS kernel version is consistent across the nodes <pre><code># ceph config set mgr mgr/cephadm/config_checks_enabled\n\n# ceph cephadm config-check status\n# ceph cephadm config-check ls\n</code></pre>"},{"location":"Ceph/CephLogging/#monitor-cephadm-log-messages","title":"Monitor cephadm Log Messages","text":"<pre><code>ceph config set mgr mgr/cephadm/log_to_cluster_level [debug|info]\nceph -W cephadm --watch-debug\nceph -W cephadm --verbose\n</code></pre>"},{"location":"Ceph/CephLogging/#logging-to-stderr","title":"Logging to <code>stderr</code>","text":"<pre><code>journalctl -u ceph-&lt;fsid&gt;@&lt;hostName&gt;\n</code></pre>"},{"location":"Ceph/CephLogging/#view-boot-log","title":"View boot log","text":"<pre><code>journalctl -b \n</code></pre>"},{"location":"Ceph/CephLogging/#data-location","title":"Data Location","text":"<p>Cephadm daemon data and logs are located in:</p> <ul> <li>/var/log/ceph/ - storage cluster logs (usually not present when logging to stderr) <li>/var/lib/ceph/ - cluster daemon data besides logs <li>/var/lib/ceph// - data for specific daemon <li>/var/lib/ceph//crash - crash reports for the storage cluster <li>/var/lib/ceph//removed - old daemon data that have been removed by cephadm"},{"location":"Ceph/CephNetworking/","title":"Ceph Networking","text":""},{"location":"Ceph/CephNetworking/#baremetal-lab-network-bonds","title":"Baremetal Lab Network Bonds","text":"<pre><code>nmcli conn add type bond con-name bond1 ifname bond1 bond.options \"mode=802.3ad,miimon=100\"\nnmcli conn add type ethernet slave-type bond con-name bond-ens4f0 ifname ens4f0 master bond1\nnmcli conn add type ethernet slave-type bond con-name bond-ens4f1 ifname ens4f1 master bond1\nnmcli conn add type vlan con-name vlan1100 dev bond1 id 1100 ip4 172.20.0.11/24 gw4 172.20.0.1 \nnmcli conn add type vlan con-name vlan1101 dev bond1 id 1101 ip4 172.20.1.21/24 \nnmcli conn del ens4f0\nnmcli conn del ens4f1\nnmcli conn mod bond1 ipv4.method disabled\nnmcli conn mod bond1 ipv6.method disabled\nnmcli conn mod bond1 802-3-ethernet.mtu 9000\nnmcli conn mod vlan1100 ipv4.dns 172.20.129.10\nnmcli conn mod vlan1101 ipv4.dns 172.20.129.10\nnmcli conn mod bond1 bond.options 'mode=4,lacp_rate=fast,updelay=1000,miimon=100,xmit_hash_policy=layer3+4'\nnmcli conn reload\nnmcli networking off &amp;&amp; nmcli networking on\n</code></pre>"},{"location":"Ceph/CephNetworking/#routes","title":"Routes","text":"<p>Add a route to network via gateway on device interface <pre><code>ip route add 10.20.0.0/24 via 10.40.0.1 dev eth1\n</code></pre></p> <p>Using NMCLI: <pre><code>nmcli conn modify eth1 _ipv4.routes \"10.20.0.0/24 10.40.0.1\"\n</code></pre></p>"},{"location":"Ceph/CephOSD/","title":"Ceph OSD","text":""},{"location":"Ceph/CephOSD/#cpu-requirements","title":"CPU Requirements","text":"<p>NVMe - 10 cores per OSD Non-NVMe SSD - 2 cores per OSD HDD - .5 core per OSD  </p> <p>HDD: sockets * cores * GHz * 100 = estimated 4k randrw performance SSD: sockets * cores * GHz * 1500 = estimated 4k randrw performance  </p>"},{"location":"Ceph/CephOSD/#drive-recommendations","title":"Drive Recommendations","text":"<p>SSD Interface - NVMe SSD Technology - TLC  DWPD - 3   </p> <p>DWPD - Drive Writes per Day: how many times an SSD's usable capacity can be overwritten in 24-hours w/in the specified lifetime. TBW - TeraBytes Written: total amount of data written to the SSD over the specified lifetime.  </p> Capability QLC TLC MLC SLC Architecture 4-bits 3-bits 2-bits 1-bit Capacity Highest High Medium Lowest Endurance Low Medium High Highest Cost $ $$ $$$ $$$$ DWPD to TBW Conversion TBW to DWPD Conversion TBW = DWPD * SSD Capacity(TB) * Lifetime(years) * 365 Days DWPD = TBW / (SSD Capacity(TB) * Lifetime(years) * 365 Days)"},{"location":"Ceph/CephOSD/#osd-flags","title":"OSD Flags","text":"<p>Backfill - adding/removing OSDs to the cluster.  Backfill does a per object comparison.  No blocked IO/writes. Recovery - when OSDs crash/fail and come back online.  Recovery uses the pglogs to determine changed objects. Rebalance - optimize the allocation of PGs.  </p> <p>Minimize the Data Pause during OSD or node loss Differences between backfill and recovery</p>"},{"location":"Ceph/CephOSD/#osd-configuration-and-bluestore","title":"OSD Configuration and Bluestore","text":"<p>Use the .asok socket to pull information from the OSD daemons.  The bluefs stats command will show the </p> <pre><code># pwd\n/var/run/ceph/8484edfa-8c65-11ed-a639-90e2babd60b8\n# ceph --admin-daemon ./ceph-osd.5.asok config show\n# ceph --admin-daemon ./ceph-osd.5.asok bluefs stats\n</code></pre>"},{"location":"Ceph/CephOSD/#add-osd-to-the-ceph-cluster","title":"Add OSD to the Ceph Cluster","text":"<pre><code># ceph orch daemon add osd serverName:/dev/vda\nCreated osd(s) 9 on host \u2018serverName\u2019\n</code></pre> <p>Details of OSD: <pre><code># ceph osd find 9\n{\n   \u201cOsd\u201d: 9\n   \u201cAdds\u201d:  {\n        \u201cAddrvec\u201d: [\n\u2026\n}\n</code></pre></p>"},{"location":"Ceph/CephOSD/#remove-osd-from-ceph-cluster","title":"Remove OSD from Ceph Cluster","text":"<p>Mark the OSD 'out'; this may already be the status. Mark the OSD 'down' Remove the OSD Once removed it will be in the DNE (does not exists status) Remove the OSD from the crush map Remove the authentication/authorization keys  </p> <pre><code># ceph osd out osd.XX\n# ceph osd down osd.XX\n# ceph osd rm osd.XX\n# ceph osd crush rm osd.XX\n# ceph auth del osd.XX\n</code></pre>"},{"location":"Ceph/CephOSD/#replace-osd","title":"Replace OSD","text":"<pre><code>ceph orch daemon rm osd.xx --zap --replace\n</code></pre> <p>Use the following command to ensure the device can be reused if necessary: <pre><code>ceph orch device zap --force 'serverName' /dev/sdx\n</code></pre></p>"},{"location":"Ceph/CephOSD/#manage-osds","title":"Manage OSDs","text":"<p>Start|stop|restart OSD daemon: <pre><code># ceph orch daemon [start|stop|restart] osd.13\nScheduled to stop osd.13 on host \u2018ServerName\u2019 \n</code></pre></p> <p>Verify the correct OSD service is stopped <pre><code># ceph orch ps serverName \u2014daemon-type osd \n</code></pre></p>"},{"location":"Ceph/CephOSD/#updated-osd-device-class","title":"Updated OSD Device Class:","text":"<pre><code>for i in {0..59}\ndo \n    ceph osd crush rm-device-class osd.$i\n    ceph osd crush set-device-class ssd osd.$i\ndone\n</code></pre>"},{"location":"Ceph/CephOSD/#encryption","title":"Encryption","text":"<p>LUKS keys to unencrypt the drives are stored in the MON.  The key to authenticate to the MON is a cephx key. <pre><code>client.osd-lockbox.f7626ae7-a6df-4229-99c5-aaccb1aebf5c\n    key: AQAm2bVjC/V4ABAA+DIzvkQh7MqoSsUreWnmAQ==\n    caps: [mon] allow command \"config-key get\" with key=\"dm-crypt/osd/f7626ae7-a6df-4229-99c5-aaccb1aebf5c/luks\"\n</code></pre></p>"},{"location":"Ceph/CephOSD/#deepscrub","title":"Deepscrub","text":"<pre><code>ceph pg dump pgs_brief | grep deep\ndumped pgs_brief\n4.2d     active+clean+scrubbing+deep           [47,11,58]          47           [47,11,58]              47\n4.27     active+clean+scrubbing+deep           [51,44,48]          51           [51,44,48]              51\n3.5      active+clean+scrubbing+deep           [55,40,19]          55           [55,40,19]              55\n5.1a     active+clean+scrubbing+deep           [46,57,43]          46           [46,57,43]              46\n32.19    active+clean+scrubbing+deep            [41,15,1]          41            [41,15,1]              41\n4.45     active+clean+scrubbing+deep             [18,4,2]          18             [18,4,2]              18\n4.62     active+clean+scrubbing+deep            [38,25,9]          38            [38,25,9]              38\n3.6f     active+clean+scrubbing+deep            [12,33,3]          12            [12,33,3]              12\n</code></pre>"},{"location":"Ceph/CephOSD/#backfill","title":"Backfill","text":"<pre><code>ceph pg dump pgs_brief | grep backfill\n</code></pre>"},{"location":"Ceph/CephOSD/#ceph-pools","title":"Ceph Pools","text":""},{"location":"Ceph/CephOSD/#primary-affinity","title":"Primary Affinity","text":"<p>Manually control the selection of an OSD as the primary for a PG by setting the primary_affinity.  Mitigate issues or bottlenecks by configuring the cluster to avoid using slow disks or controllers for a primary OSD.</p> <p><pre><code># ceph osd primary-affinity &lt;osd-number&gt; &lt;affinity&gt;  \n</code></pre> Where affinity is a real number between 0 and 1.  </p>"},{"location":"Ceph/CephOSD/#create-pool","title":"Create Pool","text":"<p>Replicated Pool: <pre><code># ceph osd pool create &lt;name&gt; &lt;pg_num&gt; &lt;pgp_num&gt; replicated &lt;CRUSH_ruleSet&gt;\n</code></pre></p> <p>Where pg_num is the total configured number of placement groups and pgp_num is the effective number of placement groups.  This should be the same. For the CRUSH_ruleSet the osd_pool_default_crush_replicated_ruleset configuration parameter sets the default values.</p> <p>Erasure Coded Pool: <pre><code># ceph osd pool create &lt;name&gt; &lt;pg_num&gt; &lt;pgp_num&gt; erasure &lt;erasureProfile&gt; &lt;CRUSH_ruleSet&gt;\n</code></pre></p> <p>Where erasureProfile is the name of the profile created with the ceph osd erasure-code-profile set command.  The profile contains the k and m values and the erasure code plug in to use.</p> <ul> <li>plugin - optional; erasure coding algorithm</li> <li>directory - optional; location of plug in library (/usr/lib64/ceph/erasure-code) </li> <li>crush-failure-domain - optional; controls chunk placement, default is hosts can be set to osd or rack</li> <li>crush-device-class - optional; select OSDs backed by devices of this class for the pool where class is hdd, ssd, name</li> <li>crush-root - optional; set the root node of the CRUSH rule set </li> </ul>"},{"location":"Ceph/CephOSD/#auto-scaling","title":"Auto-scaling","text":"<p>The pg_autoscale_mode property is enabled by default and allows Ceph to make recommendations and automatically adjust the pg_num and pgp_num parameters.  This can be changed on a per pool basis:</p> <p>Status: <pre><code># ceph osd pool autoscale-status\n</code></pre></p> <p>Set autoscale on specific pool: <pre><code># ceph osd pool get &lt;name&gt; pg_autoscale_mode [off|on|warn]\n</code></pre></p> <p>Get the default autoscale mode: <pre><code># ceph config get mon osd_pool_default_pg_autoscale_mode\n</code></pre></p>"},{"location":"Ceph/CephOSD/#management","title":"Management","text":"<p>Set replicas for the pool: <pre><code># ceph osd pool set &lt;name&gt; size|min_size &lt;int&gt;\n</code></pre></p> <p>Set application type for the pool: <pre><code># ceph osd pool application enable &lt;poolName&gt; [rbd|rgw|cephfs]\n</code></pre></p> <p>List erasure-code-profiles <pre><code># ceph osd erasure-code-profile ls\n</code></pre></p> <p>Get a profile\u2019s details: <pre><code># ceph osd erasure-code-profile get\n</code></pre></p> <p>Create erasure-code-profile <pre><code># ceph osd erasure-code-profile set &lt;name&gt; k=4 m=2 \n</code></pre></p> <p>Set quotas: <pre><code># ceph osd pool &lt;name&gt; set-quota\n</code></pre></p> <p>Rename pool: <pre><code># ceph osd pool rename &lt;oldName&gt; &lt;newName&gt;\n</code></pre></p> <p>Delete the pool: <pre><code># ceph config get mon mon_allow_pool_delete\n# ceph tell mon.* config set mon_allow_pool_delete true\n# ceph osd pool delete &lt;name&gt; &lt;name&gt; \u2014yes-i-really-really-mean-it\n</code></pre></p> <p>Details of a placement group: <pre><code># ceph pg &lt;pgID&gt; query\n</code></pre></p> <p>To have ceph manage all devices automatically: <pre><code># ceph orch apply osd \u2014all-available-devices\n</code></pre> This creates a systemd service as osd.all-available-devices (ceph orch ls)</p>"},{"location":"Ceph/CephOSD/#nvme-firmware-update","title":"NVMe Firmware Update","text":"<p>Grab the ISO file from Samsung Website. Mount the ISO file: <pre><code>mount ./Samsung_SSD_970_EVO_Plus_4B2QEXM7.iso /mnt/iso\n</code></pre></p> <p>Extract the ISO and run the fumagician command: <pre><code>mkdir /tmp/fwupdate; cd /tmp/fwupdate\ngzip -dc /mnt/iso/initrd | cpio -idv --no-absolute-filenames\ncd root/fumagician\n./fumagician\n</code></pre></p>"},{"location":"Ceph/CephOSD/#performance-testing","title":"Performance Testing","text":""},{"location":"Ceph/CephOSD/#fio-utility","title":"FIO Utility","text":"<p>Random Write Testing (no caching): <pre><code>fio --name=rbd-test-direct1 --ioengine=libaio --direct=1 --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre></p> <p>With caching: <pre><code>fio --name=rbd-test-direct0 --ioengine=libaio --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre></p> <p>Using input file:</p> <pre><code>cat rbd.fio\n[global]\nioengine=rbd\nclientname=admin\npool=rbd\nrbdname=myimage\nrw=randwrite\nbs=4k\n[rbd_iodepth32]\niodepth=32\n\nfio ./rbd.fio\n</code></pre>"},{"location":"Ceph/CephOSD/#rados-bench-tool","title":"Rados Bench Tool","text":"<p>Random Write Testing (leave data) using 16 concurrent threads of 4194304 bytes for 10 seconds: <pre><code>rados bench -p rbd 10 write --no-cleanup\n</code></pre></p>"},{"location":"Ceph/CephOSD/#dd-utility","title":"DD Utility","text":"<p>Write 1 Gib of data 4 times (no caching): <pre><code>dd if=/dev/zero of=here bs=1G count=4 oflag=direct\n</code></pre></p> <p>Wipe an msdos label from device: <pre><code># dd if=/dev/zero of=/dev/vdb bs=512 count=1\n</code></pre></p>"},{"location":"Ceph/CephRGW/","title":"Ceph RGW Gateway","text":""},{"location":"Ceph/CephRGW/#introduction-information","title":"Introduction / Information","text":"<p>Pool created for RGW:</p> Pool Name Description .rgw.root Stores information records default.rgw.control Control pool default.rgw.meta User keys and critical metadata default.rgw.log Logs of all buckets and object actions default.rgw.buckets.index Indexes of buckets default.rgw.buckets.data Bucket data default.rgw.bucket.non-ec MPU uploads"},{"location":"Ceph/CephRGW/#service-specification-file","title":"Service Specification File","text":"<pre><code># vi rgw-service.yaml\nservice_type: rgw\nservice_id: myrealm.myzone\nservice_name: rgw.myrealm.myzone\nplacement:\n  count: 4\n  hosts:\n  - serverd.lab.example.com\n  - servere.lab.example.com\nspec:\n  rgw_frontend_port: 8080\n  rgw_realm: realm_name\n  rgw_zone: zone_name\n  ssl: true\n  rgw_frontend_ssl_certificate: |\n    -----BEGIN PRIVATE KEY-----\n    ...\n    -----END PRIVATE KEY-----\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n  networks:\n    - 172.20.17.0/24    \n</code></pre> <p>Deploy the service: <pre><code># ceph orch apply -i rgw-service.yaml\nScheduled rgw.myrealm.myzone update...\n</code></pre></p>"},{"location":"Ceph/CephRGW/#multisite-deployment","title":"Multisite Deployment","text":""},{"location":"Ceph/CephRGW/#configuration","title":"Configuration","text":"<p>Zone: backed by its own RHCS cluster with one or more RGWs associated with it. Zone Group: set of one or more zones.  Data replicated to all zones in the zone group with one zone being the primary zone. Realm: Represents the global namespace; contains one or more zone groups; one zone group is the primary zone group.  </p> <p>Configure the primary zone:</p> <ul> <li>Create a realm  </li> <li>Create a primary zone group  </li> <li>Create primary zone  </li> <li>Create system user  </li> <li>Commit the changes  </li> <li>Create RGW service for the primary zone  </li> <li>Udpate the configuration database  </li> </ul> <pre><code># radosgw-admin realm create --default --rgw-realm=gold\n# radosgw-admin zonegroup create --rgw-zonegroup=us --master --default --endpoints=http://cephstorage01:80\n# radosgw-admin zone create --rgw-zone=datacenter01 --master --rgw-zonegroup=us --endpoints=http://cephstorage01:80 --access-key=blah --secret=blahblah --default\n# radosgw-admin user create --uid=sysadm --display-name=\"SysAdmin\" --access-key=blah --secret=unknown --system\n# radosgw-admin period update --commit\n# ceph orch apply rgw gold-service --realm=gold --zone=datacenter01 --placement=\"1 cephstorage01\"\n# ceph config set client.rgw rgw_zone datacenter01\n</code></pre> <p>Configure the secondary zone:  </p> <ul> <li>Pull the realm configuration  </li> <li>Pull the period  </li> <li>Create a secondary zone  </li> <li>Commit the changes  </li> <li>Create RGW service for the secondary zone  </li> <li>Update the configuration database  </li> </ul> <pre><code># radosgw-admin realm pull --rgw-realm=gold --url=http://cephstorage01:80 --access-key=blah --secret=unknown --default\n# radosgw-admin period pull --url=http://cephstorage01:8000 --access-key=blah --secret=unknown \n# radosgw-admin zone create --rgw-zone=datacenter02 --master --rgw-zonegroup=us --endpoints=http://cephstg01:80 --access-key=blah --secret=blahblah --default\n# radosgw-admin period update --commit\n# ceph orch apply rgw gold-service --realm=gold --zone=datacenter02 --placement=\"1 cephstg01\"\n# ceph config set client.rgw rgw_zone datacenter02\n</code></pre>"},{"location":"Ceph/CephRGW/#failover","title":"Failover","text":"<p>During a primary site failure, the secondary site continues to serve read and write operations but new buckets and users cannot be created unless a secondary site is promoted as a replacement.</p> <p>To promote a secondary zone, modify the zone and zone group, commit the period. <pre><code># radosgw-admin zone modify --master --rgw-zone=datacenter02\n# radosgw-admin zonegroup modify --rgw-zonegroup=us --endpoints=http://cephstg01:80\n# radosgw-admin period update --commit\n</code></pre></p>"},{"location":"Ceph/CephRGW/#s3-api","title":"S3 API","text":""},{"location":"Ceph/CephRGW/#user-authentication","title":"User Authentication","text":"<p>RADOS Gateway supports only a subset of the Amazon S3 API policy language applied to buckets. No policy support is available for users, groups, or roles. Bucket policies are managed through standard S3 operations rather than using the radosgw-admin command.</p> <p>Create an RGW User: <pre><code>radosgw-admin user create --uid=s3user --display-name=\"User Name\" --email=user@example.com [--access-key=blah] [--secret=unknown]  \n</code></pre></p> <p>NOTE: If the access key and secret are not specified, the system will generate them.</p> <p>Regenerate only the secret key of an existing user: <pre><code># radosgw-admin key create --uid=s3user --access-key=\"8PI2D9ARWNGJI99K8TOS\" --gen-secret\n</code></pre></p> <p>Generate additional access key for an existing user: <pre><code># radosgw-admin key create --uid=s3user --gen-access-key\n</code></pre></p> <p>Remove an access key from a user: <pre><code># radosgw-admin key rm --uid=s3user --access-key=\"8PI2D9ARWNGJI99K8TOS\"\n</code></pre></p> <p>Disable or enable an S3 user: <pre><code># radosgw-admin user [suspend|enable] --uid=s3user \n</code></pre></p> <p>Modify user's access level: <pre><code># radosgw-admin user modify --uid=s3user --access=[read|write|readwrite|full]\n</code></pre></p> <p>NOTE: The <code>full</code> access includes readwrite and the access control management capability.</p>"},{"location":"Ceph/CephRGW/#quotas","title":"Quotas","text":"<p>Set quota on number of objects; scope of user: <pre><code># radosgw-admin quota set --quota-scope=user --uid=s3user --max-objects=1000\n# radosgw-admin quota enable --quota-scope=user --uid=s3user\n</code></pre></p> <p>Set quota on size for loghistory; scope of bucket: <pre><code># radosgw-admin quota set --quota-scope=bucket --uid=loghistory --max-objects=1024B\n# radosgw-admin quota enable --quota-scope=bucket --uid=loghistory\n</code></pre></p> <p>Global quotas: <pre><code># radosgw-admin global quota set --quota-scope=bucket --max-objects=2500\n# radosgw-admin global quota enable --quota-scope=bucket \n</code></pre></p> <p>NOTE: Use the radosgw-admin period update --commit command to commit the changes or alternatively restart the RGW instances to pick up the change.  </p> <p>Retrieve user info and stats: <pre><code># radosgw-admin user info --uid=s3user\n# radosgw-admin user stats --uid=s3user \n</code></pre></p> <p>Usage statistics of a user at a specific time: <pre><code># radosgw-admin usage show --uid=s3user --start-date=start --end-date=end\n</code></pre></p>"},{"location":"Ceph/CephRGW/#url-access-method","title":"URL Access Method","text":"<p>Default access method is http://server.example.com/bucket.  To enable wildcard URLs using the http://bucket.server.example.com method, set the <code>rgw_dns_name</code> parameter.  </p> <pre><code># ceph config set client.rgw rgw_dns_name dns_suffix\n</code></pre> <p>NOTE: Where dns_suffix is the FQDN to be used.  </p>"},{"location":"Ceph/CephRGW/#bucket-metadata","title":"Bucket Metadata","text":"<p>View the metadata of a bucket: <pre><code># radosgw-admin metadata get bucket:testbucket\n{\n    \"key\": \"bucket:testbucket\",\n    \"ver\": {\n        \"tag\": \"_hLrerQczwHjyMJIyj-TAdNU\",\n        \"ver\": 1\n    },\n    \"mtime\": \"2023-04-20T18:40:27.109229Z\",\n    \"data\": {\n        \"bucket\": {\n            \"name\": \"testbucket\",\n            \"marker\": \"e85a3c82-f275-441f-acc1-e117e6b23cb9.28819.1\",\n            \"bucket_id\": \"e85a3c82-f275-441f-acc1-e117e6b23cb9.28819.1\",\n            \"tenant\": \"\",\n            \"explicit_placement\": {\n                \"data_pool\": \"\",\n                \"data_extra_pool\": \"\",\n                \"index_pool\": \"\"\n            }\n        },\n        \"owner\": \"operator\",\n        \"creation_time\": \"2023-04-20T18:40:23.269355Z\",\n        \"linked\": \"true\",\n        \"has_bucket_info\": \"false\"\n    }\n}\n</code></pre></p>"},{"location":"Ceph/CephRGW/#swift-api","title":"Swift API","text":"<p>Install the swift client: <pre><code># sudo pip3 install --upgrade python-swiftclient\n</code></pre></p> <p>Create a subuser: <pre><code># radosgw-admin subuser create --uid=s3user --subuser=username:swift --access=[read|write|readwrite|full]\n</code></pre></p> <p>NOTE: --uid specifies an existing account.e   </p> <p>Create swift authentication key: <pre><code># radosgw-admin key create --subuser=username:swift --key-type=swift --secret=unknown\n</code></pre></p> <p>Remove swift subuser key:  <pre><code># radosgw-admin key rm --subuser=username:swift \n</code></pre></p> <p>Modify swift user access: <pre><code># radosgw-admin subuser modify --subuser=username:swift --access=[read|write|readwrite|full]\n</code></pre></p> <p>Remove swift user access: <pre><code># radosgw-admin subuser rm --subuser=username:swift [--purge-data] [--purge-keys]\n</code></pre></p> <p>Creating users within tenants: <pre><code># radosgw-admin user create --tenant devtenant --uid=devuser --display-name=\"A Swift User\" --subuser=devuser:swift --key-type swift --secret=unknown\n</code></pre></p> <p>NOTE: Any further reference to the subuser, must include the tenant (i.e. devtenant$devuser:swift)</p> <p>Get info/List containers <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown stat [container]\n# swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown list\n</code></pre></p> <p>Create a container: <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown post mycontainer\n</code></pre></p> <p>Upload an object to a container: <pre><code># swift -A http://172.20.17.120:8080/auth/1.0 -U operator:swift -K unknown upload mycontainer /tmp/10B.bin\n</code></pre></p>"},{"location":"Ceph/CephRados/","title":"Ceph RADOS Block Devices","text":""},{"location":"Ceph/CephRados/#introduction","title":"Introduction","text":"<p>To manage the RBD images, use the <code>rbd</code> command to create, list, retrieve info, resize, and remvoe block devices in the Ceph Storage platform.</p> <p>The rbd_default_pool parameter specifies the name of the default pool used to store RBD images.  Use the command <code>ceph config get osd rbd_default_pool</code> to get the default pool name.</p> <p>Create an erasure coded RBD pool on SSDs and initialize the pool for RBD images: <pre><code># ceph osd erasure-code-profile set ecrbd k=4 m=2 crush-device-class=ssd\n# ceph osd pool create customrbd 32 32 ecrbd\n# ceph osd pool application enable ecrbd rbd\n# rbd pool init\n</code></pre></p> <p>Create an RBD image: <pre><code># rbd create test_pool/image1 --size=128M\n</code></pre></p>"},{"location":"Ceph/CephRados/#accessing-the-rbd-image-via-kernel-client-krbd","title":"Accessing the RBD image via Kernel Client (krbd)","text":"<p>Use the rbd map/unmap commands to map the RBD devices to /dev/rbdx devices <pre><code># rbd map rbd/imagename\n# rbd unmap /dev/rbd0\n</code></pre></p> <p>Show mapped devices: <pre><code># rbd showmapped\n</code></pre></p> <p>Setting up Automount: <pre><code># vi /etc/ceph/rbdmap\n# RbdDevice     Parameters\n#poolname/imagename id=client,keyring=/etc/ceph/ceph.client.keyring\ntest_pool/image1    id=testpool,keyring=/etc/ceph/ceph.client.testpool.keyring\n:wq\n# vi /etc/fstab\nUUID=e4f488bd-5775-4c5e-b376-eee627699f2b   /   xfs defaults    0   0\nUUID=7B77-95E7  /boot/efi   vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt  0   2\n/dev/rbd/testpool/image1    /mnt/image1    xfs   noauto 0 0 \n:wq\n# rbdmap map\n# rbd showmapped\nid  pool       namespace  image   snap  device   \n0   test_pool             image1  -     /dev/rbd0\n# systemctl enable rbdmap\n</code></pre></p>"},{"location":"Ceph/CephRados/#snapshots-and-cloning","title":"Snapshots and Cloning","text":"Name Description layering To enable cloning striping v2 support for enhanced performance exclusive lock Required when using RBD Mirroring object-map Object map support (requires exclusive lock) fast-diff Requires object map and exclusive lock deep-flatten Flattens all snapshots of the image journaling Required when using RBD Mirroring data-pool EC data pool support <p>NOTE: Must use fsfreeze --freeze to halt all I/O to the image before taking a snapshot.  Use the --unfreeze parameter to resume filesystem operations.</p> <p>NOTE: Deleting an RBD image fails if a snapshot exists.  Use <code>rbd snap purge</code> to delete the snapshots.</p>"},{"location":"Ceph/CephRados/#snapshots","title":"Snapshots","text":"<p>Snapshots are read-only copies of an RBD image created at a specific point in time.</p> <p>Creating a snapshot: <pre><code># rbd snap create pool/image@snapname\n# rbd snap ls pool/image\n</code></pre></p> <p>Rollback to a previous snapshot: <pre><code># rbd snap rollback pool/image@snapname\n</code></pre></p> <p>Delete a snapshot: <pre><code># rbd snap rm pool/image@snapname\n</code></pre></p> <p>Get snapshot disk usage: <pre><code># rbd disk-usage pool/image\nNAME           PROVISIONED  USED  \nimage1@mysnap        5 GiB  36 MiB\nimage1               5 GiB     0 B\n&lt;TOTAL&gt;              5 GiB  36 MiB\n</code></pre></p>"},{"location":"Ceph/CephRados/#cloning","title":"Cloning","text":"<p>Clones are read/write copies of an RBD image that use a protected RBD snapshot as a base.  Clones can be flatten, or converted to an independent image from the source.</p> <p>Create the clone, protect the snap from deletion, create the clone from the protected snapshot: <pre><code># rbd snap create pool/image@snapname\n# rbd snap protect pool/image@snapname\n# rbd clone pool/image@snapname pool/cloneimage\n</code></pre></p> <p>To set the COR option one the specific image or globally: <pre><code># ceph config set client rbd_clone_copy_on_read true\n# ceph config set global rbd_clone_copy_on_read true\n</code></pre></p> <p>Clone management commands: <pre><code># rbd children pool/image@snapname\n# rbd clone pool/image@snapname pool/cloneimage\n# rbd flatten pool/cloneimage\n</code></pre></p>"},{"location":"Ceph/CephRados/#importing-and-exporting-images","title":"Importing and Exporting Images","text":"<p>The <code>rbd export</code> command allows you to export an RBD image (or snapshot) to a file.  Use the <code>rbd import</code> command to import the file to another RBD image.  </p> <p>Export: <pre><code># rbd export pool/image /tmp/img-exp.dat\n</code></pre></p> <p>NOTE: Use the <code>--export-format 1|2</code> option to convert earlier format 1 images to the newer format 2.</p> <p>Differential Export: <pre><code># rbd export-diff pool/image /tmp/exp-diff.dat\n</code></pre></p> <p>Import: <pre><code># rbd import /tmp/img-exp.dat pool/image\n</code></pre></p> <p>NOTE: use the <code>export-format 1|2</code> option to specify the data format of the data to be imported.  Use the <code>--image-format 1|2</code> option  to specify the data format to import as along with the --stripe-count, --object-size, and --image-feature options.  </p> <p>Differential Import: <pre><code># rbd import-diff /tmp/img-exp.dat pool/image\n</code></pre></p>"},{"location":"Ceph/CephRados/#take-out-the-trash","title":"Take out the trash","text":"<p>Use the <code>rbd trash mv</code> command to move an image from a pool to the trash.  Then delete the image from the trash using the <code>rbd trash rm</code> command.</p>"},{"location":"Ceph/CephRados/#rbd-mirrors","title":"RBD Mirrors","text":"<p>RBD Mirroring supports both active/passive and active/active along with two modes, pool mode and image mode.  </p> <ul> <li>Pool mode - automatically enables mirroring for each RBD image created in a mirrored pool.  Ceph creates the image on the remote cluster.  </li> <li>Image mode - selectively enabled for individual RBD images.  </li> </ul> <p>Data is asynchronously mirrored using either journal-based or snapshot based mirroring.  </p> <ul> <li>Journal-based - to ensure point-in-time and crash consistent replication.  Data is written to the associated journal before the actual image.  The remote cluster reads the journal and replays the updates to its local copy of the image.  </li> <li>Snapshot-based - uses scheduled or manually created snapshots to replicate crash-consistent images.  The remote cluster determines any data or metadata updates between two mirror snapshots and copies the deltas to the image's local copy.  The RBD fast-diff image feature enables quick determination of updated data blocks without having to scan the entire image. <p>NOTE: The complete delta between two snapshots must be synced prior to use during a failover.  Any partial sync will be rolled back the moment of failover.</p> </li> </ul>"},{"location":"Ceph/CephRados/#configuration","title":"Configuration","text":"<p>To configure mirroring, ensure the pool is defined on both the local and remote cluster. <pre><code># rbd mirror pool peer bootstrap create --site-name primary poolname &gt; /tmp/primary.token\n</code></pre></p> <p>NOTE: Copy the /tmp/primary.token to the secondary cluster.  </p> <p>On secondary site: <pre><code># ceph orch apply rbd-mirror --placement=nodename\n# rbd mirror pool peer bootstrap import --site-name secondary --direction rx-only test_pool /tmp/bootstrap.token\n</code></pre></p> <p>Enable pool mirroring: <pre><code># rbd mirror pool enable pool [pool|image]\n</code></pre></p> <p>Enable image mirroring: <pre><code># rbd mirror image enable pool/image\n</code></pre></p> <p>Snapshot-based mirroring: <pre><code># rbd mirror image enable pool/image snapshot\n</code></pre></p> <p>NOTE: Journal mirror must not be enabled; use <code>rbd mirror image disable pool/image</code> if necessary.</p>"},{"location":"Ceph/CephRados/#failover-procedure","title":"Failover Procedure","text":"<p>On the primary site: <pre><code># rbd mirror image demote pool/image\n</code></pre> On the secondary site: <pre><code># rbd mirror image promote pool/image\n</code></pre></p> <p>NOTE: Use the --force option during the promotion if you cannot demote the primary site.  </p>"},{"location":"Ceph/CephTuning/","title":"Ceph Performance Tuning","text":"<ul> <li>IOPS - HDDs are in the range of 50-200; SSDs are thousands to hundreds of thousands; NVMe are some hundreds of thousands. </li> <li> <p>Throughput - HDDs around 150Mb/s; SSDs are ~500Mb/s; NVMe are ~2,000Mb/s.  </p> </li> <li> <p>Tune the BlueStore back end used by OSDs</p> </li> <li>Adjust the schedule for automatic data scrubbing and deep scrubbing</li> <li>Adjust the schedule of asynchronous snapshot trimming (snapshot cleanup)</li> <li>Control rate of backfill and recovery operations with OSD failures/additions</li> </ul>"},{"location":"Ceph/CephTuning/#blog-links","title":"Blog Links","text":"<p>CPU Scaling BlueStore (Default vs. Tuned) Performance Comparison Ceph Block Storage Performance on All-Flash Cluster with BlueStore backend RHCS Bluestore performance Scalability ( 3 vs 5 nodes ) RHCS 3.2 Bluestore Advanced Performance Investigation </p>"},{"location":"Ceph/CephTuning/#recovery-and-backfill","title":"Recovery and Backfill","text":"<ul> <li> <p>Recovery occurs when OSD becomes inaccessible and then back online.  OSD has to recover the latest copy of the data.  Default is to allow 3 simultaneous recovery operations for HDDs and 10 for SSDs.</p> </li> <li> <p>Backfill occurs with new OSDs and when and OSD dies and Ceph reassigns its PGs to other OSDs.  Default is to allow 1 PG backfill to/from an OSD at a time.</p> </li> </ul> Parameter Definition osd_recovery_op_priority range of 1-63; default is 3 osd_recovery_max_active concurrent recovery operations per OSD in parallel osd_recovery_threads Number of threads for data recovery osd_max_backfills concurrent backfill operations per OSD osd_backfill_scan_min Minimum number of objects for backfill scan osd_backfill_scan_max Maximum number of objects for backfill scan osd_backfill_full_ratio Threshold for backfill requests to an OSd osd_rbackfill_retry_interval Seconds to wait before retrying backfill requests"},{"location":"Ceph/CephTuning/#iops-optimized","title":"IOPS optimized","text":"<p>Workloads on block devices.  Typical deployments require high-performance SAS drives for storage and journals place on SSDs or NVMe.  </p> <ul> <li>Use two OSDs per NVMe device.  </li> <li>NVMe drives have data, the block database, and WAL collocated on the same storage device.  </li> <li>Assuming a 2 GHz CPU, use 10 cores per NVMe or 2 cores per SSD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 2 OSDs.  </li> </ul>"},{"location":"Ceph/CephTuning/#throughput-optimized","title":"Throughput optimized","text":"<p>Workloads on RGW.  </p> <p>NOTE: Workloads on RGW are often throughput-intensive and are backed on HDDs.  However, the bucket index pool is typically I/O-intensive so store it on SSD/NVMe.  One index/bucket that is stored in one RADOS object.  When a bucket stores more than 100K objects, the index performance degrades.  Use sharding for buckets by setting the <code>rgw_override_bucket_index_max_shards</code> parameter.  Recommended value is number of objects/100,000.  As the index grows, Ceph must reshard the bucket.  Enable automatic resharding with <code>rgw_dynamic_resharding</code>.  </p> <ul> <li>Use one OSD per HDD.  </li> <li>Place the block database and WAL on SSDs or NVMes.  </li> <li>Use at least 7,200 RPM HDD drives.  </li> <li>Assuming a 2 GHz CPU, use one-half core per HDD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 12 OSDs.</li> </ul>"},{"location":"Ceph/CephTuning/#capacity-optimized","title":"Capacity optimized","text":"<p>Workloads that require a large amount of data as inexpensively as possible; usually trading performance for price using SATA drives.</p> <ul> <li>Use one OSD per HDD.  </li> <li>HDDs have data, the block database, and WAL collocated on the same storage device.  </li> <li>Use at least 7,200 RPM HDD drives.  </li> <li>Assuming a 2 GHz CPU, use one-half core per HDD.  </li> <li>Allocate 16 GB RAM as a baseline, plus 5 GB per OSD.  </li> <li>Use 10 GbE NICs per 12 OSDs.  </li> </ul>"},{"location":"Ceph/CephTuning/#performance-stress-test","title":"Performance Stress Test","text":""},{"location":"Ceph/CephTuning/#rados-bench-command","title":"RADOS Bench Command","text":"<pre><code># rados -p &lt;mypool&gt; bench &lt;seconds&gt; --io-type [write|seq|rand] -b &lt;objsize&gt; -t concurrency [--no-cleanup]\n</code></pre> <p>Defaults:        --io-size  4096 bytes        --io-threads  16        --io-total  1 GB        --io-pattern   seq</p>"},{"location":"Ceph/CephTuning/#fragmentation","title":"Fragmentation","text":"<p>Check fragmentation on the OSDs:</p> <pre><code># ceph tell &lt;osd.ID&gt; bluestore allocator score block\n</code></pre> <p>Value 0 to .7 is considered acceptable. Value .7 to .9 is considered safe fragmentation. Value .9 and above indicates severe fragmentation that is causing performance issues.  </p>"},{"location":"Ceph/CephTuning/#scrubbing","title":"Scrubbing","text":"<p>You can control scrubbing globally or at the pool level.  To control at the pool level use the <code>ceph osd pool set poolName parameterValue</code> command.</p> Parameter Description noscrub If set to true, Ceph doesn't light scrub the pool; default is false nodeep-scrub If set to true, Ceph doesn't deep scrub the pool; default is false scrub_min_interval Wait a minimum number of seconds between scrubs; default is 0 (uses global parameter) scrub_max_interval Do not wait more than this number of seconds before performing a scrub; default is 0 deep_scrub_interval Interval for deep scrubbing; default is 0"},{"location":"Ceph/CephTuning/#light-scrubbing","title":"Light Scrubbing","text":"<p>Verifes an objects presence, checksum, and size.</p> Parameter Description osd_scrub_end_hour = end_hour Specifies the time to stop scrubbing; 0 - 23 osd_scrub_load_threshold Perform scrub is system load is below the threshold; default is .5 osd_scrub_min_interval Wait a minimum number of seconds between scrubs; default is 86,400 osd_scrub_internval_randomize_ratio Add a random dealy to the value defined in the osd_scrub_min_interval parameter; default is .5 osd_scrub_max_interval Do not wait more than this number of seconds before performing a scrub regardless of load; default 604,800 osd_scrub_priority Set the priority for scrub operations; default is 5; relative to osd_client_op_priority (default 64) <pre><code>ceph pg scrub &lt;pg-id&gt;\n</code></pre>"},{"location":"Ceph/CephTuning/#deep-scrubbing","title":"Deep Scrubbing","text":"<p>Reads the data and recalculates and verifies the objects checksum.</p> <p>You can enable/disable deep scrubbing at the cluster level by using the <code>ceph osd set nodeep-scrub</code> and <code>ceph osd unset nodeep-scrub</code> commands.</p> Parameter Description osd_deep_scrub_interval Interval for deep scrubbing; default is 604,800 osd_scrub_sleep Introduces a pause between deep scrub disk reads.  Increase the value to slow down scrub operations; default is 0 <pre><code>ceph pg deep-scrub &lt;pg-id&gt;\n</code></pre>"},{"location":"Ceph/CephTuning/#snapshot-trimming","title":"Snapshot Trimming","text":"<p>Ceph schedules the removal of the snapshot data as an asynchronous operation when a snapshot is removed. To reduce the impact, configure a pause after the deletion with the osd_snap_trim_sleep parameter.  This adds a delay before allowing the next snapshot trim operation. Default is 0.</p> <p>Control the snapshot trimming process using the osd_snap_trim_priority parameter.  Default 5</p>"},{"location":"Ceph/CephTuning/#logging","title":"Logging","text":"<p>Ceph logging levels are on a scale of 1 (terse) to 20 (verbose).  A single value can be used for both log level and memory level or separate them with a slash for different values (i.e. 1/5 where 1 is for log level and 5 is for memory log level).</p> <p>Get the logging config for daemons: <pre><code>ceph --admin-daemon ./ceph-osd.11.asok config show | grep debug\n</code></pre></p> <p>NOTE: The asok files are found in /var/run/ceph/fsid.</p>"},{"location":"Ceph/Cephv6installation/","title":"Ceph v6.x Installation","text":""},{"location":"Ceph/Cephv6installation/#introduction","title":"Introduction","text":""},{"location":"Ceph/Cephv6installation/#goals","title":"Goals:","text":"<ul> <li>Enable field on Ceph v6.0</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to Ceph Product management and engineering teams at IBM</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with Ceph</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Ceph/Cephv6installation/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Ceph 32G 8 <ul><li>1x pxe</li><li>1x ceph-frontend</li><li>1x ceph-backend</li> <ul><li>64GB</li><li>100GB</li>"},{"location":"Ceph/Cephv6installation/#building-your-kni-lab","title":"Building Your KNI Lab","text":""},{"location":"Ceph/Cephv6installation/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy Ceph Storage Cluster.  </p> </li> <li> <p>Update the project_name and project_password parameters. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p></p> </li> <li> <p>Wait the deployment to finish which can take up to ~10-15 minutes.  </p> </li> </ol>"},{"location":"Ceph/Cephv6installation/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  Update the project_name and project_password along with the networks to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output. </p> <pre><code>external_network: vlan1117\nnetworks:  \n  - { name: \"ceph-frontend\", cidr: \"10.20.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n\u00a0\u00a0- { name: \"ceph-backend\", cidr: \"10.20.1.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 ceph nodes.  Update the project_name and project_password along with the instances to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output.  </p> <pre><code>instances:  \n  - { name: \"ceph1\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n\u00a0\u00a0- { name: \"ceph2\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n  - { name: \"ceph3\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n</code></pre> </li> </ol>"},{"location":"Ceph/Cephv6installation/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Ceph/Cephv6installation/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0 </p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  Take note of the IP addresses for the VLAN1117 network.  You will use the 172.20.17.X addresses to access the servers.  </p> <p> </p> </li> <li> <p>Start each instance; In the Actions colume, select Start Instance for each node in the cluster.  </p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0  </p> </li> </ol> <p>To access your instance, ssh as the cloud-user using the VLAN IP addresses.  Make sure you are connected to the NA-SSA VPN.</p>"},{"location":"Ceph/Cephv6installation/#ceph-v6-installation","title":"Ceph v6. Installation","text":"<p>The full Red Hat documentation for the Ceph installation is available here.  The below precedures are for the OpenInfra Lab environment and have been scaled down to only include the required steps.  </p>"},{"location":"Ceph/Cephv6installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Red Hat Enterprise Linux 9.2 EUS or later.  </li> <li>Ansible 2.9 or later.  </li> <li>Valid Red Hat subsription with the appropriate entitlements.  </li> <li>Root-level access to all nodes.  </li> <li>An active Red Hat Network or service account to access the Red Hat Registry.  </li> </ul> <p>NOTE: Ensure that you are connected to the NA-SSA VPN</p> <ol> <li> <p>Login to ceph01.  Update the /etc/hosts files with the IP and names.</p> <p>NOTE: Your IP address will be different.  </p> <pre><code>ssh cloud-user@ceph01\n$ vi /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n172.20.17.40     ceph01 \n172.20.17.120    ceph02 \n172.20.17.191    ceph03 \n\n10.40.0.190      ceph01-stg\n10.40.0.125      ceph02-stg\n10.40.0.245      ceph03-stg\n</code></pre> </li> <li> <p>Grab the rhel9 repository file from the DNS Utility server</p> <pre><code>$ sudo curl http://172.20.129.10/hextupleo-repo/rhel9.repo -o /etc/yum.repos.d/rhel8.repo\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1379  100  1379    0     0   269k      0 --:--:-- --:--:-- --:--:--  448k\n[cloud-user@ceph1 ~]$ cat /etc/yum.repos.d/rhel9.repo\n[rhel-9-for-x86_64-highavailability-rpms]\nname=rhel-9-for-x86_64-highavailability-rpms\nbaseurl=http://172.20.129.13/repos/rhel-9-for-x86_64-highavailability-rpms/\nenabled=1\ngpgcheck=0\n...\n[rhceph-6-tools-for-rhel-9-x86_64-rpms]\nname=rhceph-6-tools-for-rhel-9-x86_64-rpms\nbaseurl=http://172.20.129.13/repos/rhceph-6-tools-for-rhel-9-x86_64-rpms/\nenabled=1\ngpgcheck=0\n</code></pre> </li> <li> <p>Update all packages using dnf on all servers.</p> <pre><code>$ cat /etc/redhat-release \nRed Hat Enterprise Linux release 9.1 (Plow)\n$ sudo dnf update -y\n...\nInstalled:\nansible-collection-ansible-posix-1.2.0-1.3.el9ost.noarch                  ansible-collection-community-general-4.0.0-1.1.el9ost.noarch                 \nansible-core-2.14.2-5.el9_2.x86_64                                        cephadm-ansible-2.15.0-1.el9cp.noarch                                        \ngit-core-2.39.3-1.el9_2.x86_64                                            libnsl2-2.0.0-1.el9.x86_64                                                   \nmpdecimal-2.5.1-3.el9.x86_64                                              python3.11-3.11.2-2.el9_2.1.x86_64                                           \npython3.11-cffi-1.15.1-1.el9.x86_64                                       python3.11-cryptography-37.0.2-5.el9.x86_64                                  \npython3.11-libs-3.11.2-2.el9_2.1.x86_64                                   python3.11-pip-wheel-22.3.1-2.el9.noarch                                     \npython3.11-ply-3.11-1.el9.noarch                                          python3.11-pycparser-2.20-1.el9.noarch                                       \npython3.11-pyyaml-6.0-1.el9.x86_64                                        python3.11-setuptools-wheel-65.5.1-2.el9.noarch                              \npython3.11-six-1.16.0-1.el9.noarch                                        sshpass-1.09-4.el9.x86_64                                                    \n\nComplete!    ...\n$ sudo reboot\nConnection to 172.20.17.117 closed by remote host.\nConnection to 172.20.17.117 closed.\n</code></pre> <p>NOTE: Don't forget to do all servers in the cluster.  </p> </li> <li> <p>Generate the ssh key files for the root user on ceph01.  Update the authorized_keys file on all nodes and append the contents of the id_rsa.pub file.  </p> </li> <li> <p>Install the cephadm-ansible package on ceph01 (or the first node in the cluster).  </p> <pre><code>$ sudo dnf install -y cephadm-ansible\n...\nInstalled:\n  ansible-2.9.27-1.el8ae.noarch                    cephadm-ansible-1.8.0-1.el8cp.noarch                    python3-jmespath-0.9.0-11.el8.noarch                    sshpass-1.09-4.el8.x86_64                   \n\nComplete!\n</code></pre> </li> <li> <p>Create the inventory hosts and registry-login.json files on ceph01.  Change the permissions on the registry-login.json file.</p> <pre><code>$ cd /usr/share/cephadm-ansible \n$ vi hosts\nceph1\nceph2\nceph3\n\n[admin]\nceph1\n$ sudo mkdir /root/ceph\n$ sudo vi /root/ceph/registry.json\n{\n \"url\":\"registry.redhat.io\",\n \"username\":\"myuser1\",\n \"password\":\"mypassword1\"\n}\n$ sudo chmod 600 registry.json     \n</code></pre> <p>NOTE: The user name is the user name that you use to login to registry.redhat.io.  This is used to download the ceph containers.</p> </li> </ol>"},{"location":"Ceph/Cephv6installation/#installation","title":"Installation","text":"<ol> <li> <p>Run the Ceph ansible preflight playbook.  </p> <pre><code># sudo -i\n# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars \"ceph_origin=custom\" -e \"custom_repo_url=http://172.20.129.13/repos/rhceph-6-tools-for-rhel-9-x86_64-rpms/\"\n</code></pre> <p>NOTE: Use the custom_repo_url when for a disconnected installation.</p> </li> <li> <p>Create the bootstrap configuration file on ceph01 (or first node in the cluster).</p> <pre><code>service_type: host\naddr: ceph01\nhostname: ceph01\n---\nservice_type: host\naddr: ceph02\nhostname: ceph02\n---\nservice_type: host\naddr: ceph03\nhostname: ceph03\n---\nservice_type: host\naddr: ceph04\nhostname: ceph04\n---\nservice_type: mon\nplacement:\n  host_pattern: \"ceph0[1-3]\"\n---\nservice_type: osd\nservice_id: initial_osds\nplacement:\n  host_pattern: \"ceph0[1-3]\"\ndata_devices:\n  paths:\n   - /dev/vdb\n</code></pre> </li> <li> <p>Run the cephadm bootstrap command.  </p> <pre><code># cephadm bootstrap --mon-ip 10.40.0.193 --apply-spec /root/ceph/initial-config.yaml --initial-dashboard-password changeme  --registry-json /root/ceph/registry-login.json --cluster-network 10.20.1.0/24\n</code></pre> </li> <li> <p>Once the bootstrap is complete, check the status of the cluster with the <code>ceph status</code> command.  </p> </li> <li> <p>If firewalld is enabled, ensure the following ports are opened on all nodes that run the <code>MON</code> and/or <code>OSD</code> service:  </p> <p>MON:</p> <pre><code># firewall-cmd --zone-public --add-port=6789/tcp\n# firewall-cmd --zone-public --add-port=6789/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-public --add-service=ceph-mon\n# firewall-cmd --zone-public --add-service=ceph-mon --permanent\n</code></pre> <p>OSD:</p> <pre><code># firewall-cmd --zone-public --add-port=6800-7300/tcp\n# firewall-cmd --zone-public --add-port=6800-7300/tcp --permanent\n</code></pre> <p>Or use the service name:  </p> <pre><code># firewall-cmd --zone-[public|cluster] --add-service=ceph\n# firewall-cmd --zone-[public|cluster] --add-service=ceph --permanent\n</code></pre> </li> <li> <p>Ensure the MTU size is set to 9000 on the network interfaces.</p> <pre><code># nmcli conn modify 'eth0' 802-3-ethernet.mtu 9000\n# nmcli conn down 'eth0'\n# nmcli conn up 'eth0'\n# ip link show 'eth0\n</code></pre> </li> <li> <p>Set the labels for the servers.</p> </li> </ol> <pre><code># ceph orch host ls\nHOST           ADDR         LABELS                  STATUS  \ncephstorage01  172.20.0.11  _admin mon mgr grafana          \ncephstorage02  172.20.0.12  mon mgr rgw _admin              \ncephstorage03  172.20.0.13  mon mgr rgw                     \ncephstorage04  172.20.0.14  mon mgr rgw                     \ncephstorage05  172.20.0.15                                  \ncephstorage06  172.20.0.16                                  \n6 hosts in cluster\n\n# ceph orch host label add cephstorage01 mgr\n</code></pre> <p>Note: Labels can be mon, mgr, rgw, admin, or whatever you choose.</p>"},{"location":"Ceph/Cephv6installation/#appendix","title":"Appendix","text":""},{"location":"Ceph/Cephv6installation/#export-service-specification","title":"Export Service Specification","text":"<pre><code>ceph orch ls --service_type type --service_name name --export\n</code></pre>"},{"location":"Ceph/Cephv6installation/#create-floating-ip-address","title":"Create floating IP address","text":"<pre><code>[stack@bgp-undercloud ~] openstack floating ip create --subnet 372459e8-25f9-4885-b71a-6889ffff02bf --project ceph-blm 18743df0-57aa-4571-9d62-439e0570b059\n</code></pre>"},{"location":"How%20To/Certificates/","title":"Certificates","text":""},{"location":"How%20To/Certificates/#ca-key-and-certificate-files","title":"CA Key and Certificate Files","text":"<p>The CA .key and .pem files are in the /etc/httpd/conf/ssl.key directory on the DNS Utility server in the same directory.</p> <pre><code># openssl genrsa -des3 -out openinfraCA.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.......................................................................................................................................+++++\n...........................................................................................+++++\ne is 65537 (0x010001)\nEnter pass phrase for openinfraCA.key: *******\nVerifying - Enter pass phrase for openinfraCA.key: *******\n\n\n# openssl req -x509 -new -nodes -key openinfraCA.key -sha256 -days 1095 -out openinfraCA.pem\nEnter pass phrase for openinfraCA.key:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:openinfra.lab\nEmail Address []:userName@redhat.com\n</code></pre> <p>Copy the new CA .pem file to the anchors directory; run update-ca-trust to add it to the list of trusted CA certificates:</p> <pre><code># cp openinfraCA.pem /usr/share/pki/ca-trust-source/anchors/\n# update-ca-trust\n</code></pre> <p>Use the trust list command to verify the new CA is included in the list of trusted CAs:</p> <pre><code># trust list | grep -B2 -A2 openinfra.lab \npkcs11:id=%91%54%10%D3%0D%E4%AD%A7%08%E7%18%EF%A8%62%F7%BF%59%D6%4D%6E;type=cert\n    type: certificate\n    label: openinfra.lab\n    trust: anchor\n    category: authority\n</code></pre>"},{"location":"How%20To/Certificates/#creating-a-certificate-for-a-serverservice","title":"Creating a Certificate for a Server/Service","text":"<p>There\u2019s two ways to do this, manual and scripted.  Both are provided here.</p>"},{"location":"How%20To/Certificates/#scripted-process","title":"Scripted Process","text":"<p>Login to the Lab DNS server (172.20.129.10). Switch to the root user.   Add an entry in /etc/hosts for the Server/Service you are generating the SSL certificate for. Change directory to /root/ssl-certifcates and run the script.  </p> <pre><code>cd /root/ssl-certificates  \n./create-certificate.sh &lt;FQDN&gt;  \n</code></pre> <p>Example: ./create-certificate jira-sm.openinfra.lab  </p> <p>You will be prompted for the pass phrase to the openinfraCA.key. </p> <p>NOTE: The pass phrase is in the Cloud Infra Lab spreadsheet.</p> <p>All output files will start with the FQDN. Example:  </p> <p>-rw-r--r--. 1 root root 1517 Mar 22 17:16 jira-sm.openinfra.lab.crt -rw-r--r--. 1 root root 1054 Mar 22 17:16 jira-sm.openinfra.lab.csr -rw-r--r--. 1 root root  254 Mar 22 17:16 jira-sm.openinfra.lab.ext -rw-------. 1 root root 1675 Mar 22 17:16 jira-sm.openinfra.lab.key  </p>"},{"location":"How%20To/Certificates/#manual-process","title":"Manual Process","text":"<p>Generate the key file:</p> <pre><code># openssl genrsa -out cephrgw.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.........+++++\n..............................................................+++++\ne is 65537 (0x010001)\n</code></pre> <p>Generate a Certificate Signing Request file:</p> <pre><code># openssl req -new -key cephrgw.key -out cephrgw.csr\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:cephrgw \nEmail Address []:youremail@redhat.com\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:Redhat1!\nAn optional company name []:      \n</code></pre> <p>Create an x509 V3 extension config file to define the Subject Alternate Names (SAN)</p> <pre><code># cat cephrgw.ext\nauthorityKeyIdentifier = keyid,issuer\nbasicConstraints = CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = cephrgw.cephlab.openinfra.lab\nDNS.2 = cephrgw\nDNS.3 = *.cephrgw\nDNS.4 = *.cephrgw.cephlab.openinfra.lab\n</code></pre> <p>Create the signed certificate using the .csr and .ext file along with the openinfraCA.pem and .key files:</p> <pre><code># openssl x509 -req -in cephrgw.csr -CA openinfraCA.pem -CAkey openinfraCA.key -CAcreateserial -out cephrgw.crt -days 365 -sha256 -extfile cephrgw.ext\nSignature ok\nsubject=C = US, ST = North Carolina, L = Raleigh, O = Red Hat, OU = OpenInfrastructure Lab, CN = cephrgw, emailAddress = bmclaren@redhat.com\nGetting CA Private Key\nEnter pass phrase for openinfraCA.key: Redhat1!\n</code></pre> <p>NOTE: The option \u2013CAcreateserial generates the openinfraCA.srl file.  In subsequent calls to create a signed certificate, use the -CAserial parameter with this file (-CAserial openinfraCA.srl).  The file is used to keep track of the unique serial numbers.</p>"},{"location":"How%20To/DocumentationContribution/","title":"Documentation Contribution","text":"<p>The documentation is hosted in GitHub at https://github.com/redhat-openinfra-lab/openinfra-docs.</p>"},{"location":"How%20To/DocumentationContribution/#pre-requisites","title":"Pre-Requisites","text":"<p>Local client must have python and git.  After installing python, you will need to use pip to install the mkdocs and mkdocs-material packages.  Due to the use of pip we recommend using a Virtual Environment to work on the documentation.  This page will explain how to create/configure this.</p> <p>While it is not required, VS Code makes updating the documents easy and integrates with GitHub nicely.</p>"},{"location":"How%20To/DocumentationContribution/#update-the-site-or-add-documentation","title":"Update the Site or Add Documentation","text":"<p>As the site requires pip to install mkdocs and mkdocs-material, we recommend creating a Virtual Environment to work on the page documentation.  This page will explain how to do this.</p>"},{"location":"How%20To/DocumentationContribution/#configure-virtual-environment","title":"Configure virtual environment","text":"<ol> <li> <p>Install python3 and pip</p> </li> <li> <p>Create the virtual environment</p> </li> </ol> <pre><code>export VENV=${HOME}/virtual_environments/mkdocs\npython3 -m venv ${VENV}\n</code></pre> <ol> <li> <p>Activate the Virtual Environment (You will want to do this step everytime you want to work on the doumentation)</p> </li> <li> <p>Use pip to install mkdocs and mkdocs-material</p> </li> </ol> <pre><code>python3 -m pip install --upgrade mkdocs mkdocs-material\n</code></pre>"},{"location":"How%20To/DocumentationContribution/#clone-github-repository","title":"Clone Github Repository","text":"<p>Clone the site to your local repository</p> <pre><code>git clone git@github.com:redhat-openinfra-lab/openinfra-docs.git\n</code></pre>"},{"location":"How%20To/DocumentationContribution/#working-locally","title":"Working locally","text":"<p>Once you have a copy of the repository, mkdocs can serve the documentation so you can view your changes real-time.  Remember, you will need to activate the virtual environment using Step 3 above before trying to start the mkdocs server. If you are working on the documentation on your desktop, you can simply start the server using:</p> <pre><code>cd openinfra-docs\nmkdocs serve\n</code></pre> <p>This will start a server listening on port 8000 on your loopback device.  If you are using something other than your desktop, such as a Virtual Machine, you will need to open a port on your firewall and utilize the <code>-a</code> switch when starting the mkdocs serve command to specify an IP:PORT to start on.  Example:</p> <pre><code>cd openinfra-docs\nmkdocs serve -a 172.20.135.10:8000\n</code></pre> <p>The site is configured to automatically publish changes to the gh-pages sites when a <code>push</code> to the master branch is executed.  Check the status of the pages build and deployment under the Actions menu.</p>"},{"location":"How%20To/jirachangerequest/","title":"Lab Change Requests","text":"<p>Use these procedures to requests a change in the NA-SSA Lab using Jira Service Management.  </p> <ol> <li> <p>Log in to your Jira Service Management account.  </p> </li> <li> <p>Click on the \u201cCreate\u201d button located in the top of the screen.  </p> <p></p> </li> <li> <p>Select the \u201cChange Request\u201d option from the list of issue types.  </p> </li> <li> <p>Fill in the required fields in the Create Issue form such as Project, Summary, Description, Priority, and other custom fields that may be relevant to your organization's processes.  </p> <p></p> </li> <li> <p>Assign the issue to the appropriate team member or group responsible for the change.  </p> </li> <li> <p>Add any necessary comments or attachments to provide more information or context for the change request.  </p> </li> <li> <p>Save the issue and wait for approval from the appropriate parties.  </p> <ul> <li>If the change request is approved, the approving party will set the status to \"Awaiting Implementation\" to indicate that it is ready for implementation.  The approving party will assign the issue to the team member or group responsible for implementing the change.  </li> <li>If the change request is declined, the approving party will set the status to \"Declined\" and add comments explaining why the request was declined.  </li> <li>If the change request is no longer needed, the approving party will set the status to \"Cancelled\".  </li> <li>Once the change request has been implemented, the implementation team will set the status to \"Resolved\" to indicate that the change has been completed.  </li> </ul> </li> <li> <p>Verify that the change has been successful and meets the requirements of the change request.  </p> </li> <li> <p>Finally, set the status to \"Closed\" to indicate that the change request is completed and closed.</p> </li> </ol> <p>That's it! These steps should help you create a change request using Jira Service Management and manage its lifecycle through various status changes.  Remember to communicate with your team members and stakeholders throughout the process to ensure everyone is aware of the status of the change request.</p>"},{"location":"Openshift/Containers/","title":"OCP","text":""},{"location":"Openshift/Containers/#download-link","title":"Download Link","text":"<p>RHOCP Installers Product Documentation</p>"},{"location":"Openshift/Containers/#control-plane-components","title":"Control Plane Components","text":"Component Description etcd Distributed key-value database that stores cluster config details kube-apiserver Front-end server that exposes the Kubernates API kube-scheduled Watcher service that determins an available compute node for new pod requests"},{"location":"Openshift/Containers/#compute-plane-components","title":"Compute Plane Components","text":"Component Description kubelet Main agent on each cluster compute node, responsible for executing pod requests that come from API and scheduler kube-proxy Provides network configuration adn communication for pods on a node. cri-o CRI-O Engine, represents a small OCI-compliant runtime engine. &gt; NOTE: CRI (Container Runtime Interface) is a plug-in interface that provides configurable communication betwween kubelet and pod config requests"},{"location":"Openshift/Containers/#ocp-cli","title":"OCP CLI","text":"<p>Login <pre><code># oc login -u user -p password https://api.ocp4.example.com:6443\n</code></pre></p> <p>Retrieve the Web-Console Link: <pre><code># oc whoami --show-console\n</code></pre></p> <p>Get Kubernates control plane URL: <pre><code># oc cluster-info\n</code></pre></p> <p>Get nodes in the cluster: <pre><code># oc get nodes\n</code></pre></p> <p>List of installed operators: <pre><code># oc get clusteroperator\n</code></pre></p> <p>Get resources: <pre><code># oc get [all|*resource_type*] [ -n namespace ]\n</code></pre></p> <p>Describe a resource: <pre><code># oc describe *resouce_type* *resource_name*\n</code></pre></p> <p>Get API-Resources: <pre><code># oc api-resources [ --api-group='' ]\n</code></pre></p> <p>Deployments: <pre><code># oc get deploy\n# oc get deploy *name* -o wide\n# oc describe deployment *name*\n</code></pre></p> <p>Describe a POD: <pre><code># oc get pods\nNAME                    READY   STATUS      REASTARTS   AGE\nmyapp-77fb5cd997-xr889  1/1     Running     0           13m\n# oc describe pod myapp-77fb5cd997-xr889 \nName:               myapp-77fb5cd997-xr889 \nNamespace:          cli-resources\nPriority:           0\nService Account:    default\n...\n</code></pre></p> <p>NOTE: Use the oc get pod with the -o yaml to format the output in YAML format; this yields more details about a resource than the <code>describe</code> option.</p> <p>Explain the fields of an API resource: <pre><code># oc explain .pods.spec.containers.resources [--recursive]\n</code></pre></p> <p>Delete a resource: <pre><code># oc delete pod quotes-ui\n</code></pre></p> <p>NOTE: When deleting managed resources, such as pods, results in the automatic creation of new instances of those resources.  When a project is deleted, it deletes all the resources and applications within it.</p>"},{"location":"Openshift/Containers/#containers","title":"Containers","text":"Options Description -t --tty meaning pseudo-tty -i --interactive; standard input is kep open into the container -d --detach; run the container in the background -e environment variables <p>List images that have been downloaded locally: <pre><code># podman images\n</code></pre></p> <p>Run the container in the background: <pre><code># podman run -d -p 8080 registry.redhat.io/rhel8/httpd-24\n</code></pre></p>"},{"location":"Openshift/Containers/#appendix","title":"Appendix","text":"Acronym Description S2I Source-to-image feature uses a BuildConfig to build a container image from application source code that is stored in a Git Repo."},{"location":"Openshift/Containers/#json-formatting","title":"JSON Formatting","text":"<p>Similar to JSON styled queries, use the -o custom-columns option: <pre><code>$ oc get pods \\\n-o custom-columns=PodName:\".metadata.name\",\\\nContainerName:\"spec.containers[].name\",\\\nPhase:\"status.phase\",\\\nIP:\"status.podIP\",\\\nPorts:\"spec.containers[].ports[].containerPort\"\nPodName                  ContainerName   Phase     IP          Ports\nmyapp-77fb5cd997-xplhz   myapp           Running   10.8.0.60   &lt;none&gt;\n</code></pre></p> <p>JSONPath expression: <pre><code>$ oc get pods  \\\n-o jsonpath='{range .items[]}{\"Pod Name: \"}{.metadata.name}\n{\"Container Names:\"}{.spec.containers[].name}\n{\"Phase: \"}{.status.phase}\n{\"IP: \"}{.status.podIP}\n{\"Ports: \"}{.spec.containers[].ports[].containerPort}\n{\"Pod Start Time: \"}{.status.startTime}{\"\\n\"}{end}'\nPod Name: myapp-77fb5cd997-xplhz\nContainer Names:myapp\nPhase: Running\nIP: 10.8.0.60\nPorts:\nPod Start Time: 2023-03-15T18:45:40Z\n</code></pre></p>"},{"location":"Openshift/OCPBaremetalPI/","title":"OCP Baremetal IPI Installation","text":""},{"location":"Openshift/OCPBaremetalPI/#introduction","title":"Introduction","text":"<p>Goals:</p> <ul> <li>Enable field on OpenShift Baremetal 4.12    </li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features    </li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs\u00a0    </li> <li>Send valuable feedback to Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OpenShift Baremetal</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Openshift/OCPBaremetalPI/#lab-access","title":"Lab Access","text":""},{"location":"Openshift/OCPBaremetalPI/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.</p> <p>We have limited resources available, but there should be enough room for about 10 virtual environments.  </p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snap-shotting, adding more cinder volumes to OCS nodes or even adding more networks via either OpenStack CLI, Horizon, or Ansible Tower.</p> Role vRAM vCPU vNIC Disk Bootstrap 20G 6 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 100GB Master 16GB 4 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 25 GB Worker 24GB 12 <ul><li>1x pxe</li><li>1x baremetal</li> 50GB 100GB OSD Custom (optional)"},{"location":"Openshift/OCPBaremetalPI/#building-your-kni-lab","title":"Building Your KNI Lab:","text":""},{"location":"Openshift/OCPBaremetalPI/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.</p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy OpenShift Baremetal Environment</p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.</p> <p></p> </li> <li> <p>The jobs can be monitored under Jobs in the left pane.\u00a0 Additional jobs will be initiated to create the project, network, instances, and bare metal bootstrap.\u00a0 Wait the deployment to finish which can take ~10-15 minutes.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> <p>NOTE:  Ensure the quota_vcpus value is set to at least 100 and the quota_ram value is set to at least 240000.  These the requirements needed to deploy an OCP cluster with ODF Essentials using the kni.worker.xlarge flavor.</p> <pre><code>quota_vcpus: 100\nquota_ram: 240000\nquota_instances: 30\nquota_ports: 200\n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>\"ext_network\": \"vlan1117\",\nnetworks:\n- { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"baremetal0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }\n- { name: \"virtualipmi\", cidr: \"10.30.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }\n</code></pre> </li> <li> <p>Set user/project and password using the same project and password used previously when creating the project.  Submit the job.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 controllers 2 computes and 3 ceph nodes.  </p> <p>NOTE: Don't forget to include the az parameter!</p> <pre><code>\"az\": \"leaf1\"\n</code></pre> <p>With OCS (xlarge workers):  </p> <pre><code>instances:\n  - { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n  - { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",  net_name7: \"\", net_name8: \"\"  }\n</code></pre> <p>Without ODF (normal workers): <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n</code></pre></p> </li> <li> <p>Set user/project and password and submit the job using the same project and password used previously when creating the project.  Submit the job. </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to - HextupleO - configure OCP BM Bootstrap</p> </li> <li> <p>Follow the survey and submit the job</p> </li> <li> <p>At the end you will be getting a screen similar to this one:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user kni using the password you set in the playbook.</p>"},{"location":"Openshift/OCPBaremetalPI/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"Openshift/OCPBaremetalPI/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0</p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.</p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address. You will use that IP to access your undercloud node. This will match the above output from the Ansible job (even though it does not in this document).\u00a0 You can SSH to this IP as kni using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0   </p> </li> <li> <p>Go to Routers, select the existing router (projectName_router); click the Interfaces tab and then click the Add Interface icon on the right. \u00a0 Add the baremetal0 interface in the Subnet dropdown.\u00a0 Click Submit.  </p> <p></p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#bootstrap","title":"Bootstrap","text":"<ol> <li> <p>Access the bootstrap server via ssh as the <code>kni</code> user using the IP address obtained in step 3 of the Accessing Your Project\u2019s OpenStack Environment section above and the password specified in Tower when deploying the KNI environment.</p> </li> <li> <p>You can now start deploying Openshift Baremetal (KNI) based on the standard instructions below or feel free to deploy using any other documented process.  </p> <p>INFO: Repos Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the rhel8.repo file from here:</p> <p>[kni@bootstrap ~]$ sudo curl http://172.20.129.10/hextupleo-repo/rhel8.repo -o /etc/yum.repos.d/rhel8.repo </p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-vanilla-openshift-baremetal","title":"Deploying Vanilla Openshift Baremetal","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, we highly encourage everyone to get at least one vanilla deployment done and get familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation found here.\u00a0 Please also review the official documentation for accuracy and open any Bugzilla\u2019s against it.</p>"},{"location":"Openshift/OCPBaremetalPI/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<ol> <li> <p>Log into Bootstrap VM.  </p> <pre><code>ssh kni@&lt;bootstrapInstance&gt;  / password specified in Tower\n</code></pre> </li> <li> <p>Update all packages on the system.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf update -y\n...\nComplete!\n[kni@bootstrap ~]$ sudo reboot\n</code></pre> </li> <li> <p>Install the KNI Packages.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf install -y libvirt qemu-kvm mkisofs python3-devel jq ipmitool\n</code></pre> </li> <li> <p>Modify the user to add the libvirt group to the newly created kni user.  </p> <pre><code>[kni@bootstrap ~]$ sudo usermod --append --groups libvirt kni\n</code></pre> <p>NOTE: If you receive the error <code>DB version too old [0.21], expected [0.23] for domain implicit_files!</code> stop sssd with the <code>systemctl stop sssd</code> command, remove the cache files in /var/lib/sss/db directory, and restart sssd with the <code>systemctl start sssd</code> command</p> </li> <li> <p>Start and enable libvirtd; verify the daemon started successfully.</p> <pre><code>[kni@bootstrap ~]$ sudo systemctl enable libvirtd --now\n[kni@bootstrap ~]$ systemctl status libvirtd\n\u25cf libvirtd.service - Virtualization daemon\n   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; vendor preset: enabled)\n   Active: active (running) since Thu 2023-02-16 10:07:21 EST; 1s ago\n     Docs: man:libvirtd(8)\n           https://libvirt.org\n Main PID: 7506 (libvirtd)\n    Tasks: 21 (limit: 32768)\n   Memory: 18.1M\n   CGroup: /system.slice/libvirtd.service\n       \u251c\u25007407 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u251c\u25007408 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u2514\u25007506 /usr/sbin/libvirtd --timeout 120\n</code></pre> </li> <li> <p>Create the default storage pool and start it.</p> <pre><code>[kni@bootstrap ~]$ sudo virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images\nPool default defined\n\n[kni@bootstrap ~]$ sudo virsh pool-start default \nPool default started\n\n[kni@bootstrap ~]$ sudo virsh pool-autostart default\nPool default marked as autostarted\n</code></pre> </li> <li> <p>Set up networking using the reconfig-net.sh script in the ~/GoodieBag directory.  Once the connections are reconfigured, the script will display the final configuration.  The output should look similar to what is displayed below.  The script will also add the baremetal subnet to the external router if it was not completed in the Accessing Your Project\u2019s OpenStack Environment section in the Horizon GUI.  </p> <pre><code>[kni@bootstrap ~]$ cd GoodieBag\n[kni@bootstrap GoodieBag]$ ./reconfig-net.sh -h\nScript to reconfigure the eth1 and eth2 interfaces on bootstrap server in OpenStack for Openshift deployments.\nUsage: reconfig-net.sh [-a|--all|interfaceName]\nIf -a or --all is passed, eth1 and eth2 will both be reconfigured as provisioning and baremetal bridge interfaces.\nIf -r or --router is passed, interface reconfiguration is skipped and router configuration will complete.\n\n\n[kni@bootstrap ~]$ sudo ./reconfig-net.sh -a\nConnection 'System eth1' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)\nConnection 'System eth2' (9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04) successfully deleted.\nConnection 'provisioning' (1b5a7497-4ff5-43ec-bb3e-df01e8f070e8) successfully added.\nConnection 'bridge-slave-eth1' (3a46f2d4-5a3e-4b8d-994b-0a055ade7a84) successfully added.\nConnection 'provisioning' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5) \nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/7) \nprovisioning configured successfully.\n\nConnection 'System eth2' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)\nConnection 'System eth2' (3a73717e-65ab-93e8-b518-24f5af32dc0d) successfully deleted.\nConnection 'baremetal' (9ba0964b-3667-42f5-ae81-a0a0936985cf) successfully added.\nConnection 'bridge-slave-eth2' (f2b52c9b-087f-4b7c-9186-0ed593897c73) successfully added.\nConnection 'baremetal' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/9)\nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/11)\nbaremetal configured successfully.\n\nCurrent network interface configuration:\nNAME               UUID                                  TYPE      DEVICE       \nSystem eth0        5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0         \nbaremetal          9ba0964b-3667-42f5-ae81-a0a0936985cf  bridge    baremetal    \nprovisioning       1b5a7497-4ff5-43ec-bb3e-df01e8f070e8  bridge    provisioning \nvirbr0             00fb2639-d510-4c0b-bdad-2a471e7c67c5  bridge    virbr0            \nbridge-slave-eth1  3a46f2d4-5a3e-4b8d-994b-0a055ade7a84  ethernet  eth1         \nbridge-slave-eth2  f2b52c9b-087f-4b7c-9186-0ed593897c73  ethernet  eth2         \n\nInstalling required packages to add the baremetal subnet to the external router.\nVerifying baremetal subnet has been added to external router.\nExternal router configuration correctly.\n[kni@bootstrap GoodieBag]$ \n</code></pre> </li> <li> <p>Create a pull-secret.txt file.  In a web browser, navigate to  Install OpenShift on Bare Metal with user-provisioned infrastructure, in the Pull Secret section, click the Copy pull secret link.  </p> <p></p> </li> <li> <p>Create a pull-secret.txt file in the kni user\u2019s home directory by pasting the data just copied.  </p> <pre><code>[kni@bootstrap ~]$ vi pull-secret.txt\n{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3lc3NobGVhc2UtZGaWZ0LXJl9hY2NfNViOTE3NDA1NWIyMWU4ZWQxN2ExYjhmOTU6SDA3UETQxN2Q2NzKSK92JMD3Kkk20kl\n\u2026\nNLXRmeUxFcFVRZnVuRVGl2czNjckTJWdkNpVQkdaVypeHVX1jTVhBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}\n:wq\n[kni@bootstrap ~]$\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#openshift-installation","title":"OpenShift Installation","text":"<p>The installation is based on the latest-4.12 version.  This will need to be updated as new versions are released.</p> <ol> <li> <p>Retrieve the GA OpenShift Installer using the get-ocp-installer.sh script in the kni user\u2019s ~/GoodieBag directory.  This script will download the openshift-client installer, ensure the required packages are installed, create the install-config.yaml file, update the file with the appropriate IP addresses, ssh public keys, and pull-secret, and configure DHCP and DNS.  </p> <pre><code>[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh -h \n/tmp/get-ocp-installer.sh usage:\n-v  Specify version, default is \"latest-4.12\".\n-s  Specify full path of pull-secret.txt file, default is \"~/pull-secret.txt\".\n-d  Specify directory to extract the release image in, default is current directory.\n-h  Display help/usage information.\n\n[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh\nGetting the release image name.\nDownloading the openshift-client-linux.tar.gz file.\nExtracting the openshift-client installer.\nThe openshift-baremetal-installer installed successfully.\n\nMaking sure required packages are installed.\nLast metadata expiration check: 0:21:44 ago on Mon 13 Feb 2023 03:02:05 PM EST.\nPackage ansible-2.9.27-1.el8ae.noarch is already installed.\nPackage python3-shade-1.32.0-2.20220110211405.47fe056.el8ost.noarch is already installed.\nPackage python3-openstackclient-4.0.2-2.20220427020029.el8ost.noarch is already installed.\nDependencies resolved.\nNothing to do.\nComplete!\ndone.\nGenerating the install-config.yaml file.\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [Generate configs] **************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************\nok: [localhost]\n\nTASK [Learn kni instances in the project] **************************************************************************************\nok: [localhost]\n\nTASK [Show kni  instances] **************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        {\n            \"OS-DCF:diskConfig\": \"MANUAL\",\n            \"OS-EXT-AZ:availability_zone\": \"leaf1\",\n            \"OS-EXT-SRV-ATTR:host\": null,\n            \"OS-EXT-SRV-ATTR:hostname\": null,\n            \"OS-EXT-SRV-ATTR:hypervisor_hostname\": null,\n    \u2026\n\nPLAY RECAP **********************************************************************************************\nLocalhost        : ok=6    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\ndone.\nUpdating the install-config yaml file.\nConfiguring DHCP and DNS.\nIf you have a failed attempt at the installation, use the ~/GoodieBag/cleanup-ocp.sh script to reset the environment.\nGo forth and deploy openshift-baremetal-installation.\n[kni@bootstrap ~]$\n</code></pre> </li> <li> <p>Review the contents of the ~/GoodieBag/install-configs.yaml file.  All IP addresses for the server instances should be updated from the default of X.X.X.X, and the pullSecrets and sshKey variables should be updated with the correct information.  </p> <pre><code>[kni@bootstrap ~]$ more GoodieBag/install-config.yaml\n\u2026\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:a7:80:64\"\n        role: master\n        rootDeviceHints:\n          deviceName: \"/dev/vda\"\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://10.30.0.129\"\n          username: \"deployodf\"\n          password: \"Passw0rd\"\n\u2026\n</code></pre> </li> <li> <p>To ensure the installation doesn\u2019t get interrupted if there is a disconnect in your ssh session, run the installation in a tmux window.  The tmux utility was installed by the get-ocp-installer.sh script.  </p> <pre><code>[kni@bootstrap ~]$ tmux\n</code></pre> </li> <li> <p>Deploy OpenShift using the openshift-baremetal-install command.  </p> <pre><code>[kni@bootstrap ~]$ openshift-baremetal-install --dir ~/clusterconfigs --log-level debug create cluster\n</code></pre> </li> <li> <p>You can monitor your deployment in another window.  </p> <pre><code>[kni@bootstrap ~]$ sudo virsh list\n Id   Name                        State\n -------------------------------------------\n 1    kni-test2-ntpgv-bootstrap   running\n</code></pre> <p>To display the console output of the boostrap-vm (ctrl + <code>]</code> to exit): <pre><code>[kni@bootstrap ~]$ sudo virsh console &lt;bootstrap-vm&gt;\n</code></pre></p> <p>View the status of the servers deployed through OpenStack: <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc\n(kni-test2) [kni@bootstrap ~]$ openstack server list --insecure\n+--------------------------------------+------------------+---------+----------\n| ID                                   | Name             | Status  | Networks                                                                        | Image          | Flavor |\n+--------------------------------------+------------------+---------+----------\n| d339db1a-4e50-4c69-8063-d7d92bfcc190 | ipmi_kni-master1 | ACTIVE  | virtualipmi=10.30.0.132                                                         | virtualipmi    |        |\n| ccfaf78c-80f5-4b01-b3ee-ab0fd82a07c0 | ipmi_kni-master2 | ACTIVE  | virtualipmi=10.30.0.85                                                          | virtualipmi    |        |\n| 14a636dd-3709-48e9-a6e2-be689d434ac7 | ipmi_kni-master3 | ACTIVE  | virtualipmi=10.30.0.49                                                          | virtualipmi    |        |\n| 77fda0c4-8a88-4970-806e-540e3946dd3a | ipmi_kni-worker1 | ACTIVE  | virtualipmi=10.30.0.45                                                          | virtualipmi    |        | \n| 80c13e57-2869-4473-99e2-c4748daaf84c | ipmi_kni-worker2 | ACTIVE  | virtualipmi=10.30.0.147                                                         | virtualipmi    |        |\n| 32f1de18-8bdb-4bdf-bdc3-57ddb0e522d8 | ipmi_kni-worker3 | ACTIVE  | virtualipmi=10.30.0.210                                                         | virtualipmi    |        |\n| 85369613-44ec-47fa-9a9b-2476ca9278e8 | kni-master1      | ACTIVE  | baremetal0=10.20.0.8; provisioning0=10.10.0.200                                 | pxeboot        |        |\n| 438088a3-0182-4f9f-bcce-485bce5973d3 | kni-master2      | ACTIVE  | baremetal0=10.20.0.119; provisioning0=10.10.0.160                               | pxeboot        |        |\n| 0239711f-e661-47f3-bc71-eea60eec8763 | kni-master3      | ACTIVE  | baremetal0=10.20.0.100; provisioning0=10.10.0.209                               | pxeboot        |        |\n| 26798b59-1d6c-4d93-b51a-26b59c725a33 | kni-worker1      | SHUTOFF | baremetal0=10.20.0.77; provisioning0=10.10.0.91                                 | pxeboot        |        |\n| 3f11240b-5806-40f9-92d4-82a58df4053b | kni-worker2      | SHUTOFF | baremetal0=10.20.0.236; provisioning0=10.10.0.94                                | pxeboot        |        |\n| 75015e47-7210-4bb8-8f45-8da5f3f78829 | kni-worker3      | SHUTOFF | baremetal0=10.20.0.124; provisioning0=10.10.0.109                               | pxeboot        |        || 27c33f7a-ea95-4cd6-aec9-691fbcfe27c1 | bootstrap        | ACTIVE  | baremetal0=10.20.0.122; vlan1117=10.9.65.140; provisioning0=10.10.0.45 | rhel82-update1 |        |\n+--------------------------------------+------------------+---------+----------\n</code></pre></p> <p>Other commands that will come in handy in the later state of the deployment:</p> <pre><code>[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n[kni@bootstrap ~]$ oc get clusteroperators\n[kni@bootstrap ~]$ oc get nodes\n</code></pre> <p>We hope it works! If it has, then at the end of the deployment you will see something like this:</p> <pre><code>INFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.kni-test2.hexo.lab\nINFO Login to the console with user: \"kubeadmin\", and password: \"mv22Z-Tv2Zb-pTFgq-xAvki\"\nDEBUG Time elapsed per stage:\nDEBUG     Infrastructure: 29m15s\nDEBUG Bootstrap Complete: 9m54s\nDEBUG  Bootstrap Destroy: 13s\nDEBUG  Cluster Operators: 34m31s\nINFO Time elapsed: 1h13m54s\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#accessing-your-environment","title":"Accessing Your Environment","text":"<p>There should be a few ways to access your environment including using sshuttle or a jumphost.</p>"},{"location":"Openshift/OCPBaremetalPI/#using-sshuttle","title":"Using sshuttle","text":"<p>The OpenShift Console is not available on the VPN network the lab environment is deployed on but rather the private network of your project.  To access the console, you can use the sshuttle utility which allows you to create a VPN connection using an ssh connection to the bootstrap server.  You need root access on the client machine but not on the bootstrap server.  The sshuttle utility requirements are python 2.3 or higher which are already installed on the bootstrap server. </p>"},{"location":"Openshift/OCPBaremetalPI/#linux-client-installation","title":"Linux Client Installation","text":"<ol> <li> <p>Install using the dnf command from the EPEL repository.  </p> <pre><code>$ dnf install -y sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required to use sshuttle.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.  </p> <pre><code>$ sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n</code></pre> </li> <li> <p>Access your OpenShift Console in your browser using the link:  </p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#macos-client-installation","title":"MacOS Client Installation","text":"<ol> <li> <p>If not installed already, install homebrew.  Open a terminal window and grab the install.sh script from GitHub and run it.  Wait for the command to finish.  If you are prompted to enter a password, enter your Mac user\u2019s login password and press ENTER.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Make the brew command available inside the terminal window.</p> <pre><code>\"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\n. ~/.zprofile\n</code></pre> </li> <li> <p>Install sshuttle.</p> <pre><code>$ brew install sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.</p> <pre><code>~ % sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n[local sudo] Password: \nkni@172.20.17.124's password: \nc : Connected to server.\n</code></pre> <p>NOTE: As traffic is generated on the VPN connection, warning messages will be displayed in the terminal.</p> <p>Example:</p> <p>s: warning: closed channel 7 got cmd=TCP_DATA len=517  c : warning: closed channel 11 got cmd=TCP_STOP_SENDING len=0   s: warning: closed channel 11 got cmd=TCP_DATA len=517   s: warning: closed channel 11 got cmd=TCP_EOF len=0</p> </li> <li> <p>Access your OpenShift Console in your browser using the link:</p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> <p>You will also need to add an entry in your /etc/hosts file for the DNS resolution.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#using-a-jumphost-from-openstack","title":"Using a JumpHost from OpenStack","text":"<ol> <li> <p>Access your OpenStack environment, select Compute-&gt;Instances.  Click the Launch Instance icon on the right.  Enter the Instance Name, Description, AZ, and Count.  Click Next.  </p> <p>NOTE: At this time, the procedures below will yield issues with accessing the jumphost due to a known issue with injecting the ssh keys. Please use the procedures under Using sshuttle above.</p> </li> <li> <p>On the Source screen, make sure Select Boot Source is Image and Create New Volume is No.  In the Available list of images, find fedora-cloud37 and click the up arrow to the right of the entry to move it up to the Allocated section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Flavor screen, find t2.medium in the list of Available flavors, click the up arrow to the right of the entry to move it to Allocated.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Networks screen, add the vlan1117 and baremetal0 entries in the Available list to the Allocated list.  Scroll to the bottom, click Next.</p> </li> <li> <p>Click Next in the lower right on the Network Ports screen.  On the Security Groups screen, click the down arrow next to the default security group in the Allocated section to move it to the Available section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Key Pair screen, import your ssh keys using the Import Key Pair icon.  Remove the hextupleo_pub_key key from Allocated by clicking the down arrow to the far right.  Once complete, click the Launch Instance in the lower right as the remaining sections are not required or needing to be updated.</p> </li> <li> <p>Ssh to environment and enable x11 and maybe firefox/chrome if you\u2019d like.  </p> <p>``` [fedora@fedora-jumpbox ~]$ sudo dnf -y groupinstall gnome [fedora@fedora-jumpbox ~]$ sudo dnf -y group install \"Basic Desktop\" GNOME [fedora@fedora-jumpbox ~]$ sudo systemctl set-default graphical.target Removed /etc/systemd/system/default.target. Created symlink /etc/systemd/system/default.target \u2192 /usr/lib/systemd/system/graphical.target. [fedora@fedora-jumpbox ~]$ sudo -i [root@fedora-jumpbox ~]# passwd fedora Changing password for user fedora. New password:  BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word Retype new password:  passwd: all authentication tokens updated successfully. [root@fedora-jumpbox ~]# reboot</p> </li> <li> <p>Adding DNS is optional and not a requirement.</p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-odf-storage","title":"Deploying ODF (storage)","text":"<p>Official docs -&gt; https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/index </p> <ol> <li> <p>Install local storage operator; click Operators-&gt;OperatorHub.  Search for local.  Select the Local Storage Operator, in the pop up window, click the Install icon.  </p> <p></p> </li> <li> <p>Keep the defaults; scroll to the bottom of the screen and click the Install icon.  </p> <p></p> </li> <li> <p>Install ODF Operator; click Operators-&gt;OperatorHub, search for Data Foundation.  Select the OpenShift Data Foundation Operator, in the pop-up window keep the defaults, scroll to the bottom and click Install.  </p> <p></p> </li> <li> <p>Once the operator has successfully been installed, the GUI will indicate that a change has occurred and to refresh.  Create the StorageSystem; select Storage-&gt;Data Foundation.  Click the Storage System tab; click the Create StorageSystem icon on the right.  </p> <p></p> </li> <li> <p>For the Backing storage type, select Create a new StorageClass using local storage devices.  Click Next.  </p> <p></p> </li> <li> <p>The worker nodes have been preconfigured with 100GB secondary drives.  </p> <pre><code>[kni@bootstrap ~]$ ssh core@kni-worker1\nRed Hat Enterprise Linux CoreOS 412.86.202301191053-0\nPart of OpenShift 4.12, RHCOS is a Kubernetes native operating system\nmanaged by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\nhttps://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html\n\n[core@kni-worker1 ~]$ sudo fdisk -l | grep vdb\nDisk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors\n</code></pre> </li> <li> <p>On the Create local volume set screen, enter a name for the volume set.  In the example below, localvolumes is used.  Verify the Disks on all nodes (3 node) is selected in the Filter disks by section and All is selected for the Disk type.  Click the Next icon.  </p> <p></p> </li> <li> <p>A pop up confirmation window will display providing additional information to consider if this is a stretched cluster.   Click the Yes icon if the settings are correct to create the LocalVolumeSet.  </p> </li> <li> <p>On the Capacity and nodes screen, accept the defaults and click Next.  </p> <p></p> </li> <li> <p>On the Security and network screen, accept the default of SDN.  Click Next.</p> </li> <li> <p>On the Review and create screen, review the configuration and click Next if correct to create the Storage System.</p> </li> <li> <p>It will take several minutes to create the local volumes and configure the storage.  You can monitor the progress on the Storage-&gt;Data Foundation-&gt;StorageSystems screen.  Click the ocs-storage-cluster-storagesystem link to access the Overview dashboard.  As the system configures the storages, messages will be logged in the Activity section.  Once complete, the Status for Storage Cluster and Data Resiliency should be green.  </p> <p></p> </li> <li> <p>To verify local storage has been create, click the BlockPools menu item along the top; click the ocs-storagecluster-cephblockpool link.  In the Inventory widget, click the links to view the available Storage Classes or the Persistent Volume Claims.  </p> <p></p> <p>Storage Class:</p> <p></p> <p>PersistentVolumeClaims:</p> <p></p> </li> <li> <p>The local storage and volume claims can be viewed with the CLI.  </p> <pre><code>[kni@bootstrap ~]$ oc get all -n openshift-local-storage\nNAME                                         READY   STATUS    RESTARTS   AGE\npod/diskmaker-discovery-2xnxv                2/2     Running   0          3m46s\npod/diskmaker-discovery-9vjbf                2/2     Running   0          3m49s\npod/diskmaker-discovery-ff2cc                2/2     Running   0          4m2s\npod/diskmaker-manager-c54j8                  2/2     Running   0          2m10s\npod/diskmaker-manager-prgbh                  2/2     Running   0          2m10s\npod/diskmaker-manager-xtw58                  2/2     Running   0          2m10s\npod/local-storage-operator-9bc77c9cf-rzqjt   1/1     Running   0          32m\n\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/local-storage-discovery-metrics   ClusterIP   172.30.23.42    &lt;none&gt;        8383/TCP   11m\nservice/local-storage-diskmaker-metrics   ClusterIP   172.30.248.10   &lt;none&gt;        8383/TCP   2m10s\n\nNAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/diskmaker-discovery   3         3         3       3            3           &lt;none&gt;          11m\ndaemonset.apps/diskmaker-manager     3         3         3       3            3           &lt;none&gt;          2m10s\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/local-storage-operator   1/1     1            1           32m\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/local-storage-operator-9bc77c9cf   1         1         1       32m\n\n[kni@bootstrap ~]$ oc get pv\nNAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\nlocal-pv-2cc6fb0    100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-3faa9a90   100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-e2c0ad97   100Gi      RWO            Delete           Available           localvolumes            114s     3m45s\n</code></pre> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#deploying-ocpv","title":"Deploying OCPv","text":""},{"location":"Openshift/OCPBaremetalPI/#deleting-your-project","title":"Deleting Your Project","text":"<ol> <li> <p>To remove your entire project, use the Template in Ansible Automation Platform.</p> <p>Ansible Automation Platform</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Hextupleo - delete my project.</p> <p></p> </li> <li> <p>Update the project_name: and project_password: fields.  Click the Next icon in the lower left.</p> </li> <li> <p>Review the information on the Preview screen.  When ready to move forward, click the Launch icon in the lower left corner.  The Jobs Output screen for the new job will display ongoing output as the system progresses through the Ansible playbook.  You can monitor to completion or just check back to ensure your Job finishes successfully.</p> <p></p> </li> <li> <p>Verify your job finished successfully in the Jobs listing.</p> <p></p> </li> </ol>"},{"location":"Openshift/OCPBaremetalPI/#troubleshooting","title":"Troubleshooting","text":"<p>Getting NTP applied for workers and masters.  </p> <pre><code>[kni@bootstrap ~]$ wget \n[kni@bootstrap ~]$ wget \n\n[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n\n[kni@bootstrap ~]$ oc apply -f 99_workers-chrony-configuration.yaml\n[kni@bootstrap ~]$ oc apply -f 99_masters-chrony-configuration.yaml\n</code></pre> <p>Check status of IPMI.  </p> <pre><code>kni@bootstrap ~]$ ipmitool -I lanplus -H 10.30.0.129 -U deployodf -P Passw0rd chassis power status\n</code></pre>"},{"location":"Openshift/OCPBaremetalPI/#appendix","title":"Appendix","text":"<p>The get-ocp-installer.sh script is provided as a quick method to complete the prep work before installing OpenShift.  If you want to execute manually, the commands are provided below for your reference.  </p> <p>Set the environment variables and download the openshift-client-linux.tar.gz file.  </p> <pre><code>[kni@bootstrap ~]$ export VERSION=latest-4.12\n[kni@bootstrap ~]$ export RELEASE_IMAGE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/release.txt | grep 'Pull From: quay.io' | awk -F ' ' '{print $3}')\n[kni@bootstrap ~]$ export cmd=openshift-baremetal-install\n[kni@bootstrap ~]$ export pullsecret_file=~/pull-secret.txt\n[kni@bootstrap ~]$ export extract_dir=$(pwd)\n[kni@bootstrap ~]$ curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/openshift-client-linux.tar.gz | tar zxvf - oc\noc\n[kni@bootstrap ~]$ sudo cp oc /usr/local/bin\n[kni@bootstrap ~]$ oc adm release extract --registry-config \"${pullsecret_file}\" --command=$cmd --to \"${extract_dir}\" ${RELEASE_IMAGE}\n[kni@bootstrap ~]$ sudo cp openshift-baremetal-install /usr/local/bin\n</code></pre> <p>Verify the installer file has been downloaded.  </p> <pre><code>[kni@bootstrap ~]$ ls\nGoodieBag  nohup.out  oc  openshift-baremetal-install  pull-secret.txt\n</code></pre> <p>Create or generate install-config.yaml.</p> <p>For your convenience, there is an ansible playbook inside a Goodiebag directory that helps gather mac information for the OCP nodes. It uses openstack APIs to gather the information. Install the required packages first Note: This step would typically not be performed on different hardware.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf -y install ansible python3-shade python3-openstackclient\n</code></pre> <p>Generate the install-config.yaml  </p> <pre><code>[kni@bootstrap ~]$ ansible-playbook GoodieBag/generate-configs.yml\n</code></pre> <p>The file will look similar to this:  </p> <pre><code>[kni@bootstrap ~]$ cat GoodieBag/install-config.yaml \napiVersion: v1\nbasedomain: hexo4.lab\nmetadata:\n  name: \"kni-test\"\nnetworking:\n  machineCIDR: 10.20.0.0/24\n  networkType: OVNKubernetes\ncompute:\n  - name: worker\n     replicas: 3\ncontrolPlane:\n  name: master\n  replicas: 3\n  platform:\n    baremetal: {}\nplatform:\n  baremetal:\n    apiVIP: &lt;api-ip&gt;\n    ingressVIP: &lt;wildcard-ip&gt;\n    provisioningNetworkCIDR: &lt;CIDR&gt;\n    hosts:\n      - name: \"kni_worker3\"\n        bootMACAddress: \"fa:16:3e:0f:21:4d\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker2\"\n        bootMACAddress: \"fa:16:3e:5c:94:dd\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker1\"\n        bootMACAddress: \"fa:16:3e:86:ce:e8\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master3\"\n        bootMACAddress: \"fa:16:3e:2e:7a:86\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master2\"\n        bootMACAddress: \"fa:16:3e:23:03:f5\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:43:75:71\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\npullSecret: '&lt;pull_secret&gt;'\nsshKey: '&lt;ssh_pub_key&gt;'\n</code></pre> <p>You must update the ipmi IP addresses for each of the server instances, the pullSecret, and sshKey variables.  Get the ipmi addresses using the openstack server command.  First, you need to source the variables in the ~/GoodieBag/rc file.   <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc   &lt; - replace with your projectname\n(kni-test) [kni@bootstrap ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"      \" $8 }'\nipmi_kni_worker3      virtualipmi=10.30.0.106\nipmi_kni_worker2      virtualipmi=10.30.0.122\nipmi_kni_worker1      virtualipmi=10.30.0.139\nipmi_kni-master3      virtualipmi=10.30.0.37\nipmi_kni-master2      virtualipmi=10.30.0.150\nipmi_kni-master1      virtualipmi=10.30.0.132\n</code></pre> <p>Update each server\u2019s host section replacing the X.X.X.X with the correct IP address.</p> <pre><code>hosts:\n      - name: \"kni_worker3\"\n      bootMACAddress: \"fa:16:3e:0f:21:4d\"\n      role: worker\n      hardwareProfile: unknown\n      bmc:\n        address: \"ipmi://X.X.X.X\"  \u2190 replace with 10.30.0.106\n        username: \"kni-test\"\n        password: \"changeme\"\n</code></pre> <p>Update the pullSecret variable at the bottom of the file with the pull-secret.txt file contents you created in the Openshift Installation.  </p> <pre><code>(kni-test) [kni@bootstrap ~]$ cat pull-secret.txt \n{\"auths\":{\"cloud.openshift.com\":{\"auth\":.....\n\npullSecret:'{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3BlbnNoaWHVXQkdaVy1jTVhZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NMkRRN1RTWQxN2ExYjhmOTU6TQxN2Q2NzViOTESDA3UEFGOFNA4QzJTQVOTDNDUFU5SlJY8UV0pSWl\n\u2026\nnZnVzNfN3NDA1NWIyMWU4Zjckx2ZGJHVVzFJd1FNLXRmeUxFcFVRZnVuRTJWdkNpo3MEw3MlJGl2czNkRzLUdpeBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}'\n</code></pre> <p>Update the sshKey variable with your ssh public key.  If you have not generated it, use the ssh-keygen command.  </p> <pre><code>[kni@bootstrap ~]$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/kni/.ssh/id_rsa): &lt;enter&gt;\nEnter passphrase (empty for no passphrase): &lt;enter&gt; \nEnter same passphrase again: \nYour identification has been saved in /home/kni/.ssh/crapid_rsa.\nYour public key has been saved in /home/kni/.ssh/crapid_rsa.pub.\nThe key fingerprint is:\nSHA256:CVnMl7uiOYLFK1jNkWQZ7M29cbsNhdP13Hfvbsy5Yfg kni@bootstrap\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   ..o o.  .     |\n|    =  oo o   .  |\n|   + +o. . + . o.|\n|    + o.o.= o   *|\n|   + .  S+ =    +|\n|  . =   o +   . .|\n| o o . o . + . *.|\n|. o o +   . . o.*|\n|   . . .       Eo|\n+----[SHA256]-----+\n[kni@bootstrap ~]$ cat .ssh/id_rsa.pub \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3PtY3V32PbEbXpuVaaPV\n\u2026\nfionHKM8gdFbXo8yWqCTdT3HuMs9gjjjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap\n</code></pre> <p>Copy the id_rsa.pub contents and paste as the sshKey.</p> <pre><code>sshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3Pt  \n\u2026\nfjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap'\n</code></pre> <p>Openshift Baremetal IPI also requires DHCP and DNS to be configured.  Copy the respective files from the GoodieBag directory to /etc.  Enable and start the dnsmasq daemon.  </p> <pre><code>[kni@bootstrap ~]$ sudo cp GoodieBag/hosts /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/kni.dns /etc/dnsmasq.d/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf.upstream /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf /etc/\n[kni@bootstrap ~]$ sudo systemctl enable dnsmasq\nCreated symlink /etc/systemd/system/multi-user.target.wants/dnsmasq.service \u2192 /usr/lib/systemd/system/dnsmasq.service.\n[kni@bootstrap ~]$ sudo systemctl start dnsmasq\n</code></pre> <p>If this is not your first attempt at installing OpenShift, cleanup the remnants of the first attempt. </p> <pre><code>[kni@bootstrap ~]$ for i in $(sudo virsh list | tail -n +3 | grep bootstrap | awk {'print $2'})\ndo\n    sudo virsh destroy $i;\n    sudo virsh undefine $i;\n    sudo virsh vol-delete $i --pool default;\n    sudo virsh vol-delete $i.ign --pool default;\ndone\n[kni@bootstrap ~]$ rm -rf ~/clusterconfigs/auth ~/clusterconfigs/terraform* ~/clusterconfigs/tls ~/clusterconfigs/metadata.json\n</code></pre> <p>Create the clusterconfigs working directory in the kni home directory.</p> <pre><code>[kni@bootstrap ~]$ mkdir ~/clusterconfigs\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/","title":"OSP 16.2 Installation Instructions","text":""},{"location":"Openstack/OSP16.2Instructions/#introduction","title":"Introduction","text":""},{"location":"Openstack/OSP16.2Instructions/#goals","title":"Goals:","text":"<ul> <li>Enable field on OSP 16.2</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to OSP Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OSP and director </li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"Openstack/OSP16.2Instructions/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Undercloud 16G 4 <ul><li>1x pxe</li><li>1x external</li> 100GB Controller 12GB 2 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li><li>1x external</li> 60 GB Compute 4GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li> 60GB Ceph 4GB 2 <ul><li>1x pxe</li><li>1x storage</li><li>1x storagemgmt</li><li>1x storage</li> <ul><li>50GB OSD</li><li>100GB (osd)</li> HCI (optional) 8GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li> <ul><li>60GB OSD</li><li>100GB (osd)</li> Custom (optional)"},{"location":"Openstack/OSP16.2Instructions/#building-your-hextupleo-lab","title":"Building your HextupleO Lab:","text":"<p>HextupleO is an upstream project built with ansible playbooks talking directly to OpenStack APIs via python-shade libraries. It also nicely integrates with Ansible Automation Platform for ease of deployment and manageability (you can learn more about it here).</p>"},{"location":"Openstack/OSP16.2Instructions/#default-deployment","title":"Default Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Deploy OpenStack Environment. </p> </li> <li> <p>Provide a unique project_name and password. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Wait for the deployment to finish.  You can monitor via the Jobs Output screen or you can monitor each individual job by selecting Jobs from the left pane and selecting the appropriate jobs.  There are four jobs, create project, create networks, create instances, and configure OSP undercloud.</p> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates Tab and hit the \u201crocket\u201d icon next to - \u201cHextupleol - create  project\u201d, after you hit \u201cNext\u201d, you will get a survey prompting for user and password</p> </li> <li> <p>Set user/project and password and submit the job. The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create networks.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>networks:  \n  - { name: \"external0\", cidr: \"10.1.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"internalAPI0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"tenant0\", cidr: \"10.30.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storage0\", cidr: \"10.40.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storagemgmt0\", cidr: \"10.50.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provider0\", cidr: \"10.60.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"virtualipmi\", cidr: \"10.70.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create instances.  Below is a good starting config with controllers, 2 compute, and 3 Ceph nodes.  </p> <pre><code>instances:  \n  - { name: \"undercloud\", image: \"rhel-8.2\", flavor: \"undercloud\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"provider1-vlan217, net_name2: \"provisioning0, net_name3: \"external0\", net_name4: \"provider0\", net_name5: \"virtualipmi\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_controller1\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller2\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller3\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_compute1\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_compute2\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph1\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph2\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph3\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - configure OSP undercloud.  </p> </li> <li> <p>Follow the survey and submit the job.</p> </li> <li> <p>At the end you will be getting a screen similar to this:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user stack using the password you set in the playbook.</p>"},{"location":"Openstack/OSP16.2Instructions/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":"<p>This is accessing the Openstack that your environment is deployed to.</p>"},{"location":"Openstack/OSP16.2Instructions/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link: </p> <p>Horizon Login</p> <p>NOTE: You must be connected to the NA-SSA VPN</p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  </p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address that is associated with a provider vlan. You will use that IP to access your undercloud node. This should match the above output from the Ansible job.  You can SSH to this IP as the stack user using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected. </p> </li> </ol> <p>NOTE: We have created Tenant (over GENEVE/VXLAN) networks to satisfy all the non-routable networks. Only External Network is connected to one of the provider routable networks and accessible from outside of your deployed environment.</p> <p></p>"},{"location":"Openstack/OSP16.2Instructions/#undercloud","title":"Undercloud","text":"<ol> <li> <p>Access undercloud via ssh using stack@ip-learned-from-horizon  / password specified in Tower.  </p> </li> <li> <p>You can now start deploying OSP16.2 based on standard instructions (or whatever version you have staged).  See below notes for helpful information in the deployment process.</p> <p>NOTE:  Repos:  Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the osp repo file from here:</p> <p>[stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p>~/GoodieBag/deploy.sh - pre-configured deploy script that can be used as a template.</p> <p>~/templates - Directory that has multiple templates that have been pre-configured for making the vanilla deployment easier to perform. Undercloud.conf - sample undercloud.conf file with pre-configured known working settings like IP pools, interface settings, and more.  The egrep command is used to remove blank and comment lines.</p> <pre><code>[stack@undercloud ~]$ egrep -v '(^$|^#)' undercloud.conf\n[DEFAULT]\ncertificate_generation_ca = local\nclean_nodes = true\ncontainer_images_file = /home/stack/templates/containers-prepare-parameter.yaml\ngenerate_service_certificate = true\nhieradata_override = /home/stack/templates/undercloud_hiera.yaml\nlocal_interface = eth1\nlocal_ip = 10.10.0.10/24\nlocal_mtu = 8946\nlocal_subnet = ctlplane-subnet\novercloud_domain_name = hextupleo.lab\nsubnets = ctlplane-subnet\nundercloud_admin_host = 10.10.0.11\nundercloud_debug = false\nundercloud_hostname = osp-blm-undercloud.hextupleo.lab\nundercloud_nameservers = 172.20.129.10\nundercloud_ntp_servers = 172.20.129.10\nundercloud_public_host = 10.1.0.11\n[ctlplane-subnet]\ncidr = 10.10.0.0/24\ndhcp_end = 10.10.0.149\ndhcp_start =  10.10.0.100\ndns_nameservers = 10.10.0.10\ngateway = 10.10.0.10\ninspection_iprange = 10.10.0.200,10.10.0.249\nmasquerade = true\nmasquerade_network = 10.10.0.0/16\n[stack@undercloud ~]$  \n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#deploy-vanilla-osp-162","title":"Deploy Vanilla OSP 16.2","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, It is highly encouraged for everyone to get at least one vanilla deployment done to get themselves familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation. Please review the official documentation for accuracy and open any Bugzilla\u2019s against it!</p>"},{"location":"Openstack/OSP16.2Instructions/#undercloud-installation","title":"Undercloud Installation","text":"<ol> <li> <p>Log into the undercloud VM as the stack user using the IP address that was obtained during the VM deployments in Horizon.</p> <p>NOTE: You must be connected to the NA-SSA VPN to access the environment.</p> </li> <li> <p>Complete the RHEL upgrade requirements.</p> <p>Make sure you grabbed the repo configuration from the DNS server. [stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p><pre><code>[stack@undercloud ~]$ sudo dnf module reset container-tools\n[stack@undercloud ~]$ sudo dnf module enable -y container-tools:3.0\n[stack@undercloud ~]$ sudo yum update -y\n[stack@undercloud ~]$ sudo reboot \n</code></pre> 3. Install the TripleO Director, Ceph Ansible, tmux (optional) packages, and prepare the container images.</p> <pre><code>[stack@undercloud ~]$ sudo yum install -y python3-tripleoclient ceph-ansible tmux\n[stack@undercloud ~]$ openstack tripleo container image prepare default --local-push-destination --output-env-file ~/templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\nOutput env file exists, moving it to backup.\n</code></pre> </li> <li> <p>Starting with OSP15, Red Hat is moving to the Container Registry that requires an active Red Hat account.  In order to continue the installation, you need to include your credentials into the container-prepare-parameter.yaml file.  Service credentials can be created for the deployment using the Customer Portal Terms-Based-Registry site.</p> <p>Click the New Service Account icon in the upper right corner.  Enter a name for the service account and a description.  Click the Create icon.</p> <p></p> <p>Click the Copy icon on the far right to copy the new token to your local clipboard.</p> <p></p> </li> <li> <p>Update the containers-prepare-parameter.yaml file with the new service account name and token.  Open the file and jump to the bottom.  Add the highlighted lines and paste the user name and token generated from Step 4.  The ContainerImageRegistryCredentials space aligns with the ContainerImagePrepare line at the top of the file.</p> <pre><code>[stack@undercloud ~]$ vi templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-08T09:20:45.446840\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n...\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4N\n      ...\n      j1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>Update the undercloud.conf file if necessary.  This should not be required, but feel free to review and understand the parameters.</p> </li> <li> <p>Install the undercloud.</p> <pre><code>[stack@undercloud ~]$ openstack undercloud install\nundercloud_admin_host or undercloud_public_host is not in the same cidr as local_ip.\nConfig option undercloud_public_host \"10.1.0.11\" not in defined CIDR \"10.10.0.0/24\"\nRunning: sudo --preserve-env openstack tripleo deploy --standalone --standalone-role Undercloud --stack undercloud --local-domain=hextupleo.lab --local-ip=10.10.0.10/24 --templates=/usr/share/openstack-tripleo-heat-templates/ --networks-file=network_data_undercloud.yaml --heat-native -e /usr/share/openstack-tripleo-heat-templates/environments/undercloud.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/use-dns-for-vips.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/podman.yaml -e /home/stack/templates/containers-prepare-parameter.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/masquerade-networks.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic-inspector.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/mistral.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/zaqar-swift-backend.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/tempest.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/public-tls-undercloud.yaml --public-virtual-ip 10.1.0.11 --control-virtual-ip 10.10.0.11 -e /usr/share/openstack-tripleo-heat-templates/environments/ssl/tls-endpoints-public-ip.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/undercloud-haproxy.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/services/undercloud-keepalived.yaml --deployment-user stack --output-dir=/home/stack --cleanup -e /home/stack/tripleo-config-generated-env-files/undercloud_parameters.yaml --hieradata-override=/home/stack/templates/undercloud_hiera.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/tripleo-validations.yaml --log-file=install-undercloud.log -e /usr/share/openstack-tripleo-heat-templates/undercloud-stack-vstate-dropin.yaml\nThe heat stack undercloud action is CREATE\n2023-04-10 13:54:09.767 11371 INFO migrate.versioning.api [-] 70 -&gt; 71... \n2023-04-10 13:54:09.804 11371 INFO migrate.versioning.api [-] done\n...\n</code></pre> </li> <li> <p>Once the installation is complete, verify the containers are up and source the stackrc file to set the environment for the undercloud environment.</p> <pre><code>[stack@undercloud ~]$ sudo podman ps\n[stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ \n</code></pre> </li> <li> <p>Obtain the images for the overcloud nodes.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo dnf -y install rhosp-director-images rhosp-director-images-ipa-x86_64\n</code></pre> </li> <li> <p>Extract the tar balls for the overcloud and ironic images.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/images\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/overcloud-full-latest-16.2.tar\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/ironic-python-agent-latest-16.2.tar\n</code></pre> </li> <li> <p>The root password can be changed in the overcloud image using the procedures below.  This is optional and is not required.</p> <pre><code>(undercloud) [stack@undercloud ~]$ export LIBGUESTFS_BACKEND=direct\n(undercloud) [stack@undercloud ~]$ virt-customize -a overcloud-full.qcow2 --root-password password:changeme\n</code></pre> <p>NOTE: the libguestfs-tools package may need to be installed:        sudo dnf install -y libguestfs-tools</p> </li> <li> <p>Upload the overcloud images and verify they are available to the undercloud.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud image upload --image-path /home/stack/images\n(undercloud) [stack@undercloud ~]$ openstack image list\n+--------------------------------------+------------------------+--------+\n| ID                                   | Name                   | Status |\n+--------------------------------------+------------------------+--------+\n| f3e73e81-acca-45d7-a848-7d6de40933ed | overcloud-full         | active |\n| f8162188-4ec9-4312-9519-2e6421bd52a8 | overcloud-full-initrd  | active |\n| da2195d1-d75f-4134-b906-1302ff9943af | overcloud-full-vmlinuz | active |\n+--------------------------------------+------------------------+--------+\n</code></pre> </li> <li> <p>Verify the DNS server has been set for the cltplane subnet.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack subnet show ctlplane-subnet\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field             | Value                                                                                                                                                   |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| allocation_pools  | 10.10.0.100-10.10.0.149                                                                                                                                 |\n| cidr              | 10.10.0.0/24                                                                                                                                            |\n| created_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n| description       |                                                                                                                                                         |\n| dns_nameservers   | 10.10.0.10                                                                                                                                              |\n| enable_dhcp       | True                                                                                                                                                    |\n| gateway_ip        | 10.10.0.10                                                                                                                                              |\n| host_routes       |                                                                                                                                                         |\n| id                | 2096d5b9-7516-4146-ae4b-919a73f82a8f                                                                                                                    |\n| ip_version        | 4                                                                                                                                                       |\n| ipv6_address_mode | None                                                                                                                                                    |\n| ipv6_ra_mode      | None                                                                                                                                                    |\n| location          | cloud='', project.domain_id=, project.domain_name='Default', project.id='cd6a92e810154ab882d290a70e8c6afc', project.name='admin', region_name='', zone= |\n| name              | ctlplane-subnet                                                                                                                                         |\n| network_id        | 316ccfd5-f1b3-455c-9856-cbbed5b65ba7                                                                                                                    |\n| prefix_length     | None                                                                                                                                                    |\n| project_id        | cd6a92e810154ab882d290a70e8c6afc                                                                                                                        |\n| revision_number   | 0                                                                                                                                                       |\n| segment_id        | None                                                                                                                                                    |\n| service_types     |                                                                                                                                                         |\n| subnetpool_id     | None                                                                                                                                                    |\n| tags              |                                                                                                                                                         |\n| updated_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#overcloud-installation","title":"Overcloud Installation","text":"<p>The first step in deploying the overcloud is to generate the instackenv.yaml file.  Once this is complete, the file needs to be updated with the IP addresses of the VMs that were deployed by HextupleO.</p> <ol> <li> <p>Generate the instackenv.yaml file using the ansible-playbook.  Once the file is generated, source the projectNamerc file for the overcloud environment.  This will allow you to get the list of servers and their IP addresses.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/GoodieBag\n(undercloud) [stack@undercloud ~]$ ansible-playbook generate_instackenv.yml\n(undercloud) [stack@undercloud ~]$ source *projectName*rc\n(myproject) [stack@undercloud ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"    \" $8 }'\nipmi_overcloud_ceph3    virtualipmi=10.70.0.240\nipmi_overcloud_ceph2    virtualipmi=10.70.0.236\nipmi_overcloud_ceph1    virtualipmi=10.70.0.24\nipmi_overcloud_compute2    virtualipmi=10.70.0.11\nipmi_overcloud_compute1    virtualipmi=10.70.0.22\nipmi_overcloud_controller3    virtualipmi=10.70.0.70\nipmi_overcloud_controller2    virtualipmi=10.70.0.233\nipmi_overcloud_controller1    virtualipmi=10.70.0.227\n</code></pre> </li> <li> <p>Update the instackenv.yaml file with the IP addresses.  You can do this manually, or you can use this code:</p> <pre><code>openstack server list --insecure | awk '/ipmi_/ {print $4 \"    \" $8}' &gt; /tmp/ipmi_addresses.txt\ncp ~/GoodieBag/instackenv.yaml ~/\nsed -i '/pm_addr/d' ~/instackenv.yaml\nfor NODE in $(grep 'name: ' ~/instackenv.yaml | awk '{print $NF}' | sed 's/\"//g')\ndo\n  IP=$(egrep ${NODE} /tmp/ipmi_addresses.txt | awk -F= '{print $NF}')\n  sed -i \"/name: \\\"${NODE}\\\"/a \\    pm_addr: \\\"${IP}\\\"\" ~/instackenv.yaml\ndone\n</code></pre> <p>NOTE: Make sure you view the ~/instackenv.yaml file to ensure it is correct before continuing with the installation.</p> </li> <li> <p>Register the nodes for the overcloud.  Make sure you source the stackrc file for the undercloud environment.  Once the import is complete, list the baremetal nodes and ensure all information is correct for the deployment.  All nodes will be in a power off or manageable state.</p> <pre><code>(undercloud) [stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ openstack overcloud node import ~/instackenv.yaml\n(undercloud) [stack@undercloud ~]$ openstack baremetal node list\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name                  | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | 59118ee3-785a-45ef-bef7-8ce4739e34f6 | power off    | manageable             | False       |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | 55eefb7f-e5ad-4987-89ae-fe9ee2fb10dd | power off    | manageable             | False       |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | de0b39b9-b69e-4316-a2b7-743fa95acb65 | power off    | manageable             | False       |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | 5c61b62a-5324-4c20-9dc0-e3cfa866ffbb | power off    | manageable             | False       |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | d9bc6142-5780-45a2-899e-e5dc9d806ab9 | power off    | manageable             | False       |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | 2bb2ca3e-819a-46ce-aa6e-7a4d900875e3 | power off    | manageable             | False       |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | 5a55912a-9c7c-4e35-bde2-c582bbfa1c28 | power off    | manageable             | False       |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | 7f573f3c-8389-4973-aba7-99f055071632 | power off    | manageable             | False       |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n</code></pre> <p>NOTE: Continue to monitor until the nodes are all in an manageable state.</p> </li> <li> <p>Run the introspect to assign the profiles and configure the nodes successfully.  After all nodes are in an available state, verify the proper profiles were assigned.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud node introspect --all-manageable --provide\n(undercloud) [stack@undercloud ~]$ openstack overcloud profiles list\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| Node UUID                            | Node Name             | Provision State | Current Profile | Possible Profiles |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | active          | ceph-storage    |                   |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | active          | ceph-storage    |                   |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | active          | ceph-storage    |                   |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | active          | compute         |                   |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | active          | compute         |                   |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | active          | control         |                   |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | active          | control         |                   |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | active          | control         |                   |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n</code></pre> </li> <li> <p>The undercloud public endpoints have been most likely encrypted with self-signed certificates.  Make sure to inject the cert into the deployment of the overcloud.  Copy the inject-trust-anchor-hiera.yaml file to the templates directory, copy the cert from the cm-local-ca.pem file and paste into the new file in the templates directory.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ cp /usr/share/openstack-tripleo-heat-templates/environments/ssl/inject-trust-anchor-hiera.yaml ~/templates\n(undercloud) [stack@undercloud ~]$ cat /etc/pki/ca-trust/source/anchors/cm-local-ca.pem\nBag Attributes\n    localKeyID: 39 D8 6C E9 C8 7F 65 01 20 80 25 09 E7 A4 41 EB 5D 5E 4E E9 \n    friendlyName: Local Signing Authority\nsubject=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\nissuer=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\n-----BEGIN CERTIFICATE-----\nMIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n...\nLO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\nfaM=\n-----END CERTIFICATE-----\n(undercloud) [stack@undercloud ~]$ vi ~/templates/inject-trust-anchor-hiera.yaml\n# *******************************************************************\n# This file was created automatically by the sample environment\n# generator. Developers should use `tox -e genconfig` to update it.\n# Users are recommended to make changes to a copy of the file instead\n# of the original, if any customizations are needed.\n# *******************************************************************\n# title: Inject SSL Trust Anchor on Overcloud Nodes\n# description: |\n#   When using an SSL certificate signed by a CA that is not in the default\n#   list of CAs, this environment allows adding a custom CA certificate to\n#   the overcloud nodes.\nparameter_defaults:\n  # Map containing the CA certs and information needed for deploying them.\n  # Type: json\n  CAMap:\n    undercloud:\n      content: |\n        -----BEGIN CERTIFICATE-----\n        MIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n        ...\n        LO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\n        faM=\n        -----END CERTIFICATE-----\n</code></pre> <p>NOTE: Make sure that you line up the indentation for the certificate data.  It must be indented two spaces under the content tag.  </p> </li> <li> <p>Copy the deploy.sh template from the GoodieBag directory to the stack user's home directory.  Edit the /home/stack/deploy.sh file and add the inject-trust-anchor-hiera.yaml file.  Ensure you have an understanding of each of the yaml files included.  </p> <p>NOTE: If you want to deploy the Ceph Dashboard on the external network, you need to deploy it at the time of the initial overcloud deployment.  Create the ~/templates/ceph-dashboard-network-override.yaml file and include it in the ~/deploy.sh script.</p> <pre><code>(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi templates/ceph-dashboard-newtork-override.yaml\nparameter_defaults:\n  ServiceNetworkMap: \n    CephDashboardNetwork: external\n:wq\n</code></pre> <p>NOTE: If you are going to enable the RGW service in Ceph, make sure to include the ceph-rgw.yaml file in the initial deployment.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ cat deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack myproject \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml \\\n     -e templates/ceph-dashboard-network-override.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     -e templates/ceph_dashboard_network_override.yaml \\\n     --log-file myproject_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>In HextupleO 4, we are relying on the undercloud to provide NTP services.  By default, OSP v16 doesn't allow time sync from it's chrony service.  As a workaround, execute the following which opens the port via iptables and then allows a sync via chrony.  Restart the chrony service once complete.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo iptables -I INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT\n(undercloud) [stack@undercloud ~]$ echo \"echo allow 0.0.0.0/0 &gt;&gt; /etc/chrony.conf\" | sudo /bin/bash\n(undercloud) [stack@undercloud ~]$ cat /etc/chrony.conf\n# Do not manually edit this file.\n# Managed by ansible-role-chrony\nserver 172.20.129.10 iburst minpoll 6 maxpoll 10\nbindcmdaddress 127.0.0.1\nbindcmdaddress ::1\nallow 10.10.0.0/24\ndriftfile /var/lib/chrony/drift\nlogdir /var/log/chrony\nrtcsync\nmakestep 1.0 3 \nallow 0.0.0.0/0\n(undercloud) [stack@undercloud ~]$ sudo systemctl restart chronyd\n</code></pre> </li> <li> <p>Execute the deploy.sh script to deploy the overcloud.  This takes a very long time to deploy so make sure you run the script in a tmux session.</p> <pre><code>(undercloud) [stack@undercloud ~]$ tmux  \n(undercloud) [stack@undercloud ~]$ cd   \n(undercloud) [stack@undercloud ~]$ ./deploy.sh \n...\nPLAY RECAP *********************************************************************\nmyproject-cephstorage-0 : ok=294  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-1 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-2 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-controller-0 : ok=380  changed=217  unreachable=0    failed=0    skipped=193  rescued=0    ignored=0   \nmyproject-controller-1 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-controller-2 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-novacompute-0 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0   \nmyproject-novacompute-1 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0    \nundercloud                 : ok=172  changed=56   unreachable=0    failed=0    skipped=44   rescued=0    ignored=2   \n\n2023-03-27 16:11:25.971578 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.971826 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Total Tasks: 2517       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972000 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Elapsed Time: 1:22:22.733271 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972191 |                                 UUID |       Info |       Host |   Task Name |   Run Time\n2023-03-27 16:11:25.972394 | fa163e91-32a1-f7b4-b63f-000000007b9f |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 3 using paunch | 1087.73s\n2023-03-27 16:11:25.972600 | fa163e91-32a1-f7b4-b63f-00000000707e |    SUMMARY | undercloud | tripleo-ceph-run-ansible : run ceph-ansible | 479.52s\n2023-03-27 16:11:25.972762 | fa163e91-32a1-f7b4-b63f-000000006641 |    SUMMARY | myproject-controller-2 | Wait for container-puppet tasks (generate config) to finish | 390.19s\n2023-03-27 16:11:25.972917 | fa163e91-32a1-f7b4-b63f-00000000660c |    SUMMARY | myproject-controller-1 | Wait for container-puppet tasks (generate config) to finish | 390.03s\n2023-03-27 16:11:25.973060 | fa163e91-32a1-f7b4-b63f-00000000667b |    SUMMARY | myproject-controller-0 | Wait for container-puppet tasks (generate config) to finish | 379.89s\n2023-03-27 16:11:25.973195 | fa163e91-32a1-f7b4-b63f-000000007235 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for haproxy | 250.51s\n2023-03-27 16:11:25.973368 | fa163e91-32a1-f7b4-b63f-0000000076d6 |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 2 using paunch | 215.77s\n2023-03-27 16:11:25.973582 | fa163e91-32a1-f7b4-b63f-000000007609 |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 2 using paunch | 195.36s\n2023-03-27 16:11:25.973734 | fa163e91-32a1-f7b4-b63f-00000000763f |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 2 using paunch | 195.29s\n2023-03-27 16:11:25.973875 | fa163e91-32a1-f7b4-b63f-00000000659a |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 154.03s\n2023-03-27 16:11:25.974022 | fa163e91-32a1-f7b4-b63f-000000007803 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for ovn_dbs | 150.17s\n2023-03-27 16:11:25.974162 | fa163e91-32a1-f7b4-b63f-000000006516 |    SUMMARY | myproject-controller-1 | Wait for puppet host configuration to finish | 143.97s\n2023-03-27 16:11:25.974322 | fa163e91-32a1-f7b4-b63f-00000000655d |    SUMMARY | myproject-controller-2 | Wait for puppet host configuration to finish | 143.71s\n2023-03-27 16:11:25.974517 | fa163e91-32a1-f7b4-b63f-000000007268 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for redis | 125.43s\n2023-03-27 16:11:25.974698 | fa163e91-32a1-f7b4-b63f-000000007245 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for mysql | 125.22s\n2023-03-27 16:11:25.974839 | fa163e91-32a1-f7b4-b63f-000000007b1a |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 3 using paunch | 124.18s\n2023-03-27 16:11:25.974974 | fa163e91-32a1-f7b4-b63f-000000007b50 |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 3 using paunch | 123.86s\n2023-03-27 16:11:25.975119 | fa163e91-32a1-f7b4-b63f-000000007258 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for oslo_messaging_rpc | 123.83s\n2023-03-27 16:11:25.975276 | fa163e91-32a1-f7b4-b63f-000000008a1e |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 123.51s\n2023-03-27 16:11:25.975468 | fa163e91-32a1-f7b4-b63f-000000008e17 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for cinder_volume | 122.44s\n2023-03-27 16:11:25.975619 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ End Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nAnsible passed.Overcloud configuration completed.\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    101m32.022s\nuser    0m13.412s\nsys     0m1.491s\n</code></pre> <p>NOTE: In a separate session, you can monitor the deployment with the openstack commands.  </p> <p>(undercloud) [stack@undercloud ~]$ openstack server list (undercloud) [stack@undercloud ~]$ openstack baremetal node list  </p> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#post-deployment-validations","title":"Post Deployment Validations","text":"<ol> <li> <p>Check the health of the cluster and the avialability zones.  Source the \\&lt;projectName&gt;rc file first.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ source myprojectrc\n(myproject) [stack@myproject-undercloud ~]$ openstack service list\n+----------------------------------+-----------+----------------+\n| ID                               | Name      | Type           |\n+----------------------------------+-----------+----------------+\n| 2cf5a8efe59e429f913ed11f0fe29d58 | glance    | image          |\n| 3a3aaac39577402ca2d91ae8ca70f359 | heat      | orchestration  |\n| 5879421c8cda4f2fa1b98a1ff159b10a | placement | placement      |\n| 9ce51c3e27f9448182a52c733a4cda2d | cinderv3  | volumev3       |\n| a3ddde9d1c0b48c19bf9005680716d38 | keystone  | identity       |\n| b22f9ce59c404fdbab27dd9fdd819149 | swift     | object-store   |\n| bed225f252b74b6d96dcb23249d20da0 | heat-cfn  | cloudformation |\n| e3c472eb17f545ebb7278bbcf475f322 | nova      | compute        |\n| e460149135bd4f2dabe03d8a8f3eedc1 | cinderv2  | volumev2       |\n| e7c30296b07f4147bb843723d10a7859 | neutron   | network        |\n+----------------------------------+-----------+----------------+\n</code></pre> <pre><code>(myproject) [stack@myproject-undercloud ~]$ openstack network agent list\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| ID                                   | Agent Type                   | Host                                     | Availability Zone | Alive | State | Binary         |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| 9e202644-48b9-4d40-a484-4ad6420bc752 | OVN Controller agent         | myproject-novacompute-0.localdomain |                   | :-)   | UP    | ovn-controller |\n| a43a0c19-669b-40d0-84d4-48ad9d5f514e | OVN Controller Gateway agent | myproject-controller-2.localdomain  |                   | :-)   | UP    | ovn-controller |\n| f26f3981-1e67-4063-b1ce-848deff49d18 | OVN Controller Gateway agent | myproject-controller-0.localdomain  |                   | :-)   | UP    | ovn-controller |\n| e1538367-7b98-49a0-a9cb-98273b01938a | OVN Controller agent         | myproject-novacompute-1.localdomain |                   | :-)   | UP    | ovn-controller |\n| bf65bc54-80d5-4614-b28c-9b42aaf08bc5 | OVN Controller Gateway agent | myproject-controller-1.localdomain  |                   | :-)   | UP    | ovn-controller |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n</code></pre> </li> <li> <p>Create an image with central and remote/dcn Glance service.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ curl http://10.9.71.7/cirros-0.4.0-x86_64-disk.img -o ~/cirros-0.4.0-x86_64-disk.img  \n(myproject) [stack@myproject-undercloud ~]$ qemu-img convert -f qcow2 -O raw cirros-0.4.0-x86_64-disk.img cirros-0.4.0-x86_64-disk.raw  \n(myproject) [stack@myproject-undercloud ~]$ glance image-create --disk-format raw --container-format bare --name cirros --file cirros-0.4.0-x86_64-disk.raw --visibility public  \n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#ceph-alias","title":"Ceph Alias","text":"<p>There isn't a Ceph client installed (i.e. ceph-common) on the controller nodes.  To access the Ceph cluster, all commands are run in the ceph-mon containers.  To make for less typing, I like to set up an alias for ceph on the controller nodes.</p> <pre><code>[heat-admin@osp-blm-controller-0 ~]$ vi .bash_profile\n# .bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n# User specific environment and startup programs\nhn=$(hostname)\nalias ceph=\"sudo podman exec ceph-mon-$hn ceph\"\n</code></pre> <p>NOTE: Don't forget to update the <code>.bash_profile</code> on controller-[12] nodes.</p>"},{"location":"Openstack/OSP16.2Instructions/#installation-of-ceph-dashboard","title":"Installation of Ceph Dashboard","text":"<p>The Ceph dashboard is disabled by default but can easily be enabled in the overcloud using Director.  Full documentation can be found here.</p> <p>NOTE: If deploying the Ceph Dashboard on the external network or any network other than the provisioning or ctlplane networks, it must be deployed at initial deployment of the OSP overcloud.  This is due to Puppet and HAProxy.  </p> <p>For quick reference, follow these procedures:</p> <ol> <li> <p>Source the stackrc file; reivew the templates/containers-prepare-parameter.yaml file.  This was generated in the overcloud deployment procedures and includes the containers required for Ceph and the dashboard.</p> <pre><code>[stack@blm-ospinstall-undercloud ~]$ source ./stackrc\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ cat templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4NyJ9.\n      ...\n      edIJ9DVJCI8MzWcouwKXIfGiMzjbj1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>The Ceph dashboard network is set by default to the provisioning network.  If you want to access through the ctlplane network, create an environment file and set the CephDashboardNetwork parameter to ctlplane.  Include this file in the deploy.sh script.</p> <pre><code>(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi templates/ceph-dashboard-newtork-override.yaml\nparameter_defaults:\n  CephDashboardNetwork: ctlplane\n:wq\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\n\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack blm-ospinstall \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml \\\n     -e templates/ceph-dashboard-network-override.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     --log-file blm-ospinstall_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>Run the deploy.sh script to install the dashboard stack.  This will deploy grafana, prometheus, alertmanager, and the node-exporter containers on the same nodes as the manager containers.</p> <pre><code>(undercloud) [stack@undercloud ~]$ ./deploy.sh\n...\nAnsible passed.Overcloud configuration completed.\nThe output file /home/stack/overcloud-deploy/myproject/myproject-deployment_status.yaml will be overriden\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    92m45.065s\nuser     0m12.506s\nsys       0m1.494s\n</code></pre> </li> </ol>"},{"location":"Openstack/OSP16.2Instructions/#accessing-the-ceph-dashboard","title":"Accessing the Ceph Dashboard","text":"<p>The dashboard is read only by default.  You can change the permissions, see the full documentation for the procedures keeping in mind that changes made could be overwritten by the Director.</p> <ol> <li> <p>The VIP address and the Ceph admin credentials are contained within the all.yml file on the Undercloud Director.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo grep -e dashboard_admin_password -e dashboard_frontend_vip /var/lib/mistral/myproject/ceph-ansible/group_vars/all.yml\ndashboard_admin_password: ********************\ndashboard_frontend_vip: 10.10.0.137\n(undercloud) [stack@undercloud ~]$ \n</code></pre> <p>NOTE: If the Dashboard was deployed on the external network, the dashboard_frontend_vip will still contain an IP from the ctlplane network.  To get the external network IP address and view the HAProxy configuration, login to a control node and execute the following:</p> <p><pre><code>$ sudo grep -A13 dashboard /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg \nlisten ceph_dashboard\nbind 10.1.0.95:8444 transparent\nmode http\nbalance source\nhttp-check expect rstatus 2[0-9][0-9]\nhttp-request set-header X-Forwarded-Proto https if { ssl_fc }\nhttp-request set-header X-Forwarded-Proto http if !{ ssl_fc }\nhttp-request set-header X-Forwarded-Port %[dst_port]\noption httpchk HEAD /\noption httplog\noption forwardfor\nserver osp-blm-controller-0.storage.localdomain 10.40.0.108:8444 check fall 5 inter 2000 rise 2\nserver osp-blm-controller-1.storage.localdomain 10.40.0.178:8444 check fall 5 inter 2000 rise 2\nserver osp-blm-controller-2.storage.localdomain 10.40.0.160:8444 check fall 5 inter 2000 rise 2\n</code></pre> 2. If the VIP is on a private network, open a sshuttle VPN connection using the undercloud director.  </p> <p>NOTE: In Hextupleo, the external network still requires the use of sshuttle.</p> <pre><code>bmclaren@bmclaren-mac ~ % sshuttle -r stack@blm-ospinstall 10.10.0.0/24\n[local sudo] Password: \nstack@blm-ospinstall's password: \nc : Connected to server.\n s: warning: closed channel 1 got cmd=TCP_DATA len=432\n</code></pre> </li> <li> <p>In your favorite browswer, access the dashboard at the VIP on port 8444.  </p> </li> </ol> <p></p> <p>NOTE: An error indicating you don't have permission to view this page will display, just click the Go to Dashboard icon.  </p>"},{"location":"Openstack/OSP16.2Instructions/#appendix","title":"Appendix","text":""},{"location":"Openstack/OSP16.2Instructions/#osp-and-ceph-alignment","title":"OSP and Ceph Alignment","text":"RH-OSP Director External OPS 10 Ceph 2.x Ceph 2.x or 3.x OPS 13 Ceph 3.x Ceph 3.x or 4.x OPS 16.1 Ceph 4.x Ceph 4.x or 5.x (16.1.8) OPS 16.2 Ceph 4.x Ceph 4.x or 5.x (16.2.1) OPS 17.0 Ceph 5.2+ Ceph 5.2 or TBD OSP 17.1 (when released) Ceph 6 Ceph 5.2 or 6"},{"location":"Openstack/OSP16.2Instructions/#openstack-resource-and-event-list","title":"OpenStack Resource and Event List","text":"<pre><code>watch -n 30 \"openstack stack event list blm-ospinstall --nested-depth 5 | grep -v COMPLETE|  tail -n 10; openstack stack resource list blm-ospinstall -n 5 | grep -v COMPLETE | tail -n 10\"\n\n2023-03-27 18:46:26Z [blm-ospinstall.AllNodesDeploySteps.BootstrapServerId]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:27Z [blm-ospinstall.AllNodesDeploySteps.ExternalPostDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:28Z [blm-ospinstall.AllNodesDeploySteps.ExternalUpdateTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:29Z [blm-ospinstall.AllNodesDeploySteps.ControllerExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:30Z [blm-ospinstall.AllNodesDeploySteps.CephStorageExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:31Z [blm-ospinstall.AllNodesDeploySteps.ComputeExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:32Z [blm-ospinstall.AllNodesDeploySteps.ExternalDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ControllerPostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.CephStoragePostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ComputePostConfig]: CREATE_IN_PROGRESS  state changed\n+--------------------------------------------+-------------+-----------------------------------------\n---------------------------------------------------------------------------------------------------------+-----------------+----------------------+-------------------------------------------------------------------------------------------------\n--------------------------------------------------------+\n| resource_name                              | physical_resource_id                                                                                                                                       | resource_type\n                                                                                                         | resource_status | updated_time         | stack_name\n                                                        |\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#logs","title":"Logs","text":"<p>During the deployment, as the Ansible playbooks are executing, the output is written to the ansible.log under the mistral service.  The logs is located in /var/lib/mistral/stackName</p> <pre><code>tail -f ansible.log\n2023-03-27 15:19:33,748 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.748105 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-1\n2023-03-27 15:19:33,817 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.817014 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,338 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.337763 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-2\n2023-03-27 15:19:34,340 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.340029 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-1\n2023-03-27 15:19:34,401 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.401495 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:34,423 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.422923 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,550 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.550269 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n2023-03-27 15:19:34,865 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.865084 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-2\n2023-03-27 15:19:34,928 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.928633 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:35,050 p=89502 u=mistral n=ansible | 2023-03-27 15:19:35.049924 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n...\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#ceph-network-interface-bonds","title":"Ceph Network Interface Bonds","text":"<pre><code>  OsNetConfigImpl:\n    type: OS::Heat::SoftwareConfig\n    properties:\n      group: script\n      config:\n        str_replace:\n          template:\n            get_file: ../../scripts/run-os-net-config.sh\n          params:\n            $network_config:\n              network_config:\n              - type: interface\n                name: nic1  &lt;-- Change to the provisioning network NIC (i.e. eno01)\n                mtu:\n                  get_param: ControlPlaneMtu\n                use_dhcp: false\n                addresses:\n                - ip_netmask:\n                    list_join:\n                    - /\n                    - - get_param: ControlPlaneIp\n                      - get_param: ControlPlaneSubnetCidr\n                routes:\n                  list_concat_unique:\n                    - get_param: ControlPlaneStaticRoutes\n                    - - default: true\n                        next_hop:\n                          get_param: ControlPlaneDefaultRoute\n\n              - type: ovs_bridge\n                name: br_bond0\n                - type: linux_bond\n                  name: bond0  &lt;-- StorageMgmt Network\n                  mtu:\n                    get_attr: [MinViableMtuBondApi, value]\n                  use_dhcp: false\n                  bonding_options:\n                    get_param: BondInterfaceOvsOptions\n                  dns_servers:\n                    get_param: DnsServers\n                  domain:\n                    get_param: DnsSearchDomains\n                  members:\n                    - type: interface\n                      name: nic2   &lt;-- Change this to the port for storage network (i.e. eno49)\n                      mtu:\n                        get_attr: [MinViableMtuBondApi, value]\n                      primary: true\n                    - type: interface\n                      name: nic3   &lt;-- Change this to the port for storage network (i.e. ens1f1)\n                      mtu:\n                        get_attr: [MinViableMtuBondApi, value]\n                - type: vlan\n                  device: bond0\n                  mtu:\n                    get_param: StorageMgmtMtu\n                  vlan_id:\n                    get_param: StorageMgmtNetworkVlanID\n                  addresses:\n                  - ip_netmask:\n                      get_param: StorageMgmtIpSubnet\n                  routes:\n                    list_concat_unique:\n                     - get_param: StorageMgmtInterfaceRoutes\n              - type: ovs_bridge\n                name: br_bond1\n                - type: linux_bond\n                  name: bond1    &lt;--Storage Network\n                  mtu:\n                    get_attr: [MinViableMtuBondApi, value]\n                  use_dhcp: false\n                  bonding_options:  \n                    get_param: BondInterfaceOvsOptions\n                  dns_servers:\n                    get_param: DnsServers\n                  domain:\n                    get_param: DnsSearchDomains\n                  members:\n                    - type: interface\n                      name: nic4    &lt;-- Change this to the port for storage network (i.e. eno50)\n                      mtu:\n                        get_attr: [MinViableMtuBondData, value]\n                      primary: true\n                    - type: interface\n                      name: nic5    &lt;-- Change this to the port for storage network (i.e. ens1f0)\n                      mtu:\n                        get_attr: [MinViableMtuBondData, value]\n                - type: vlan\n                  device: bond1\n                  mtu:\n                    get_param: StorageMgmtMtu\n                  vlan_id:\n                    get_param: StorageMgmtNetworkVlanID\n                  addresses:\n                  - ip_netmask:\n                      get_param: StorageMgmtIpSubnet\n                  routes:\n                    list_concat_unique:\n                     - get_param: StorageMgmtInterfaceRoutes\noutputs:\n  OS::stack_id:\n    description: The OsNetConfigImpl resource.\n    value:\n      get_resource: OsNetConfigImpl\n</code></pre> <pre><code>```\n#This file is an example of an environment file for defining the isolated\n#networks and related parameters.\nresource_registry:\n  # Network Interface templates to use (these files must exist)\n  OS::TripleO::Compute::Net::SoftwareConfig:\n    ./nic-config/compute.yaml\n  # if you are configuring HCI (Hyperconverged nodes) comment 2 lines above and uncomment 2 lines below\n  #OS::TripleO::Compute::Net::SoftwareConfig:\n  #  ./nic-config/compute-hci.yaml\n  OS::TripleO::Controller::Net::SoftwareConfig:\n    ./nic-config/controller.yaml\n  OS::TripleO::CephStorage::Net::SoftwareConfig:\n    ./nic-config/ceph-storage.yaml   &lt;-- Update the reference the proper location of this file.\n\n...\n  DnsServers: [\"172.20.129.10\",\"8.8.8.8\"]\n  # List of Neutron network types for tenant networks (will be used in order)\n  NeutronNetworkType: 'geneve,vlan'\n  # The tunnel type for the tenant network (vxlan or gre). Set to '' to disable tunneling.\n  NeutronTunnelTypes: 'geneve'\n  # Neutron VLAN ranges per network, for example 'datacentre:1:499,tenant:500:1000':\n  NeutronNetworkVLANRanges: 'datacentre:1:1000'\n  #NeutronBridgeMappings: 'datacentre:br-bond0, tenant;br-tenant  &lt;--Example update \n  NeutronBridgeMappings: 'datacentre:br-ex,provider:br-provider'\n  NeutronFlatNetworks: 'datacentre,provider'\n  # Customize bonding options, e.g. \"mode=4 lacp_rate=1 updelay=1000 miimon=100\"\n  # for Linux bonds w/LACP, or \"bond_mode=active-backup\" for OVS active/backup.\n  # BondInterfaceOvsOptions: \"mode=802.3ad lacp_rate=fast updelay=1000 miimon=100 xmit_hash_policy=layer3+4   &lt;--Example update \n  BondInterfaceOvsOptions: \"bond_mode=active-backup\"\n  TimeZone: 'US/Eastern'\n  NtpServer: 10.10.0.10    &lt;-- Update with NTP servers is they are available outside the network, otherwise make sure the Chrony update is made.\n  # TimeZone: \"America/Chicago\"  &lt;-- Add the timezone\n</code></pre>"},{"location":"Openstack/OSP16.2Instructions/#ceph-specific-info","title":"Ceph Specific Info","text":"<pre><code>tail -f /var/lib/mistral/blm-ospinstall/ceph-ansible/ceph_ansible_command.log\n...\n2023-03-27 15:18:02,889 p=683304 u=root n=ansible | INSTALLER STATUS ***************************************************************\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Monitor           : Complete (0:01:10)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Manager           : Complete (0:01:03)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph OSD               : Complete (0:02:05)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph RGW               : Complete (0:00:33)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Client            : Complete (0:00:27)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Install Ceph Crash             : Complete (0:00:41)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Monday 27 March 2023  15:18:02 -0400 (0:00:00.044)       0:07:56.045 ********** \n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | =============================================================================== \n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-container-common : pulling blm-ospinstall-undercloud.ctlplane.hextupleo.lab:8787/rhceph/rhceph-4-rhel8:latest image -- 27.71s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mon : waiting for the monitor(s) to form the quorum... ------------ 17.18s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mgr : create ceph mgr keyring(s) on a mon node -------------------- 12.77s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-handler : restart the ceph-crash service -------------------------- 12.47s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-osd : wait for all osd to be up ----------------------------------- 12.34s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mon : fetch ceph initial keys ------------------------------------- 12.06s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : generate keys ------------------------------------------------ 8.93s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : use ceph-volume lvm batch to create bluestore osds ----------- 8.36s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mgr : wait for all mgr to be up ------------------------------------ 7.46s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : assign application to pool(s) -------------------------------- 7.25s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : create openstack pool(s) ------------------------------------- 6.75s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | gather and delegate facts ----------------------------------------------- 5.94s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool crush_rule ------------------------------------ 5.88s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool min_size -------------------------------------- 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool size ------------------------------------------ 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : set pg_autoscale_mode value on pool(s) ----------------------- 5.69s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-crash : create client.crash keyring -------------------------------- 4.56s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.27s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.24s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.01s\n</code></pre> <p>openstack stack resource list  openstack stack event list  --nested-depth 5 <p>openstack stack list openstack stack event list  <p>openstack stack failures list  <p>openstack baremetal node set --property capabilities</p> <p>Manually tagging nodes for the profile: openstack baremetal node set --property capabilities='profile:control,boot_option:local' 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0</p> <p>openstack baremetal node set --property capabilities='profile:compute,boot_option:local' 484587b2-b3b3-40d5-925b-a26a2fa3036f</p> <p>openstack baremetal node set --property capabilities='profile:ceph-storage,boot_option:local' d930e613-3e14-44b9-8240-4f3559801ea6</p>"}]}