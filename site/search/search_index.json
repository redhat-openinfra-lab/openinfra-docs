{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Red Hat NA-SSA Lab's Documentation Site.","text":"<p>This site is created using MKDocs.  For full documentation visit mkdocs.org.</p>"},{"location":"#pre-requisites","title":"Pre-Requisites","text":"<p>Local client must have python and git.  After installing python, use pip to install the mkdocs and mkdocs-material packages.  It is not required, but using VS Code makes updating the documents easy and integrates with GitHub nicely.</p>"},{"location":"#update-the-site-or-add-documentation","title":"Update the Site or Add Documentation","text":"<p>Clone the site to your local repository.  Once you have a copy of the repository, update the documentation locally.  Preview your changes using mkdocs serve.  This will initialize a local webserver as 127.0.0.1:8000.</p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages.\n    /images   # images that are used in docs pages\n</code></pre>"},{"location":"#helpful-commands","title":"Helpful commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"CephInstallation/","title":"Ceph v5.x Installation Instructions","text":""},{"location":"CephInstallation/#introduction","title":"Introduction","text":""},{"location":"CephInstallation/#goals","title":"Goals:","text":"<ul> <li>Enable field on Ceph v5.0</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to Ceph Product management and engineering teams at IBM</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with Ceph</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"CephInstallation/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Ceph 32G 8 <ul><li>1x pxe</li><li>1x ceph-frontend</li><li>1x ceph-backend</li> <ul><li>64GB</li><li>100GB</li>"},{"location":"CephInstallation/#building-your-kni-lab","title":"Building Your KNI Lab","text":""},{"location":"CephInstallation/#default-configuration","title":"Default Configuration","text":"<pre><code>&gt; NOTE: I need to make the template to build a default environment including the project, networks, and instances.  Use the Customized Configuration for now.\n</code></pre> <ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.</p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy Ceph Environment</p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.</p> <p></p> </li> <li> <p>Wait the deployment to finish which can take ~10-15 minutes.</p> </li> </ol>"},{"location":"CephInstallation/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  Update the project_name and project_password along with the networks to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output. </p> <pre><code>external_network: vlan1117\nnetworks:  \n  - { name: \"ceph-frontend\", cidr: \"10.20.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n\u00a0\u00a0- { name: \"ceph-backend\", cidr: \"10.20.1.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 ceph nodes.  Update the project_name and project_password along with the instances to just include the ones below.  Click Next, verify the configuration and click Launch.  Monitor the progress of the job executing in the output.  </p> <pre><code>instances:  \n  - { name: \"ceph1\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n\u00a0\u00a0- { name: \"ceph2\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n  - { name: \"ceph3\", image: \"rhel8.7\", flavor: \"ceph\", ipmi: \"False\", extra_volume_size: \"100\", net_name1: \"ceph-frontend\", net_name2: \"ceph-backend\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\",net_name8: \"\"\u00a0}  \n</code></pre> </li> </ol>"},{"location":"CephInstallation/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"CephInstallation/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0</p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  Take note of the IP addresses for the ceph-frontend network.  You will use the 172.20.17.X addresses to access the servers.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0   </p> </li> <li> <p>Go to Routers, select the existing router (projectName_router); click the Interfaces tab and then click the Add Interface icon on the right. \u00a0 Add the ceph-frontend interface in the Subnet dropdown.\u00a0 Click Submit.  </p> <p></p> </li> <li> <p>To access your instance, ssh as the cloud-user.  Make sure you are connected to the NA-SSA VPN.</p> </li> </ol>"},{"location":"CephInstallation/#ceph-v5-installation","title":"Ceph v5. Installation","text":"<p>The full Red Hat documentation for the Ceph installation is available here.  The below precedures are for the OpenInfra Lab environment and have been scaled down to only include the required steps.  </p>"},{"location":"CephInstallation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Red Hat Enterprise Linux 8.4 EUS or later.  </li> <li>Ansible 2.9 or later.  </li> <li>Valid Red Hat subsription with the appropriate entitlements.  </li> <li>Root-level access to all nodes.  </li> <li>An active Red Hat Network or service account to access the Red Hat Registry.  </li> </ul>"},{"location":"Certificates/","title":"Certificates","text":""},{"location":"Certificates/#ca-key-and-certificate-files","title":"CA Key and Certificate Files","text":"<p>The CA .key and .pem files are in the /etc/httpd/conf/ssl.key directory on the DNS Utility server in the same directory.</p> <pre><code># openssl genrsa -des3 -out openinfraCA.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.......................................................................................................................................+++++\n...........................................................................................+++++\ne is 65537 (0x010001)\nEnter pass phrase for openinfraCA.key: *******\nVerifying - Enter pass phrase for openinfraCA.key: *******\n\n# openssl req -x509 -new -nodes -key openinfraCA.key -sha256 -days 1095 -out openinfraCA.pem\nEnter pass phrase for openinfraCA.key:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:openinfra.lab\nEmail Address []:userName@redhat.com\n</code></pre> <p>Copy the new CA .pem file to the anchors directory; run update-ca-trust to add it to the list of trusted CA certificates:</p> <pre><code># cp openinfraCA.pem /usr/share/pki/ca-trust-source/anchors/\n# update-ca-trust\n</code></pre> <p>Use the trust list command to verify the new CA is included in the list of trusted CAs:</p> <pre><code># trust list | grep -B2 -A2 openinfra.lab \npkcs11:id=%91%54%10%D3%0D%E4%AD%A7%08%E7%18%EF%A8%62%F7%BF%59%D6%4D%6E;type=cert\n    type: certificate\n    label: openinfra.lab\n    trust: anchor\n    category: authority\n</code></pre>"},{"location":"Certificates/#creating-a-certificate-for-a-serverservice","title":"Creating a Certificate for a Server/Service","text":"<p>There\u2019s two ways to do this, manual and scripted.  Both are provided here.</p>"},{"location":"Certificates/#scripted-process","title":"Scripted Process","text":"<p>Login to the Lab DNS server (172.20.129.10). Switch to the root user.   Add an entry in /etc/hosts for the Server/Service you are generating the SSL certificate for. Change directory to /root/ssl-certifcates and run the script.  </p> <pre><code>cd /root/ssl-certificates  \n./create-certificate.sh &lt;FQDN&gt;  \n</code></pre> <p>Example: ./create-certificate jira-sm.openinfra.lab  </p> <p>You will be prompted for the pass phrase to the openinfraCA.key. </p> <p>NOTE: The pass phrase is in the Cloud Infra Lab spreadsheet.</p> <p>All output files will start with the FQDN. Example:  </p> <p>-rw-r--r--. 1 root root 1517 Mar 22 17:16 jira-sm.openinfra.lab.crt -rw-r--r--. 1 root root 1054 Mar 22 17:16 jira-sm.openinfra.lab.csr -rw-r--r--. 1 root root  254 Mar 22 17:16 jira-sm.openinfra.lab.ext -rw-------. 1 root root 1675 Mar 22 17:16 jira-sm.openinfra.lab.key  </p>"},{"location":"Certificates/#manual-process","title":"Manual Process","text":"<p>Generate the key file:</p> <pre><code># openssl genrsa -out cephrgw.key 2048\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.........+++++\n..............................................................+++++\ne is 65537 (0x010001)\n</code></pre> <p>Generate a Certificate Signing Request file:</p> <pre><code># openssl req -new -key cephrgw.key -out cephrgw.csr\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:North Carolina\nLocality Name (eg, city) [Default City]:Raleigh\nOrganization Name (eg, company) [Default Company Ltd]:Red Hat\nOrganizational Unit Name (eg, section) []:OpenInfrastructure Lab\nCommon Name (eg, your name or your server's hostname) []:cephrgw \nEmail Address []:youremail@redhat.com\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:Redhat1!\nAn optional company name []:      \n</code></pre> <p>Create an x509 V3 extension config file to define the Subject Alternate Names (SAN)</p> <pre><code># cat cephrgw.ext\nauthorityKeyIdentifier = keyid,issuer\nbasicConstraints = CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = cephrgw.cephlab.openinfra.lab\nDNS.2 = cephrgw\nDNS.3 = *.cephrgw\nDNS.4 = *.cephrgw.cephlab.openinfra.lab\n</code></pre> <p>Create the signed certificate using the .csr and .ext file along with the openinfraCA.pem and .key files:</p> <pre><code># openssl x509 -req -in cephrgw.csr -CA openinfraCA.pem -CAkey openinfraCA.key -CAcreateserial -out cephrgw.crt -days 365 -sha256 -extfile cephrgw.ext\nSignature ok\nsubject=C = US, ST = North Carolina, L = Raleigh, O = Red Hat, OU = OpenInfrastructure Lab, CN = cephrgw, emailAddress = bmclaren@redhat.com\nGetting CA Private Key\nEnter pass phrase for openinfraCA.key: Redhat1!\n</code></pre> <p>NOTE: The option \u2013CAcreateserial generates the openinfraCA.srl file.  In subsequent calls to create a signed certificate, use the -CAserial parameter with this file (-CAserial openinfraCA.srl).  The file is used to keep track of the unique serial numbers.</p>"},{"location":"OCPBaremetalPI/","title":"OCP Baremetal IPI Installation","text":""},{"location":"OCPBaremetalPI/#introduction","title":"Introduction","text":"<p>Goals:</p> <ul> <li>Enable field on OpenShift Baremetal 4.12    </li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features    </li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs\u00a0    </li> <li>Send valuable feedback to Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OpenShift Baremetal</li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"OCPBaremetalPI/#lab-access","title":"Lab Access","text":""},{"location":"OCPBaremetalPI/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 17.</p> <p>We have limited resources available, but there should be enough room for about 10 virtual environments.  </p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snap-shotting, adding more cinder volumes to OCS nodes or even adding more networks via either OpenStack CLI, Horizon, or Ansible Tower.</p> Role vRAM vCPU vNIC Disk Bootstrap 20G 6 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 100GB Master 16GB 4 <ul><li>1x pxe</li><li>1x baremetal</li><li>1x virtual IPMI</li> 25 GB Worker 24GB 12 <ul><li>1x pxe</li><li>1x baremetal</li> 50GB 100GB OSD Custom (optional)"},{"location":"OCPBaremetalPI/#building-your-kni-lab","title":"Building Your KNI Lab:","text":""},{"location":"OCPBaremetalPI/#default-configuration","title":"Default Configuration","text":"<ol> <li> <p>To build your environment, please ensure you are connected to the NA-SSA VPN first.</p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Deploy OpenShift Baremetal Environment</p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.</p> <p></p> </li> <li> <p>The jobs can be monitored under Jobs in the left pane.\u00a0 Additional jobs will be initiated to create the project, network, instances, and bare metal bootstrap.\u00a0 Wait the deployment to finish which can take ~10-15 minutes.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via link - NA-SSA Ansible Automation Platform.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - Hextupleo - create\u00a0 project.  </p> </li> <li> <p>Update the Project Name and Password fields. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create networks\u201d.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>networks:  \n  - { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"1500\" }  \n\u00a0\u00a0- { name: \"baremetal0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"1500\" }  \n\u00a0\u00a0- { name: \"virtualipmi\", cidr: \"10.30.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"1500\" }  \n</code></pre> </li> <li> <p>Set user/project and password using the same project and password used previously when creating the project.  Submit the job.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to - \u201cHextupleo - create instances\u201d.  Below is a good starting config with 3 controllers 2 computes and 3 ceph nodes.  </p> <p>With OCS (xlarge workers):  </p> <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker.xlarge\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n</code></pre> <p>Without ODF (normal workers): <pre><code>instances:\n\u00a0\u00a0- { name: \"bootstrap\", image: \"rhel-8.2\", flavor: \"kni.bootstrap\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"vlan1117\", net_name2: \"provisioning0\", net_name3: \"baremetal0\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker3\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker2\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-worker1\", image: \"pxeboot\", flavor: \"kni.worker\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master3\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master2\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n\u00a0\u00a0- { name: \"kni-master1\", image: \"pxeboot\", flavor: \"kni.master\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0\", net_name2: \"baremetal0\", net_name3: \"\", net_name4: \"\", net_name5: \"\", net_name6: \"\",\u00a0 net_name7: \"\", net_name8: \"\"\u00a0 }\n</code></pre></p> </li> <li> <p>Set user/project and password and submit the job using the same project and password used previously when creating the project.  Submit the job. </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to - HextupleO - set up KNI</p> </li> <li> <p>Follow the survey and submit the job</p> </li> <li> <p>At the end you will be getting a screen similar to this one:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user kni using the password you set in the playbook.</p>"},{"location":"OCPBaremetalPI/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":""},{"location":"OCPBaremetalPI/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link:\u00a0</p> <p>HextupleO Lab </p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.</p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address. You will use that IP to access your undercloud node. This will match the above output from the Ansible job (even though it does not in this document).\u00a0 You can SSH to this IP as kni using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected on the networks.  </p> <p>INFO: We have created Tenant (overlay) networks to satisfy all the non-routable networks.\u00a0   </p> </li> <li> <p>Go to Routers, select the existing router (projectName_router); click the Interfaces tab and then click the Add Interface icon on the right. \u00a0 Add the baremetal0 interface in the Subnet dropdown.\u00a0 Click Submit.  </p> <p></p> </li> </ol>"},{"location":"OCPBaremetalPI/#bootstrap","title":"Bootstrap","text":"<ol> <li> <p>Access the bootstrap server via ssh using the IP address obtained in step 3 of the Accessing Your Project\u2019s OpenStack Environment section and the password specified in Tower when deploying the KNI environment.</p> </li> <li> <p>You can now start deploying Openshift Baremetal (KNI) based on the standard instructions below or feel free to deploy using any other documented process.  </p> <p>INFO: Repos Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the rhel8.repo file from here:</p> <p>[kni@bootstrap ~]$ sudo curl http://172.20.129.10/hextupleo-repo/rhel8.repo -o /etc/yum.repos.d/rhel8.repo </p> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-vanilla-openshift-baremetal","title":"Deploying Vanilla Openshift Baremetal","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, we highly encourage everyone to get at least one vanilla deployment done and get familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation found here.\u00a0 Please also review the official documentation for accuracy and open any Bugzilla\u2019s against it.</p>"},{"location":"OCPBaremetalPI/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<ol> <li> <p>Log into Bootstrap VM.  </p> <pre><code>ssh kni@&lt;bootstrapInstance&gt;  / password specified in Tower\n</code></pre> </li> <li> <p>Update all packages on the system.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf update -y\n...\nComplete!\n[kni@bootstrap ~]$ sudo reboot\n</code></pre> </li> <li> <p>Install the KNI Packages.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf install -y libvirt qemu-kvm mkisofs python3-devel jq ipmitool\n</code></pre> </li> <li> <p>Modify the user to add the libvirt group to the newly created kni user.  </p> <pre><code>[kni@bootstrap ~]$ sudo usermod --append --groups libvirt kni\n</code></pre> </li> <li> <p>Start and enable libvirtd; verify the daemon started successfully.</p> <pre><code>[kni@bootstrap ~]$ sudo systemctl enable \u2013now libvirtd\n[kni@bootstrap ~]$ systemctl status libvirtd\n\u25cf libvirtd.service - Virtualization daemon\n   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; vendor preset: enabled)\n   Active: active (running) since Thu 2023-02-16 10:07:21 EST; 1s ago\n     Docs: man:libvirtd(8)\n           https://libvirt.org\n Main PID: 7506 (libvirtd)\n    Tasks: 21 (limit: 32768)\n   Memory: 18.1M\n   CGroup: /system.slice/libvirtd.service\n       \u251c\u25007407 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u251c\u25007408 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper\n       \u2514\u25007506 /usr/sbin/libvirtd --timeout 120\n</code></pre> </li> <li> <p>Create the default storage pool and start it.</p> <pre><code>[kni@bootstrap ~]$ sudo virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images\nPool default defined\n\n[kni@bootstrap ~]$ sudo virsh pool-start default \nPool default started\n\n[kni@bootstrap ~]$ sudo virsh pool-autostart default\nPool default marked as autostarted\n</code></pre> </li> <li> <p>Set up networking using the reconfig-net.sh script in the ~/GoodieBag directory.  Once the connections are reconfigured, the script will display the final configuration.  The output should look similar to what is displayed below.  The script will also add the baremetal subnet to the external router if it was not completed in the Accessing Your Project\u2019s OpenStack Environment section in the Horizon GUI.  </p> <pre><code>[kni@bootstrap ~]$ cd GoodieBag\n[kni@bootstrap GoodieBag]$ ./reconfig-net.sh -h\nScript to reconfigure the eth1 and eth2 interfaces on bootstrap server in OpenStack for Openshift deployments.\nUsage: reconfig-net.sh [-a|--all|interfaceName]\nIf -a or --all is passed, eth1 and eth2 will both be reconfigured as provisioning and baremetal bridge interfaces.\nIf -r or --router is passed, interface reconfiguration is skipped and router configuration will complete.\n\n[kni@bootstrap ~]$ sudo /tmp/reconfig-net.sh -a\nConnection 'System eth1' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)\nConnection 'System eth2' (9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04) successfully deleted.\nConnection 'provisioning' (1b5a7497-4ff5-43ec-bb3e-df01e8f070e8) successfully added.\nConnection 'bridge-slave-eth1' (3a46f2d4-5a3e-4b8d-994b-0a055ade7a84) successfully added.\nConnection 'provisioning' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5) \nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/7) \nprovisioning configured successfully.\n\nConnection 'System eth2' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)\nConnection 'System eth2' (3a73717e-65ab-93e8-b518-24f5af32dc0d) successfully deleted.\nConnection 'baremetal' (9ba0964b-3667-42f5-ae81-a0a0936985cf) successfully added.\nConnection 'bridge-slave-eth2' (f2b52c9b-087f-4b7c-9186-0ed593897c73) successfully added.\nConnection 'baremetal' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/9)\nConnection successfully activated (master waiting for slaves) (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/11)\nbaremetal configured successfully.\n\nCurrent network interface configuration:\nNAME               UUID                                  TYPE      DEVICE       \nSystem eth0        5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0         \nbaremetal          9ba0964b-3667-42f5-ae81-a0a0936985cf  bridge    baremetal    \nprovisioning       1b5a7497-4ff5-43ec-bb3e-df01e8f070e8  bridge    provisioning \nvirbr0             00fb2639-d510-4c0b-bdad-2a471e7c67c5  bridge    virbr0            \nbridge-slave-eth1  3a46f2d4-5a3e-4b8d-994b-0a055ade7a84  ethernet  eth1         \nbridge-slave-eth2  f2b52c9b-087f-4b7c-9186-0ed593897c73  ethernet  eth2         \n\nInstalling required packages to add the baremetal subnet to the external router.\nVerifying baremetal subnet has been added to external router.\nExternal router configuration correctly.\n[kni@bootstrap GoodieBag]$ \n</code></pre> </li> <li> <p>Create a pull-secret.txt file.  In a web browser, navigate to Install OpenShift on Bare Metal with user-provisioned infrastructure, in the Pull Secret section, click the Copy pull secret link.  </p> <p></p> </li> <li> <p>Create a pull-secret.txt file in the kni user\u2019s home directory by pasting the data just copied.  </p> <pre><code>[kni@bootstrap ~]$ vi pull-secret.txt\n{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3lc3NobGVhc2UtZGaWZ0LXJl9hY2NfNViOTE3NDA1NWIyMWU4ZWQxN2ExYjhmOTU6SDA3UETQxN2Q2NzKSK92JMD3Kkk20kl\n\u2026\nNLXRmeUxFcFVRZnVuRVGl2czNjckTJWdkNpVQkdaVypeHVX1jTVhBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}\n:wq\n[kni@bootstrap ~]$\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#openshift-installation","title":"OpenShift Installation","text":"<p>The installation is based on the latest-4.12 version.  This will need to be updated as new versions are released.</p> <ol> <li> <p>Retrieve the GA OpenShift Installer using the get-ocp-installer.sh script in the kni user\u2019s ~/GoodieBag directory.  This script will download the openshift-client installer, ensure the required packages are installed, create the install-config.yaml file, update the file with the appropriate IP addresses, ssh public keys, and pull-secret, and configure DHCP and DNS.  </p> <pre><code>[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh -h \n/tmp/get-ocp-installer.sh usage:\n-v  Specify version, default is \"latest-4.12\".\n-s  Specify full path of pull-secret.txt file, default is \"~/pull-secret.txt\".\n-d  Specify directory to extract the release image in, default is current directory.\n-h  Display help/usage information.\n\n[kni@bootstrap ~]$ GoodieBag/get-ocp-installer.sh\nDownloading the openshift-client-linux.tar.gz file.\nExtracting the openshift-client installer.\nThe openshift-baremetal-installer installed successfully.\n\nMaking sure required packages are installed.\nLast metadata expiration check: 0:21:44 ago on Mon 13 Feb 2023 03:02:05 PM EST.\nPackage ansible-2.9.27-1.el8ae.noarch is already installed.\nPackage python3-shade-1.32.0-2.20220110211405.47fe056.el8ost.noarch is already installed.\nPackage python3-openstackclient-4.0.2-2.20220427020029.el8ost.noarch is already installed.\nDependencies resolved.\nNothing to do.\nComplete!\ndone.\nGenerating the install-config.yaml file.\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [Generate configs] **************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************\nok: [localhost]\n\nTASK [Learn kni instances in the project] **************************************************************************************\nok: [localhost]\n\nTASK [Show kni  instances] **************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        {\n            \"OS-DCF:diskConfig\": \"MANUAL\",\n            \"OS-EXT-AZ:availability_zone\": \"leaf1\",\n            \"OS-EXT-SRV-ATTR:host\": null,\n            \"OS-EXT-SRV-ATTR:hostname\": null,\n            \"OS-EXT-SRV-ATTR:hypervisor_hostname\": null,\n    \u2026\n\nPLAY RECAP **********************************************************************************************\nLocalhost        : ok=6    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\ndone.\nUpdating the install-config yaml file.\nConfiguring DHCP and DNS.\nIs this your first attempt to install OpenShift [y|n]: y\n[kni@bootstrap ~]$\n</code></pre> </li> <li> <p>Review the contents of the ~/GoodieBag/install-configs.yaml file.  All IP addresses for the server instances should be updated from the default of X.X.X.X, and the pullSecrets and sshKey variables should be updated with the correct information.  </p> <pre><code>[kni@bootstrap ~]$ more GoodieBag/install-config.yaml\n\u2026\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:a7:80:64\"\n        role: master\n        rootDeviceHints:\n          deviceName: \"/dev/vda\"\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://10.30.0.129\"\n          username: \"deployodf\"\n          password: \"Passw0rd\"\n\u2026\n</code></pre> </li> <li> <p>To ensure the installation doesn\u2019t get interrupted if there is a disconnect in your ssh session, run the installation in a tmux window.  The tmux utility was installed by the get-ocp-installer.sh script.  </p> <pre><code>[kni@bootstrap ~]$ tmux\n</code></pre> </li> <li> <p>Deploy OpenShift using the openshift-baremetal-install command.  </p> <pre><code>[kni@bootstrap ~]$ openshift-baremetal-install --dir ~/clusterconfigs --log-level debug create cluster\n</code></pre> </li> <li> <p>You can monitor your deployment in another window.  </p> <pre><code>[kni@bootstrap ~]$ sudo virsh list\n Id   Name                        State\n -------------------------------------------\n 1    kni-test2-ntpgv-bootstrap   running\n\n[kni@bootstrap ~]$ sudo virsh console &lt;bootstrap-vm&gt;\n\n[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc\n(kni-test2) [kni@bootstrap ~]$ openstack server list --insecure\n+--------------------------------------+------------------+---------+----------\n| ID                                   | Name             | Status  | Networks                                                                        | Image          | Flavor |\n+--------------------------------------+------------------+---------+----------\n| d339db1a-4e50-4c69-8063-d7d92bfcc190 | ipmi_kni-master1 | ACTIVE  | virtualipmi=10.30.0.132                                                         | virtualipmi    |        |\n| ccfaf78c-80f5-4b01-b3ee-ab0fd82a07c0 | ipmi_kni-master2 | ACTIVE  | virtualipmi=10.30.0.85                                                          | virtualipmi    |        |\n| 14a636dd-3709-48e9-a6e2-be689d434ac7 | ipmi_kni-master3 | ACTIVE  | virtualipmi=10.30.0.49                                                          | virtualipmi    |        |\n| 77fda0c4-8a88-4970-806e-540e3946dd3a | ipmi_kni-worker1 | ACTIVE  | virtualipmi=10.30.0.45                                                          | virtualipmi    |        | \n| 80c13e57-2869-4473-99e2-c4748daaf84c | ipmi_kni-worker2 | ACTIVE  | virtualipmi=10.30.0.147                                                         | virtualipmi    |        |\n| 32f1de18-8bdb-4bdf-bdc3-57ddb0e522d8 | ipmi_kni-worker3 | ACTIVE  | virtualipmi=10.30.0.210                                                         | virtualipmi    |        |\n| 85369613-44ec-47fa-9a9b-2476ca9278e8 | kni-master1      | ACTIVE  | baremetal0=10.20.0.8; provisioning0=10.10.0.200                                 | pxeboot        |        |\n| 438088a3-0182-4f9f-bcce-485bce5973d3 | kni-master2      | ACTIVE  | baremetal0=10.20.0.119; provisioning0=10.10.0.160                               | pxeboot        |        |\n| 0239711f-e661-47f3-bc71-eea60eec8763 | kni-master3      | ACTIVE  | baremetal0=10.20.0.100; provisioning0=10.10.0.209                               | pxeboot        |        |\n| 26798b59-1d6c-4d93-b51a-26b59c725a33 | kni-worker1      | SHUTOFF | baremetal0=10.20.0.77; provisioning0=10.10.0.91                                 | pxeboot        |        |\n| 3f11240b-5806-40f9-92d4-82a58df4053b | kni-worker2      | SHUTOFF | baremetal0=10.20.0.236; provisioning0=10.10.0.94                                | pxeboot        |        |\n| 75015e47-7210-4bb8-8f45-8da5f3f78829 | kni-worker3      | SHUTOFF | baremetal0=10.20.0.124; provisioning0=10.10.0.109                               | pxeboot        |        || 27c33f7a-ea95-4cd6-aec9-691fbcfe27c1 | bootstrap        | ACTIVE  | baremetal0=10.20.0.122; vlan1117=10.9.65.140; provisioning0=10.10.0.45 | rhel82-update1 |        |\n+--------------------------------------+------------------+---------+----------\n</code></pre> <p>Other commands that will come in handy in the later state of the deployment:</p> <pre><code>[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n[kni@bootstrap ~]$ oc get clusteroperators\n[kni@bootstrap ~]$ oc get nodes\n</code></pre> <p>We hope it works! If it has, then at the end of the deployment you will see something like this:</p> <pre><code>INFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.kni-test2.hexo.lab\nINFO Login to the console with user: \"kubeadmin\", and password: \"mv22Z-Tv2Zb-pTFgq-xAvki\"\nDEBUG Time elapsed per stage:\nDEBUG     Infrastructure: 29m15s\nDEBUG Bootstrap Complete: 9m54s\nDEBUG  Bootstrap Destroy: 13s\nDEBUG  Cluster Operators: 34m31s\nINFO Time elapsed: 1h13m54s\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#accessing-your-environment","title":"Accessing Your Environment","text":"<p>There should be a few ways to access your environment including using sshuttle or a jumphost.</p>"},{"location":"OCPBaremetalPI/#using-sshuttle","title":"Using sshuttle","text":"<p>The OpenShift Console is not available on the VPN network the lab environment is deployed on but rather the private network of your project.  To access the console, you can use the sshuttle utility which allows you to create a VPN connection using an ssh connection to the bootstrap server.  You need root access on the client machine but not on the bootstrap server.  The sshuttle utility requirements are python 2.3 or higher which are already installed on the bootstrap server. </p>"},{"location":"OCPBaremetalPI/#linux-client-installation","title":"Linux Client Installation","text":"<ol> <li> <p>Install using the dnf command from the EPEL repository.  </p> <pre><code>$ dnf install -y sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required to use sshuttle.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.  </p> <pre><code>$ sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n</code></pre> </li> <li> <p>Access your OpenShift Console in your browser using the link:  </p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#macos-client-installation","title":"MacOS Client Installation","text":"<ol> <li> <p>If not installed already, install homebrew.  Open a terminal window and grab the install.sh script from GitHub and run it.  Wait for the command to finish.  If you are prompted to enter a password, enter your Mac user\u2019s login password and press ENTER.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Make the brew command available inside the terminal window.</p> <pre><code>\"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\n. ~/.zprofile\n</code></pre> </li> <li> <p>Install sshuttle.</p> <pre><code>$ brew install sshuttle\n</code></pre> </li> <li> <p>There isn\u2019t any configuration required.  You can create the VPN connection using the sshuttle command.  Connect to your project\u2019s bootstrap server\u2019s IP address.</p> <pre><code>~ % sshuttle -r kni@172.20.XX.XX 10.20.0.0/24 [ -vv ]\n[local sudo] Password: \nkni@172.20.17.124's password: \nc : Connected to server.\n</code></pre> <p>NOTE: As traffic is generated on the VPN connection, warning messages will be displayed in the terminal.</p> <p>Example:</p> <p>s: warning: closed channel 7 got cmd=TCP_DATA len=517  c : warning: closed channel 11 got cmd=TCP_STOP_SENDING len=0   s: warning: closed channel 11 got cmd=TCP_DATA len=517   s: warning: closed channel 11 got cmd=TCP_EOF len=0</p> </li> <li> <p>Access your OpenShift Console in your browser using the link:</p> <p>https://console-openshift-console.apps.projectName.hexo.lab  </p> <p>NOTE: The user name is kubeadmin.  The password can be found on the bootstrap server in the kni users\u2019 ~/clusterconfig/auth directory in the the kubeadmin-password file.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#using-a-jumphost-from-openstack","title":"Using a JumpHost from OpenStack","text":"<ol> <li> <p>Access your OpenStack environment, select Compute-&gt;Instances.  Click the Launch Instance icon on the right.  Enter the Instance Name, Description, AZ, and Count.  Click Next.  </p> <p>NOTE: At this time, the procedures below will yield issues with accessing the jumphost due to a known issue with injecting the ssh keys. Please use the procedures under Using sshuttle above.</p> </li> <li> <p>On the Source screen, make sure Select Boot Source is Image and Create New Volume is No.  In the Available list of images, find fedora-cloud37 and click the up arrow to the right of the entry to move it up to the Allocated section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Flavor screen, find t2.medium in the list of Available flavors, click the up arrow to the right of the entry to move it to Allocated.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Networks screen, add the vlan1117 and baremetal0 entries in the Available list to the Allocated list.  Scroll to the bottom, click Next.</p> </li> <li> <p>Click Next in the lower right on the Network Ports screen.  On the Security Groups screen, click the down arrow next to the default security group in the Allocated section to move it to the Available section.  Scroll to the bottom, click Next.</p> </li> <li> <p>On the Key Pair screen, import your ssh keys using the Import Key Pair icon.  Remove the hextupleo_pub_key key from Allocated by clicking the down arrow to the far right.  Once complete, click the Launch Instance in the lower right as the remaining sections are not required or needing to be updated.</p> </li> <li> <p>Ssh to environment and enable x11 and maybe firefox/chrome if you\u2019d like.  </p> <p>``` [fedora@fedora-jumpbox ~]$ sudo dnf -y groupinstall gnome [fedora@fedora-jumpbox ~]$ sudo dnf -y group install \"Basic Desktop\" GNOME [fedora@fedora-jumpbox ~]$ sudo systemctl set-default graphical.target Removed /etc/systemd/system/default.target. Created symlink /etc/systemd/system/default.target \u2192 /usr/lib/systemd/system/graphical.target. [fedora@fedora-jumpbox ~]$ sudo -i [root@fedora-jumpbox ~]# passwd fedora Changing password for user fedora. New password:  BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word Retype new password:  passwd: all authentication tokens updated successfully. [root@fedora-jumpbox ~]# reboot</p> </li> <li> <p>Adding DNS is optional and not a requirement.</p> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-odf-storage","title":"Deploying ODF (storage)","text":"<p>Official docs -&gt; https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/index </p> <ol> <li> <p>Install local storage operator; click Operators-&gt;OperatorHub.  Search for local.  Select the Local Storage Operator, in the pop up window, click the Install icon.  </p> <p></p> </li> <li> <p>Keep the defaults; scroll to the bottom of the screen and click the Install icon.  </p> <p></p> </li> <li> <p>Install ODF Operator; click Operators-&gt;OperatorHub, search for Data Foundation.  Select the OpenShift Data Foundation Operator, in the pop-up window keep the defaults, scroll to the bottom and click Install.  </p> <p></p> </li> <li> <p>Once the operator has successfully been installed, the GUI will indicate that a change has occurred and to refresh.  Create the StorageSystem; select Storage-&gt;Data Foundation.  Click the Storage System tab; click the Create StorageSystem icon on the right.  </p> <p></p> </li> <li> <p>For the Backing storage type, select Create a new StorageClass using local storage devices.  Click Next.  </p> <p></p> </li> <li> <p>The worker nodes have been preconfigured with 100GB secondary drives.  </p> <pre><code>[kni@bootstrap ~]$ ssh core@kni-worker1\nRed Hat Enterprise Linux CoreOS 412.86.202301191053-0\nPart of OpenShift 4.12, RHCOS is a Kubernetes native operating system\nmanaged by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\nhttps://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html\n\n[core@kni-worker1 ~]$ sudo fdisk -l | grep vdb\nDisk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors\n</code></pre> </li> <li> <p>On the Create local volume set screen, enter a name for the volume set.  In the example below, localvolumes is used.  Verify the Disks on all nodes (3 node) is selected in the Filter disks by section and All is selected for the Disk type.  Click the Next icon.  </p> <p></p> </li> <li> <p>A pop up confirmation window will display providing additional information to consider if this is a stretched cluster.   Click the Yes icon if the settings are correct to create the LocalVolumeSet.  </p> </li> <li> <p>On the Capacity and nodes screen, accept the defaults and click Next.  </p> <p></p> </li> <li> <p>On the Security and network screen, accept the default of SDN.  Click Next.</p> </li> <li> <p>On the Review and create screen, review the configuration and click Next if correct to create the Storage System.</p> </li> <li> <p>It will take several minutes to create the local volumes and configure the storage.  You can monitor the progress on the Storage-&gt;Data Foundation-&gt;StorageSystems screen.  Click the ocs-storage-cluster-storagesystem link to access the Overview dashboard.  As the system configures the storages, messages will be logged in the Activity section.  Once complete, the Status for Storage Cluster and Data Resiliency should be green.  </p> <p></p> </li> <li> <p>To verify local storage has been create, click the BlockPools menu item along the top; click the ocs-storagecluster-cephblockpool link.  In the Inventory widget, click the links to view the available Storage Classes or the Persistent Volume Claims.  </p> <p></p> <p>Storage Class:</p> <p></p> <p>PersistentVolumeClaims:</p> <p></p> </li> <li> <p>The local storage and volume claims can be viewed with the CLI.  </p> <pre><code>[kni@bootstrap ~]$ oc get all -n openshift-local-storage\nNAME                                         READY   STATUS    RESTARTS   AGE\npod/diskmaker-discovery-2xnxv                2/2     Running   0          3m46s\npod/diskmaker-discovery-9vjbf                2/2     Running   0          3m49s\npod/diskmaker-discovery-ff2cc                2/2     Running   0          4m2s\npod/diskmaker-manager-c54j8                  2/2     Running   0          2m10s\npod/diskmaker-manager-prgbh                  2/2     Running   0          2m10s\npod/diskmaker-manager-xtw58                  2/2     Running   0          2m10s\npod/local-storage-operator-9bc77c9cf-rzqjt   1/1     Running   0          32m\n\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/local-storage-discovery-metrics   ClusterIP   172.30.23.42    &lt;none&gt;        8383/TCP   11m\nservice/local-storage-diskmaker-metrics   ClusterIP   172.30.248.10   &lt;none&gt;        8383/TCP   2m10s\n\nNAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/diskmaker-discovery   3         3         3       3            3           &lt;none&gt;          11m\ndaemonset.apps/diskmaker-manager     3         3         3       3            3           &lt;none&gt;          2m10s\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/local-storage-operator   1/1     1            1           32m\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/local-storage-operator-9bc77c9cf   1         1         1       32m\n\n[kni@bootstrap ~]$ oc get pv\nNAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\nlocal-pv-2cc6fb0    100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-3faa9a90   100Gi      RWO            Delete           Available           localvolumes            113s\nlocal-pv-e2c0ad97   100Gi      RWO            Delete           Available           localvolumes            114s     3m45s\n</code></pre> </li> </ol>"},{"location":"OCPBaremetalPI/#deploying-ocpv","title":"Deploying OCPv","text":""},{"location":"OCPBaremetalPI/#deleting-your-project","title":"Deleting Your Project","text":"<ol> <li> <p>To remove your entire project, use the Template in Ansible Automation Platform.</p> <p>Ansible Automation Platform</p> </li> <li> <p>Select Templates in the left pane; click the rocket icon in the Action column to the right of Hextupleo - delete my project.</p> <p></p> </li> <li> <p>Update the project_name: and project_password: fields.  Click the Next icon in the lower left.</p> </li> <li> <p>Review the information on the Preview screen.  When ready to move forward, click the Launch icon in the lower left corner.  The Jobs Output screen for the new job will display ongoing output as the system progresses through the Ansible playbook.  You can monitor to completion or just check back to ensure your Job finishes successfully.</p> <p></p> </li> <li> <p>Verify your job finished successfully in the Jobs listing.</p> <p></p> </li> </ol>"},{"location":"OCPBaremetalPI/#troubleshooting","title":"Troubleshooting","text":"<p>Getting NTP applied for workers and masters.  </p> <pre><code>[kni@bootstrap ~]$ wget \n[kni@bootstrap ~]$ wget \n\n[kni@bootstrap ~]$ export KUBECONFIG=/home/kni/clusterconfigs/auth/kubeconfig\n\n[kni@bootstrap ~]$ oc apply -f 99_workers-chrony-configuration.yaml\n[kni@bootstrap ~]$ oc apply -f 99_masters-chrony-configuration.yaml\n</code></pre> <p>Check status of IPMI.  </p> <pre><code>kni@bootstrap ~]$ ipmitool -I lanplus -H 10.30.0.129 -U deployodf -P Passw0rd chassis power status\n</code></pre>"},{"location":"OCPBaremetalPI/#appendix","title":"Appendix","text":"<p>The get-ocp-installer.sh script is provided as a quick method to complete the prep work before installing OpenShift.  If you want to execute manually, the commands are provided below for your reference.  </p> <p>Set the environment variables and download the openshift-client-linux.tar.gz file.  </p> <pre><code>[kni@bootstrap ~]$ export VERSION=latest-4.12\n[kni@bootstrap ~]$ export RELEASE_IMAGE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/release.txt | grep 'Pull From: quay.io' | awk -F ' ' '{print $3}')\n[kni@bootstrap ~]$ export cmd=openshift-baremetal-install\n[kni@bootstrap ~]$ export pullsecret_file=~/pull-secret.txt\n[kni@bootstrap ~]$ export extract_dir=$(pwd)\n[kni@bootstrap ~]$ curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/openshift-client-linux.tar.gz | tar zxvf - oc\noc\n[kni@bootstrap ~]$ sudo cp oc /usr/local/bin\n[kni@bootstrap ~]$ oc adm release extract --registry-config \"${pullsecret_file}\" --command=$cmd --to \"${extract_dir}\" ${RELEASE_IMAGE}\n[kni@bootstrap ~]$ sudo cp openshift-baremetal-install /usr/local/bin\n</code></pre> <p>Verify the installer file has been downloaded.  </p> <pre><code>[kni@bootstrap ~]$ ls\nGoodieBag  nohup.out  oc  openshift-baremetal-install  pull-secret.txt\n</code></pre> <p>Create or generate install-config.yaml.</p> <p>For your convenience, there is an ansible playbook inside a Goodiebag directory that helps gather mac information for the OCP nodes. It uses openstack APIs to gather the information. Install the required packages first Note: This step would typically not be performed on different hardware.  </p> <pre><code>[kni@bootstrap ~]$ sudo dnf -y install ansible python3-shade python3-openstackclient\n</code></pre> <p>Generate the install-config.yaml  </p> <pre><code>[kni@bootstrap ~]$ ansible-playbook GoodieBag/generate-configs.yml\n</code></pre> <p>The file will look similar to this:  </p> <pre><code>[kni@bootstrap ~]$ cat GoodieBag/install-config.yaml \napiVersion: v1\nbasedomain: hexo4.lab\nmetadata:\n  name: \"kni-test\"\nnetworking:\n  machineCIDR: 10.20.0.0/24\n  networkType: OVNKubernetes\ncompute:\n  - name: worker\n     replicas: 3\ncontrolPlane:\n  name: master\n  replicas: 3\n  platform:\n    baremetal: {}\nplatform:\n  baremetal:\n    apiVIP: &lt;api-ip&gt;\n    ingressVIP: &lt;wildcard-ip&gt;\n    provisioningNetworkCIDR: &lt;CIDR&gt;\n    hosts:\n      - name: \"kni_worker3\"\n        bootMACAddress: \"fa:16:3e:0f:21:4d\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker2\"\n        bootMACAddress: \"fa:16:3e:5c:94:dd\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni_worker1\"\n        bootMACAddress: \"fa:16:3e:86:ce:e8\"\n        role: worker\n        hardwareProfile: unknown\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master3\"\n        bootMACAddress: \"fa:16:3e:2e:7a:86\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master2\"\n        bootMACAddress: \"fa:16:3e:23:03:f5\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\n      - name: \"kni-master1\"\n        bootMACAddress: \"fa:16:3e:43:75:71\"\n        role: master\n        hardwareProfile: default\n        bmc:\n          address: \"ipmi://X.X.X.X\"\n          username: \"kni-test\"\n          password: \"changeme\"\npullSecret: '&lt;pull_secret&gt;'\nsshKey: '&lt;ssh_pub_key&gt;'\n</code></pre> <p>You must update the ipmi IP addresses for each of the server instances, the pullSecret, and sshKey variables.  Get the ipmi addresses using the openstack server command.  First, you need to source the variables in the ~/GoodieBag/rc file.   <pre><code>[kni@bootstrap ~]$ source GoodieBag/&lt;project_name&gt;rc   &lt; - replace with your projectname\n(kni-test) [kni@bootstrap ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"      \" $8 }'\nipmi_kni_worker3      virtualipmi=10.30.0.106\nipmi_kni_worker2      virtualipmi=10.30.0.122\nipmi_kni_worker1      virtualipmi=10.30.0.139\nipmi_kni-master3      virtualipmi=10.30.0.37\nipmi_kni-master2      virtualipmi=10.30.0.150\nipmi_kni-master1      virtualipmi=10.30.0.132\n</code></pre> <p>Update each server\u2019s host section replacing the X.X.X.X with the correct IP address.</p> <pre><code>hosts:\n      - name: \"kni_worker3\"\n      bootMACAddress: \"fa:16:3e:0f:21:4d\"\n      role: worker\n      hardwareProfile: unknown\n      bmc:\n        address: \"ipmi://X.X.X.X\"  \u2190 replace with 10.30.0.106\n        username: \"kni-test\"\n        password: \"changeme\"\n</code></pre> <p>Update the pullSecret variable at the bottom of the file with the pull-secret.txt file contents you created in the Openshift Installation.  </p> <pre><code>(kni-test) [kni@bootstrap ~]$ cat pull-secret.txt \n{\"auths\":{\"cloud.openshift.com\":{\"auth\":.....\n\npullSecret:'{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"b3BlbnNoaWHVXQkdaVy1jTVhZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NMkRRN1RTWQxN2ExYjhmOTU6TQxN2Q2NzViOTESDA3UEFGOFNA4QzJTQVOTDNDUFU5SlJY8UV0pSWl\n\u2026\nnZnVzNfN3NDA1NWIyMWU4Zjckx2ZGJHVVzFJd1FNLXRmeUxFcFVRZnVuRTJWdkNpo3MEw3MlJGl2czNkRzLUdpeBZ2xOQTFScw==\",\"email\":\"user@redhat.com\"}}}'\n</code></pre> <p>Update the sshKey variable with your ssh public key.  If you have not generated it, use the ssh-keygen command.  </p> <pre><code>[kni@bootstrap ~]$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/kni/.ssh/id_rsa): &lt;enter&gt;\nEnter passphrase (empty for no passphrase): &lt;enter&gt; \nEnter same passphrase again: \nYour identification has been saved in /home/kni/.ssh/crapid_rsa.\nYour public key has been saved in /home/kni/.ssh/crapid_rsa.pub.\nThe key fingerprint is:\nSHA256:CVnMl7uiOYLFK1jNkWQZ7M29cbsNhdP13Hfvbsy5Yfg kni@bootstrap\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   ..o o.  .     |\n|    =  oo o   .  |\n|   + +o. . + . o.|\n|    + o.o.= o   *|\n|   + .  S+ =    +|\n|  . =   o +   . .|\n| o o . o . + . *.|\n|. o o +   . . o.*|\n|   . . .       Eo|\n+----[SHA256]-----+\n[kni@bootstrap ~]$ cat .ssh/id_rsa.pub \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3PtY3V32PbEbXpuVaaPV\n\u2026\nfionHKM8gdFbXo8yWqCTdT3HuMs9gjjjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap\n</code></pre> <p>Copy the id_rsa.pub contents and paste as the sshKey.</p> <pre><code>sshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAkkfCV0+7GCSABgQC1VX0CnifVays6EnaWHfeP5Qg30FOcr6m3Pt  \n\u2026\nfjffMI/PAXZG0WXgEBsQlNoGrExgk/5116aZx7ioqqU= kni@bootstrap'\n</code></pre> <p>Openshift Baremetal IPI also requires DHCP and DNS to be configured.  Copy the respective files from the GoodieBag directory to /etc.  Enable and start the dnsmasq daemon.  </p> <pre><code>[kni@bootstrap ~]$ sudo cp GoodieBag/hosts /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/kni.dns /etc/dnsmasq.d/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf.upstream /etc/\n[kni@bootstrap ~]$ sudo cp GoodieBag/resolv.conf /etc/\n[kni@bootstrap ~]$ sudo systemctl enable dnsmasq\nCreated symlink /etc/systemd/system/multi-user.target.wants/dnsmasq.service \u2192 /usr/lib/systemd/system/dnsmasq.service.\n[kni@bootstrap ~]$ sudo systemctl start dnsmasq\n</code></pre> <p>If this is not your first attempt at installing OpenShift, cleanup the remnants of the first attempt. </p> <pre><code>[kni@bootstrap ~]$ for i in $(sudo virsh list | tail -n +3 | grep bootstrap | awk {'print $2'})\ndo\n    sudo virsh destroy $i;\n    sudo virsh undefine $i;\n    sudo virsh vol-delete $i --pool default;\n    sudo virsh vol-delete $i.ign --pool default;\ndone\n[kni@bootstrap ~]$ rm -rf ~/clusterconfigs/auth ~/clusterconfigs/terraform* ~/clusterconfigs/tls ~/clusterconfigs/metadata.json\n</code></pre> <p>Create the clusterconfigs working directory in the kni home directory.</p> <pre><code>[kni@bootstrap ~]$ mkdir ~/clusterconfigs\n</code></pre>"},{"location":"OSP16.2Instructions/","title":"OSP 16.2 Installation Instructions","text":""},{"location":"OSP16.2Instructions/#introduction","title":"Introduction","text":""},{"location":"OSP16.2Instructions/#goals","title":"Goals:","text":"<ul> <li>Enable field on OSP 16.2</li> <li>Test basics features work (no regression)</li> <li>Test new selected set of features</li> <li>Optionally test tech preview features</li> <li>Report product and documentation bugs </li> <li>Send valuable feedback to OSP Product management and engineering teams</li> <li>Convert the work accomplished into lab training guides in order to enable more folks in the field</li> <li>Have fun with OSP and director </li> <li>Get to work with and get to know each other.</li> </ul>"},{"location":"OSP16.2Instructions/#hextupleo","title":"HextupleO","text":"<p>The lab access will be provided in the form of nested virtualization managed by RHOSP 16.1.  We have limited resources available, but there should be enough room for about 20 virtual environments.</p> <p>Standard roles have been pre-defined, but since we are using OpenStack to manage it, we are capable of being flexible, with resizing flavors, snapshotting, adding more cinder volumes to ceph nodes or even adding more networks via either OpenStack CLI Horizon or Cloudforms.</p> Role vRAM vCPU vNIC Disk Undercloud 16G 4 <ul><li>1x pxe</li><li>1x external</li> 100GB Controller 12GB 2 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li><li>1x external</li> 60 GB Compute 4GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li> 60GB Ceph 4GB 2 <ul><li>1x pxe</li><li>1x storage</li><li>1x storagemgmt</li><li>1x storage</li> <ul><li>50GB OSD</li><li>100GB (osd)</li> HCI (optional) 8GB 4 <ul><li>1x pxe</li><li>1x internal api</li><li>1x tenant</li><li>1x storage</li><li>1x storagemgmt</li> <ul><li>60GB OSD</li><li>100GB (osd)</li> Custom (optional)"},{"location":"OSP16.2Instructions/#building-your-hextupleo-lab","title":"Building your HextupleO Lab:","text":"<p>HextupleO is an upstream project built with ansible playbooks talking directly to OpenStack APIs via python-shade libraries. It also nicely integrates with Ansible Automation Platform for ease of deployment and manageability (you can learn more about it here).</p>"},{"location":"OSP16.2Instructions/#default-deployment","title":"Default Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Deploy OpenStack Environment. </p> </li> <li> <p>Provide a unique project_name and password. The project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Wait for the deployment to finish.  You can monitor via the Jobs Output screen or you can monitor each individual job by selecting Jobs from the left pane and selecting the appropriate jobs.  There are four jobs, create project, create networks, create instances, and configure OSP undercloud.</p> </li> </ol>"},{"location":"OSP16.2Instructions/#customized-deployment","title":"Customized Deployment","text":"<ol> <li> <p>To build your environment, please ensure you are connected to NA-SSA VPN.  </p> </li> <li> <p>Access RHAAP via this link: Ansible Automation Platform </p> </li> <li> <p>Go to Templates Tab and hit the \u201crocket\u201d icon next to - \u201cHextupleol - create  project\u201d, after you hit \u201cNext\u201d, you will get a survey prompting for user and password</p> </li> <li> <p>Set user/project and password and submit the job. The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create networks.  In hexo4 you are encouraged to experiment with different settings. However if you\u2019d like to start with the known network configuration the default templates should be fine.  </p> <pre><code>networks:  \n  - { name: \"external0\", cidr: \"10.1.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provisioning0\", cidr: \"10.10.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"internalAPI0\", cidr: \"10.20.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"tenant0\", cidr: \"10.30.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storage0\", cidr: \"10.40.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"storagemgmt0\", cidr: \"10.50.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"provider0\", cidr: \"10.60.0.0/24\", dhcp: \"False\", snat: \"False\", mtu: \"8938\" }  \n  - { name: \"virtualipmi\", cidr: \"10.70.0.0/24\", dhcp: \"True\", snat: \"True\", mtu: \"8938\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - create instances.  Below is a good starting config with controllers, 2 compute, and 3 Ceph nodes.  </p> <pre><code>instances:  \n  - { name: \"undercloud\", image: \"rhel-8.2\", flavor: \"undercloud\", ipmi: \"False\", extra_volume_size: \"0\", net_name1: \"provider1-vlan217, net_name2: \"provisioning0, net_name3: \"external0\", net_name4: \"provider0\", net_name5: \"virtualipmi\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_controller1\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller2\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_controller3\", image: \"pxeboot\", flavor: \"overcloud-controller\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"external0, net_name3: \"internalAPI0\", net_name4: \"tenant0\", net_name5: \"storage0\", net_name6: \"storagemgmt\", net_name7: \"provider0\", net_name8: \"\" }  \n  - { name: \"overcloud_compute1\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_compute2\", image: \"pxeboot\", flavor: \"overcloud-compute\", ipmi: \"True\", extra_volume_size: \"0\", net_name1: \"provisioning0, net_name2: \"interalAPI0, net_name3: \"tenant0\", net_name4: \"storage0\", net_name5: \"provider0\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph1\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph2\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n  - { name: \"overcloud_ceph3\", image: \"pxeboot\", flavor: \"overcloud-ceph\", ipmi: \"True\", extra_volume_size: \"100\", net_name1: \"provisioning0, net_name2: \"storage0, net_name3: \"storagemgmt0\", net_name4: \"\", net_name5: \"\", net_name6: \"\", net_name7: \"\", net_name8: \"\" }  \n</code></pre> </li> <li> <p>Set user/project and password and submit the job.  The user name/project name and password are used to access your environment via CLI and the Horizon GUI.  Choose a password you will remember.  </p> </li> <li> <p>Finally, go to Templates tab and hit the \u201crocket\u201d icon next to Hextupleo - configure OSP undercloud.  </p> </li> <li> <p>Follow the survey and submit the job.</p> </li> <li> <p>At the end you will be getting a screen similar to this:</p> </li> </ol> <p></p> <p>You can ssh to this IP as the user stack using the password you set in the playbook.</p>"},{"location":"OSP16.2Instructions/#accessing-your-project-in-horizon","title":"Accessing Your Project in Horizon","text":"<p>This is accessing the Openstack that your environment is deployed to.</p>"},{"location":"OSP16.2Instructions/#horizon","title":"Horizon","text":"<ol> <li> <p>Using the project name as the username and password that you specified when you launched your job, log into the Horizon Dashboard with this link: </p> <p>Horizon Login</p> <p>NOTE: You must be connected to the NA-SSA VPN</p> </li> <li> <p>Go to the Compute-&gt;Instances tab and make sure all of your requested nodes have been created.  </p> </li> <li> <p>Scroll down to the bottom of the list and note the Undercloud Public IP address that is associated with a provider vlan. You will use that IP to access your undercloud node. This should match the above output from the Ansible job.  You can SSH to this IP as the stack user using the password you provided in the playbook.  </p> <p></p> </li> <li> <p>Go to Network-&gt;Network Topology and get familiar with how the VMs are connected. </p> </li> </ol> <p>NOTE: We have created Tenant (over GENEVE/VXLAN) networks to satisfy all the non-routable networks. Only External Network is connected to one of the provider routable networks and accessible from outside of your deployed environment.</p> <p></p>"},{"location":"OSP16.2Instructions/#undercloud","title":"Undercloud","text":"<ol> <li> <p>Access undercloud via ssh using stack@  / password specified in Tower.   <li> <p>You can now start deploying OSP16.2 based on standard instructions (or whatever version you have staged).  See below notes for helpful information in the deployment process.</p> <p>NOTE:  Repos:  Even though you could register to your CDN and start using your own repos, there are local synced repos that are available over LAN. This should be much quicker to download from. Simply grap the osp repo file from here:</p> <p>[stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p>~/GoodieBag/deploy.sh - pre-configured deploy script that can be used as a template.</p> <p>~/templates - Directory that has multiple templates that have been pre-configured for making the vanilla deployment easier to perform. Undercloud.conf - sample undercloud.conf file with pre-configured known working settings like IP pools, interface settings, and more.  The egrep command is used to remove blank and comment lines.</p> <pre><code>[stack@undercloud ~]$ egrep -v '(^$|^#)' undercloud.conf\n[DEFAULT]  \ncertificate_generation_ca = local  \nclean_nodes = true  \ncontainer_images_file = /home/stack/templates/containers-prepare-parameter.yaml  \ngenerate_services_certificate = true  \nhieradata_override = /home/stack/templates/undercloud_hiera.yaml  \nlocal_interface = eth1  \nlocal_ip = 10.10.0.10/24  \nlocal_mtu = 8946  \nlocal_subnet = ctlplane-subnet  \novercloud_domain_name = hextupleo.lab  \nsubnets - ctlplane-subnet  \nundercloud_admin_host = 10.10.0.11  \nundercloud_debug = false  \nundercloud = hostname = myproject-undercloud.hextupleo.lab  \nundercloud_nameservers = 10.9.71.7  \nundercloud_ntp_servers = 10.9.71.7  \nundercloud_public_host = 10.1.0.11  \n[ctlplane-subnet]  \ncidr = 10.10.0.0/24  \ndhcp_end = 10.10.0.149  \ndhcp_start = 10.10.0.100  \ndns_nameservers = 10.10.0.10  \ngateway = 10.10.0.10  \ninspection_iprange = 10.10.0.200,10.10.0.249  \nmasquerade = true  \nmasquerade_network = 10.10.0.0/16  \n[auth]  \nundercloud_admin_password = changeme  \n[stack@undercloud ~]$  \n</code></pre> </li>"},{"location":"OSP16.2Instructions/#deploy-vanilla-osp-162","title":"Deploy Vanilla OSP 16.2","text":"<p>Even though you don\u2019t have to follow this guide and just jump right into hacking, It is highly encouraged for everyone to get at least one vanilla deployment done to get themselves familiar with the process.</p> <p>Steps below are going to be very similar if not mostly identical to Red Hat official documentation. Please review the official documentation for accuracy and open any Bugzilla\u2019s against it!</p>"},{"location":"OSP16.2Instructions/#undercloud-installation","title":"Undercloud Installation","text":"<ol> <li> <p>Log into the undercloud VM as the stack user using the IP address that was obtained during the VM deployments in Horizon.</p> <p>NOTE: You must be connected to the NA-SSA VPN to access the environment.</p> </li> <li> <p>Complete the RHEL upgrade requirements.</p> <p>Make sure you grabbed the repo configuration from the DNS server. [stack@undercloud ~]$ sudo curl http://172.20.129.11/osp16.2.repo -o /etc/yum.repos.d/osp16.2.repo</p> <p><pre><code>[stack@undercloud ~]$ sudo dnf module reset container-tools\n[stack@undercloud ~]$ sudo dnf module enable -y container-tools:3.0\n[stack@undercloud ~]$ sudo yum update -y\n[stack@undercloud ~]$ sudo reboot \n</code></pre> 3. Install the TripleO Director, Ceph Ansible, tmux (optional) packages, and prepare the container images.</p> <pre><code>[stack@undercloud ~]$ sudo yum install -y python3-tripleoclient ceph-ansible tmux\n[stack@undercloud ~]$ openstack tripleo container image prepare default --local-push-destination --output-env-file ~/templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\nOutput env file exists, moving it to backup.\n</code></pre> </li> <li> <p>Starting with OSP15, Red Hat is moving to the Container Registry that requires an active Red Hat account.  In order to continue the installation, you need to include your credentials into the container-prepare-parameter.yaml file.  Service credentials can be created for the deployment using the Customer Portal Terms-Based-Registry site.</p> <p>Click the New Service Account icon in the upper right corner.  Enter a name for the service account and a description.  Click the Create icon.</p> <p></p> <p>Click the Copy icon on the far right to copy the new token to your local clipboard.</p> <p></p> </li> <li> <p>Update the containers-prepare-parameter.yaml file with the new service account name and token.  Open the file and jump to the bottom.  Add the highlighted lines and paste the user name and token generated from Step 4.  The ContainerImageRegistryCredentials space aligns with the ContainerImagePrepare line at the top of the file.</p> <pre><code>[stack@undercloud ~]$ vi templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-08T09:20:45.446840\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n...\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4N\n      ...\n      j1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>Update the undercloud.conf file if necessary.  This should not be required, but feel free to review and understand the parameters.</p> </li> <li> <p>Install the undercloud.</p> <pre><code>[stack@undercloud ~]$ openstack undercloud install\nundercloud_admin_host or undercloud_public_host is not in the same cidr as local_ip.\n\\\n</code></pre> </li> <li> <p>Once the installation is complete, verify the containers are up and source the stackrc file to set the environment for the undercloud environment.</p> <pre><code>[stack@undercloud ~]$ sudo podman ps\n[stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ \n</code></pre> </li> <li> <p>Obtain the images for the overcloud nodes.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo dnf -y install rhosp-director-images rhosp-director-images-ipa-x86_64\n</code></pre> </li> <li> <p>Extract the tar balls for the overcloud and ironic images.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/images\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/overcloud-full-latest-16.2.tar\n(undercloud) [stack@undercloud ~]$ tar -xvf /usr/share/rhosp-director-images/ironic-python-agent-latest-16.2.tar\n</code></pre> </li> <li> <p>The root password can be changed in the overcloud image using the procedures below.  This is optional and is not required.</p> <pre><code>(undercloud) [stack@undercloud ~]$ export LIBGUESTFS_BACKEND=direct\n(undercloud) [stack@undercloud ~]$ virt-customize -a overcloud-full.qcow2 --root-password password:changeme\n</code></pre> <p>NOTE: the libguestfs-tools package may need to be installed:        sudo dnf install -y libguestfs-tools</p> </li> <li> <p>Upload the overcloud images and verify they are available to the undercloud.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud image upload --image-path /home/stack/images\n(undercloud) [stack@undercloud ~]$ openstack image list\n+--------------------------------------+------------------------+--------+\n| ID                                   | Name                   | Status |\n+--------------------------------------+------------------------+--------+\n| f3e73e81-acca-45d7-a848-7d6de40933ed | overcloud-full         | active |\n| f8162188-4ec9-4312-9519-2e6421bd52a8 | overcloud-full-initrd  | active |\n| da2195d1-d75f-4134-b906-1302ff9943af | overcloud-full-vmlinuz | active |\n+--------------------------------------+------------------------+--------+\n</code></pre> </li> <li> <p>Verify the DNS server has been set for the cltplane subnet.</p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack subnet show ctlplane-subnet\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field             | Value                                                                                                                                                   |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| allocation_pools  | 10.10.0.100-10.10.0.149                                                                                                                                 |\n| cidr              | 10.10.0.0/24                                                                                                                                            |\n| created_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n| description       |                                                                                                                                                         |\n| dns_nameservers   | 10.10.0.10                                                                                                                                              |\n| enable_dhcp       | True                                                                                                                                                    |\n| gateway_ip        | 10.10.0.10                                                                                                                                              |\n| host_routes       |                                                                                                                                                         |\n| id                | 2096d5b9-7516-4146-ae4b-919a73f82a8f                                                                                                                    |\n| ip_version        | 4                                                                                                                                                       |\n| ipv6_address_mode | None                                                                                                                                                    |\n| ipv6_ra_mode      | None                                                                                                                                                    |\n| location          | cloud='', project.domain_id=, project.domain_name='Default', project.id='cd6a92e810154ab882d290a70e8c6afc', project.name='admin', region_name='', zone= |\n| name              | ctlplane-subnet                                                                                                                                         |\n| network_id        | 316ccfd5-f1b3-455c-9856-cbbed5b65ba7                                                                                                                    |\n| prefix_length     | None                                                                                                                                                    |\n| project_id        | cd6a92e810154ab882d290a70e8c6afc                                                                                                                        |\n| revision_number   | 0                                                                                                                                                       |\n| segment_id        | None                                                                                                                                                    |\n| service_types     |                                                                                                                                                         |\n| subnetpool_id     | None                                                                                                                                                    |\n| tags              |                                                                                                                                                         |\n| updated_at        | 2023-03-08T15:07:54Z                                                                                                                                    |\n+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> </li> </ol>"},{"location":"OSP16.2Instructions/#overcloud-installation","title":"Overcloud Installation","text":"<p>The first step in deploying the overcloud is to generate the instackenv.yaml file.  Once this is complete, the file needs to be updated with the IP addresses of the VMs that were deployed by HextupleO.</p> <ol> <li> <p>Generate the instackenv.yaml file using the ansible-playbook.  Once the file is generated, source the projectNamerc file for the overcloud environment.  This will allow you to get the list of servers and their IP addresses.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cd ~/GoodieBag\n(undercloud) [stack@undercloud ~]$ ansible-playbook generate_instackenv.yml\n(undercloud) [stack@undercloud ~]$ source *projectName*rc\n(myproject) [stack@undercloud ~]$ openstack server list --insecure | awk '/ipmi_/ { print $4 \"    \" $8 }'\nipmi_overcloud_ceph3    virtualipmi=10.70.0.240\nipmi_overcloud_ceph2    virtualipmi=10.70.0.236\nipmi_overcloud_ceph1    virtualipmi=10.70.0.24\nipmi_overcloud_compute2    virtualipmi=10.70.0.11\nipmi_overcloud_compute1    virtualipmi=10.70.0.22\nipmi_overcloud_controller3    virtualipmi=10.70.0.70\nipmi_overcloud_controller2    virtualipmi=10.70.0.233\nipmi_overcloud_controller1    virtualipmi=10.70.0.227\n</code></pre> </li> <li> <p>Update the instackenv.yaml file with the IP addresses.  You can do this manually, or you can use this code:</p> <pre><code>openstack server list --insecure | awk '/ipmi_/ {print $4 \"    \" $8}' &gt; /tmp/ipmi_addreses.txt\ncp ~/GoodieBag/instackenv.yaml ~/\nsed -i '/pm_addr/d' ~/instackenv.yaml\nfor NODE in $(grep 'name: ' ~/instackenv.yaml | awk '{print $NF}' | sed 's/\"//g')\ndo\n  IP=$(egrep ${NODE} /tmp/ipmi_addresses.txt | awk -F= '{print $NF}')\n  sed -i \"/name: \\\"${NODE}\\\"/a \\    pm_addr: \\\"${IP}\\\"\" ~/instackenv.yaml\ndone\n</code></pre> <p>NOTE: Make sure you view the ~/instackenv.yaml file to ensure it is correct before continuing with the installation.</p> </li> <li> <p>Register the nodes for the overcloud.  Make sure you source the stackrc file for the undercloud environment.  Once the import is complete, list the baremetal nodes and ensure all information is correct for the deployment.  All nodes will be in a power off or manageable state.</p> <pre><code>(undercloud) [stack@undercloud ~]$ source ~/stackrc\n(undercloud) [stack@undercloud ~]$ openstack overcloud node import ~/instackenv.yaml\n(undercloud) [stack@undercloud ~]$ openstack baremetal node list\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name                  | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | 59118ee3-785a-45ef-bef7-8ce4739e34f6 | power off    | manageable             | False       |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | 55eefb7f-e5ad-4987-89ae-fe9ee2fb10dd | power off    | manageable             | False       |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | de0b39b9-b69e-4316-a2b7-743fa95acb65 | power off    | manageable             | False       |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | 5c61b62a-5324-4c20-9dc0-e3cfa866ffbb | power off    | manageable             | False       |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | d9bc6142-5780-45a2-899e-e5dc9d806ab9 | power off    | manageable             | False       |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | 2bb2ca3e-819a-46ce-aa6e-7a4d900875e3 | power off    | manageable             | False       |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | 5a55912a-9c7c-4e35-bde2-c582bbfa1c28 | power off    | manageable             | False       |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | 7f573f3c-8389-4973-aba7-99f055071632 | power off    | manageable             | False       |\n+--------------------------------------+-----------------------+--------------------------------------+-------------+--------------------+-------------+\n</code></pre> <p>Continue to monitor until the nodes are all in an manageable state.</p> </li> <li> <p>Run the introspect to assign the profiles and configure the nodes successfully.  After all nodes are in an available state, verify the proper profiles were assigned.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ openstack overcloud node introspect --all-manageable --provide\n(undercloud) [stack@undercloud ~]$ openstack overcloud profiles list\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| Node UUID                            | Node Name             | Provision State | Current Profile | Possible Profiles |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n| 3a55f42d-b2d7-47f8-99b1-5e086f780896 | overcloud_ceph3       | active          | ceph-storage    |                   |\n| beb33a36-ace6-497a-9178-154c37de2a71 | overcloud_ceph2       | active          | ceph-storage    |                   |\n| 9410122d-88d6-48fd-acbc-492b33ccdd12 | overcloud_ceph1       | active          | ceph-storage    |                   |\n| 19677fb9-b855-410f-be3e-c433a0cfb5af | overcloud_compute2    | active          | compute         |                   |\n| f0e0ef33-2d4b-47e5-bb06-37b5eb3e4263 | overcloud_compute1    | active          | compute         |                   |\n| c5063619-334b-4336-8985-9ce4865a378e | overcloud_controller3 | active          | control         |                   |\n| ad1f539c-2897-4143-9281-44911fe71843 | overcloud_controller2 | active          | control         |                   |\n| 19367e0f-45ac-4242-b867-423e5a81727e | overcloud_controller1 | active          | control         |                   |\n+--------------------------------------+-----------------------+-----------------+-----------------+-------------------+\n</code></pre> </li> <li> <p>The undercloud public endpoints have been most likely encrypted with self-signed certificates.  Make sure to inject the cert into the deployment of the overcloud.  Copy the inject-trust-anchor-hiera.yaml file to the templates directory, copy the cert from the cm-local-ca.pem file and paste into the new file in the templates directory.</p> <pre><code>(undercloud) [stack@undercloud ~]$ cp /usr/share/openstack-tripleo-heat-templates/environments/ssl/inject-trust-anchor-hiera.yaml ~/templates\n(undercloud) [stack@undercloud ~]$ cat /etc/pki/ca-trust/source/anchors/cm-local-ca.pem\nBag Attributes\n    localKeyID: 39 D8 6C E9 C8 7F 65 01 20 80 25 09 E7 A4 41 EB 5D 5E 4E E9 \n    friendlyName: Local Signing Authority\nsubject=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\nissuer=CN = Local Signing Authority, CN = b902a9d8-63a94f21-baaa93cc-f6428d97\n\n-----BEGIN CERTIFICATE-----\nMIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n...\nLO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\nfaM=\n-----END CERTIFICATE-----\n(undercloud) [stack@undercloud ~]$ vi ~/templates/inject-trust-anchor-hiera.yaml\n# *******************************************************************\n# This file was created automatically by the sample environment\n# generator. Developers should use `tox -e genconfig` to update it.\n# Users are recommended to make changes to a copy of the file instead\n# of the original, if any customizations are needed.\n# *******************************************************************\n# title: Inject SSL Trust Anchor on Overcloud Nodes\n# description: |\n#   When using an SSL certificate signed by a CA that is not in the default\n#   list of CAs, this environment allows adding a custom CA certificate to\n#   the overcloud nodes.\nparameter_defaults:\n  # Map containing the CA certs and information needed for deploying them.\n  # Type: json\n  CAMap:\n    undercloud:\n      content: |\n        -----BEGIN CERTIFICATE-----\n        MIIDjjCCAnagAwIBAgIRALkCqdhjqU8huqqTzPZCjZcwDQYJKoZIhvcNAQELBQAw\n        ...\n        LO+VuPv5BpVomVIT3w0cRbmzSNbUcRaX7QmcgyCxHl6FTWZllNsYYvHD6riNwWu+\n        faM=\n        -----END CERTIFICATE-----\n</code></pre> <p>NOTE: Make sure that you line up the indentation for the certificate data.  It must be indented two spaces under the content tag.  </p> </li> <li> <p>Copy the deploy.sh template from the GoodieBag directory to the stack user's home directory.  Edit the /home/stack/deploy.sh file and add the inject-trust-anchor-hiera.yaml file.  Ensure you have an understanding of each of the yaml files included.  </p> <p>NOTE: If you are going to enable the RGW service in Ceph, make sure to include the ceph-rgw.yaml file in the initial deployment.  </p> <pre><code>(undercloud) [stack@undercloud ~]$ cat deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack myproject \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     -e templates/ceph_dashboard_network_override.yaml \\\n     --log-file myproject_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>In HextupleO 4, we are relying on the undercloud to provide NTP services.  By default, OSP v16 doesn't allow time sync from it's chrony service.  As a workaround, execute the following which opens the port via iptables and then allows a sync via chrony.  Restart the chrony service once complete.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo iptables -I INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT\n(undercloud) [stack@undercloud ~]$ echo \"echo allow 0.0.0.0/0 &gt;&gt; /etc/chrony.conf\" | sudo /bin/bash\n(undercloud) [stack@undercloud ~]$ cat /etc/chrony.conf\n# Do not manually edit this file.\n# Managed by ansible-role-chrony\nserver 172.20.129.10 iburst minpoll 6 maxpoll 10\nbindcmdaddress 127.0.0.1\nbindcmdaddress ::1\nallow 10.10.0.0/24\ndriftfile /var/lib/chrony/drift\nlogdir /var/log/chrony\nrtcsync\nmakestep 1.0 3 \nallow 0.0.0.0/0\n(undercloud) [stack@undercloud ~]$ sudo systemctl restart chronyd\n</code></pre> </li> <li> <p>Execute the deploy.sh script to deploy the overcloud.  This takes a very long time to deploy so make sure you run the script in a tmux session.</p> <pre><code>(undercloud) [stack@undercloud ~]$ tmux  \n(undercloud) [stack@undercloud ~]$ cd   \n(undercloud) [stack@undercloud ~]$ ./deploy.sh \n...\nPLAY RECAP *********************************************************************\nmyproject-cephstorage-0 : ok=294  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-1 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-cephstorage-2 : ok=291  changed=149  unreachable=0    failed=0    skipped=153  rescued=0    ignored=0   \nmyproject-controller-0 : ok=380  changed=217  unreachable=0    failed=0    skipped=193  rescued=0    ignored=0   \nmyproject-controller-1 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-controller-2 : ok=378  changed=211  unreachable=0    failed=0    skipped=195  rescued=0    ignored=0   \nmyproject-novacompute-0 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0   \nmyproject-novacompute-1 : ok=333  changed=177  unreachable=0    failed=0    skipped=179  rescued=0    ignored=0    \nundercloud                 : ok=172  changed=56   unreachable=0    failed=0    skipped=44   rescued=0    ignored=2   \n\n2023-03-27 16:11:25.971578 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.971826 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Total Tasks: 2517       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972000 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Elapsed Time: 1:22:22.733271 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n2023-03-27 16:11:25.972191 |                                 UUID |       Info |       Host |   Task Name |   Run Time\n2023-03-27 16:11:25.972394 | fa163e91-32a1-f7b4-b63f-000000007b9f |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 3 using paunch | 1087.73s\n2023-03-27 16:11:25.972600 | fa163e91-32a1-f7b4-b63f-00000000707e |    SUMMARY | undercloud | tripleo-ceph-run-ansible : run ceph-ansible | 479.52s\n2023-03-27 16:11:25.972762 | fa163e91-32a1-f7b4-b63f-000000006641 |    SUMMARY | myproject-controller-2 | Wait for container-puppet tasks (generate config) to finish | 390.19s\n2023-03-27 16:11:25.972917 | fa163e91-32a1-f7b4-b63f-00000000660c |    SUMMARY | myproject-controller-1 | Wait for container-puppet tasks (generate config) to finish | 390.03s\n2023-03-27 16:11:25.973060 | fa163e91-32a1-f7b4-b63f-00000000667b |    SUMMARY | myproject-controller-0 | Wait for container-puppet tasks (generate config) to finish | 379.89s\n2023-03-27 16:11:25.973195 | fa163e91-32a1-f7b4-b63f-000000007235 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for haproxy | 250.51s\n2023-03-27 16:11:25.973368 | fa163e91-32a1-f7b4-b63f-0000000076d6 |    SUMMARY | myproject-controller-0 | Wait for containers to start for step 2 using paunch | 215.77s\n2023-03-27 16:11:25.973582 | fa163e91-32a1-f7b4-b63f-000000007609 |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 2 using paunch | 195.36s\n2023-03-27 16:11:25.973734 | fa163e91-32a1-f7b4-b63f-00000000763f |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 2 using paunch | 195.29s\n2023-03-27 16:11:25.973875 | fa163e91-32a1-f7b4-b63f-00000000659a |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 154.03s\n2023-03-27 16:11:25.974022 | fa163e91-32a1-f7b4-b63f-000000007803 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for ovn_dbs | 150.17s\n2023-03-27 16:11:25.974162 | fa163e91-32a1-f7b4-b63f-000000006516 |    SUMMARY | myproject-controller-1 | Wait for puppet host configuration to finish | 143.97s\n2023-03-27 16:11:25.974322 | fa163e91-32a1-f7b4-b63f-00000000655d |    SUMMARY | myproject-controller-2 | Wait for puppet host configuration to finish | 143.71s\n2023-03-27 16:11:25.974517 | fa163e91-32a1-f7b4-b63f-000000007268 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for redis | 125.43s\n2023-03-27 16:11:25.974698 | fa163e91-32a1-f7b4-b63f-000000007245 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for mysql | 125.22s\n2023-03-27 16:11:25.974839 | fa163e91-32a1-f7b4-b63f-000000007b1a |    SUMMARY | myproject-controller-1 | Wait for containers to start for step 3 using paunch | 124.18s\n2023-03-27 16:11:25.974974 | fa163e91-32a1-f7b4-b63f-000000007b50 |    SUMMARY | myproject-controller-2 | Wait for containers to start for step 3 using paunch | 123.86s\n2023-03-27 16:11:25.975119 | fa163e91-32a1-f7b4-b63f-000000007258 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for oslo_messaging_rpc | 123.83s\n2023-03-27 16:11:25.975276 | fa163e91-32a1-f7b4-b63f-000000008a1e |    SUMMARY | myproject-controller-0 | Wait for puppet host configuration to finish | 123.51s\n2023-03-27 16:11:25.975468 | fa163e91-32a1-f7b4-b63f-000000008e17 |    SUMMARY | myproject-controller-0 | tripleo_ha_wrapper : Run init bundle puppet on the host for cinder_volume | 122.44s\n2023-03-27 16:11:25.975619 | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ End Summary Information ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nAnsible passed.Overcloud configuration completed.\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    101m32.022s\nuser    0m13.412s\nsys     0m1.491s\n</code></pre> <p>NOTE: In a separate session, you can monitor the deployment with the openstack commands.  </p> <p>(undercloud) [stack@undercloud ~]$ openstack server list (undercloud) [stack@undercloud ~]$ openstack baremetal node list  </p> </li> </ol>"},{"location":"OSP16.2Instructions/#post-deployment-validations","title":"Post Deployment Validations","text":"<ol> <li> <p>Check the health of the cluster and the avialability zones.  Source the \\&lt;projectName&gt;rc file first.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ source myprojectrc\n(myproject) [stack@myproject-undercloud ~]$ openstack service list\n+----------------------------------+-----------+----------------+\n| ID                               | Name      | Type           |\n+----------------------------------+-----------+----------------+\n| 2cf5a8efe59e429f913ed11f0fe29d58 | glance    | image          |\n| 3a3aaac39577402ca2d91ae8ca70f359 | heat      | orchestration  |\n| 5879421c8cda4f2fa1b98a1ff159b10a | placement | placement      |\n| 9ce51c3e27f9448182a52c733a4cda2d | cinderv3  | volumev3       |\n| a3ddde9d1c0b48c19bf9005680716d38 | keystone  | identity       |\n| b22f9ce59c404fdbab27dd9fdd819149 | swift     | object-store   |\n| bed225f252b74b6d96dcb23249d20da0 | heat-cfn  | cloudformation |\n| e3c472eb17f545ebb7278bbcf475f322 | nova      | compute        |\n| e460149135bd4f2dabe03d8a8f3eedc1 | cinderv2  | volumev2       |\n| e7c30296b07f4147bb843723d10a7859 | neutron   | network        |\n+----------------------------------+-----------+----------------+\n</code></pre> <pre><code>(myproject) [stack@myproject-undercloud ~]$ openstack network agent list\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| ID                                   | Agent Type                   | Host                                     | Availability Zone | Alive | State | Binary         |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n| 9e202644-48b9-4d40-a484-4ad6420bc752 | OVN Controller agent         | myproject-novacompute-0.localdomain |                   | :-)   | UP    | ovn-controller |\n| a43a0c19-669b-40d0-84d4-48ad9d5f514e | OVN Controller Gateway agent | myproject-controller-2.localdomain  |                   | :-)   | UP    | ovn-controller |\n| f26f3981-1e67-4063-b1ce-848deff49d18 | OVN Controller Gateway agent | myproject-controller-0.localdomain  |                   | :-)   | UP    | ovn-controller |\n| e1538367-7b98-49a0-a9cb-98273b01938a | OVN Controller agent         | myproject-novacompute-1.localdomain |                   | :-)   | UP    | ovn-controller |\n| bf65bc54-80d5-4614-b28c-9b42aaf08bc5 | OVN Controller Gateway agent | myproject-controller-1.localdomain  |                   | :-)   | UP    | ovn-controller |\n+--------------------------------------+------------------------------+------------------------------------------+-------------------+-------+-------+----------------+\n</code></pre> </li> <li> <p>Create an image with central and remote/dcn Glance service.</p> <pre><code>(myproject) [stack@myproject-undercloud ~]$ curl http://10.9.71.7/cirros-0.4.0-x86_64-disk.img -o ~/cirros-0.4.0-x86_64-disk.img  \n(myproject) [stack@myproject-undercloud ~]$ qemu-img convert -f qcow2 -O raw cirros-0.4.0-x86_64-disk.img cirros-0.4.0-x86_64-disk.raw  \n(myproject) [stack@myproject-undercloud ~]$ glance image-create --disk-format raw --container-format bare --name cirros --file cirros-0.4.0-x86_64-disk.raw --visibility public  \n</code></pre> </li> </ol>"},{"location":"OSP16.2Instructions/#installation-of-ceph-dashboard","title":"Installation of Ceph Dashboard","text":"<p>The Ceph dashboard is disabled by default but can easily be enabled in the overcloud using Director.  Full documentation can be found here.</p> <p>For quick reference, follow these procedures:</p> <ol> <li> <p>Source the stackrc file; reivew the templates/containers-prepare-parameter.yaml file.  This was generated in the overcloud deployment procedures and includes the containers required for Ceph and the dashboard.</p> <pre><code>[stack@blm-ospinstall-undercloud ~]$ source ./stackrc\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ cat templates/containers-prepare-parameter.yaml\n# Generated with the following on 2023-03-22T16:49:09.066277\n#\n#   openstack tripleo container image prepare default --local-push-destination --output-env-file /home/stack/templates/containers-prepare-parameter.yaml\n#\n\nparameter_defaults:\n  ContainerImagePrepare:\n  - push_destination: true\n    set:\n      ceph_alertmanager_image: ose-prometheus-alertmanager\n      ceph_alertmanager_namespace: registry.redhat.io/openshift4\n      ceph_alertmanager_tag: v4.6\n      ceph_grafana_image: rhceph-4-dashboard-rhel8\n      ceph_grafana_namespace: registry.redhat.io/rhceph\n      ceph_grafana_tag: 4\n      ceph_image: rhceph-4-rhel8\n      ceph_namespace: registry.redhat.io/rhceph\n      ceph_node_exporter_image: ose-prometheus-node-exporter\n      ceph_node_exporter_namespace: registry.redhat.io/openshift4\n      ceph_node_exporter_tag: v4.6\n      ceph_prometheus_image: ose-prometheus\n      ceph_prometheus_namespace: registry.redhat.io/openshift4\n      ceph_prometheus_tag: v4.6\n      ceph_tag: latest\n      name_prefix: openstack-\n      name_suffix: ''\n      namespace: registry.redhat.io/rhosp-rhel8\n      neutron_driver: ovn\n      rhel_containers: false\n      tag: '16.2'\n    tag_from_label: '{version}-{release}'\n  ContainerImageRegistryCredentials:\n    registry.redhat.io:\n      11009103|my-ospdeployment: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIxOGFlYTA2ZjVmNjg0MGUzYWZlODQ0YTdkZGIzYTc4NyJ9.\n      ...\n      edIJ9DVJCI8MzWcouwKXIfGiMzjbj1ivaZNDw-u8lwRCVtj7ZnQ\n</code></pre> </li> <li> <p>The Ceph dashboard network is set by default to the provisioning network.  If you want to access through a different network, create an environment file and set the network to use.  Include this file in the deploy.sh script.</p> <pre><code>(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi templates/ceph_dashboard_newtork_override.yaml\nparameter_defaults:\n    CephDashboardNetwork:  ctlplane\n:wq\n(undercloud) [stack@blm-ospinstall-undercloud ~]$ vi deploy.sh\n#!/bin/bash\n#############################\n# This is not fully dynamic file and it might have not been populated with all right information. This is a template. You might still want to verify this is what you want before executing it\n##############################\n\nsource ~/stackrc\ncd ~/\ntime openstack overcloud deploy --templates --stack blm-ospinstall \\\n     -n templates/network_data.yaml \\\n     -e templates/node-info.yaml \\\n     -e templates/storage-environment.yaml \\\n     -e templates/ceph-custom-config.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml \\\n     -e templates/ceph_dashboard_network_override.yaml \\\n     -e templates/network-environment.yaml \\\n     -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\\n     -e templates/inject-trust-anchor-hiera.yaml \\\n     -e templates/host-memory.yaml \\\n     -e templates/containers-prepare-parameter.yaml \\\n     --log-file blm-ospinstall_deployment.log \\\n     --ntp-server 10.10.0.10\n</code></pre> </li> <li> <p>Run the deploy.sh script to install the dashboard stack.  This will deploy grafana, prometheus, alertmanager, and the node-exporter containers on the same nodes as the manager containers.</p> <pre><code>(undercloud) [stack@undercloud ~]$ ./deploy.sh\n...\nAnsible passed.Overcloud configuration completed.\nThe output file /home/stack/overcloud-deploy/myproject/myproject-deployment_status.yaml will be overriden\nOvercloud Endpoint: http://10.1.0.99:5000\nOvercloud Horizon Dashboard URL: http://10.1.0.99:80/dashboard\nOvercloud rc file: /home/stack/myprojectrc\nOvercloud Deployed without error\n\nreal    92m45.065s\nuser     0m12.506s\nsys       0m1.494s\n</code></pre> </li> </ol>"},{"location":"OSP16.2Instructions/#accessing-the-ceph-dashboard","title":"Accessing the Ceph Dashboard","text":"<p>The dashboard is read only by default.  You can change the permissions, see the full documentation for the procedures keeping in mind that changes made could be overwritten by the Director.</p> <ol> <li> <p>The VIP address and the Ceph admin credentials are contained within the all.yml file on the Undercloud Director.</p> <pre><code>(undercloud) [stack@undercloud ~]$ sudo grep -e dashboard_admin_password -e dashboard_frontend_vip /var/lib/mistral/myproject/ceph-ansible/group_vars/all.yml\ndashboard_admin_password: ********************\ndashboard_frontend_vip: 10.10.0.137\n(undercloud) [stack@undercloud ~]$ \n</code></pre> </li> <li> <p>Given that the VIP is on the private network, open a sshuttle VPN connection using the undercloud director.</p> <pre><code>bmclaren@bmclaren-mac ~ % sshuttle -r stack@blm-ospinstall 10.10.0.0/24\n[local sudo] Password: \nstack@blm-ospinstall's password: \nc : Connected to server.\n s: warning: closed channel 1 got cmd=TCP_DATA len=432\n</code></pre> </li> <li> <p>In your favorite browswer, access the dashboard at the VIP on port 8444.  </p> </li> </ol> <p></p> <p>NOTE: An error indicating you don't have permission to view this page will display, just click the Go to Dashboard icon.  </p>"},{"location":"OSP16.2Instructions/#appendix","title":"Appendix","text":""},{"location":"OSP16.2Instructions/#openstack-resource-and-event-list","title":"OpenStack Resource and Event List","text":"<pre><code>watch -n 30 \"openstack stack event list blm-ospinstall --nested-depth 5 | grep -v COMPLETE|  tail -n 10; openstack stack resource list blm-ospinstall -n 5 | grep -v COMPLETE | tail -n 10\"\n\n2023-03-27 18:46:26Z [blm-ospinstall.AllNodesDeploySteps.BootstrapServerId]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:27Z [blm-ospinstall.AllNodesDeploySteps.ExternalPostDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:28Z [blm-ospinstall.AllNodesDeploySteps.ExternalUpdateTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:29Z [blm-ospinstall.AllNodesDeploySteps.ControllerExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:30Z [blm-ospinstall.AllNodesDeploySteps.CephStorageExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:31Z [blm-ospinstall.AllNodesDeploySteps.ComputeExtraConfigPost]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:32Z [blm-ospinstall.AllNodesDeploySteps.ExternalDeployTasks]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ControllerPostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.CephStoragePostConfig]: CREATE_IN_PROGRESS  state changed\n2023-03-27 18:46:33Z [blm-ospinstall.AllNodesDeploySteps.ComputePostConfig]: CREATE_IN_PROGRESS  state changed\n+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------\n---------------------------------------------------------------------------------------------------------+-----------------+----------------------+-------------------------------------------------------------------------------------------------\n--------------------------------------------------------+\n| resource_name                              | physical_resource_id                                                                                                                                       | resource_type\n                                                                                                         | resource_status | updated_time         | stack_name\n                                                        |\n+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------\n---------------------------------------------------------------------------------------------------------+-----------------+----------------------+-------------------------------------------------------------------------------------------------\n--------------------------------------------------------+\n+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------\n---------------------------------------------------------------------------------------------------------+-----------------+----------------------+-------------------------------------------------------------------------------------------------\n--------------------------------------------------------+\n</code></pre>"},{"location":"OSP16.2Instructions/#logs","title":"Logs","text":"<p>During the deployment, as the Ansible playbooks are executing, the output is written to the ansible.log under the mistral service.  The logs is located in /var/lib/mistral/\\&lt;stackName&gt;</p> <pre><code>tail -f ansible.log\n2023-03-27 15:19:33,748 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.748105 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-1\n2023-03-27 15:19:33,817 p=89502 u=mistral n=ansible | 2023-03-27 15:19:33.817014 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,338 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.337763 | fa163e91-32a1-f7b4-b63f-000000007235 |         OK | Run init bundle puppet on the host for haproxy | blm-ospinstall-controller-2\n2023-03-27 15:19:34,340 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.340029 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-1\n2023-03-27 15:19:34,401 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.401495 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:34,423 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.422923 | fa163e91-32a1-f7b4-b63f-000000007236 |       TASK | Run pacemaker restart if the config file for the service changed\n2023-03-27 15:19:34,550 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.550269 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n2023-03-27 15:19:34,865 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.865084 | fa163e91-32a1-f7b4-b63f-000000007236 |    CHANGED | Run pacemaker restart if the config file for the service changed | blm-ospinstall-controller-2\n2023-03-27 15:19:34,928 p=89502 u=mistral n=ansible | 2023-03-27 15:19:34.928633 | fa163e91-32a1-f7b4-b63f-000000007244 |       TASK | Gather variables for each operating system\n2023-03-27 15:19:35,050 p=89502 u=mistral n=ansible | 2023-03-27 15:19:35.049924 | fa163e91-32a1-f7b4-b63f-000000007245 |       TASK | Run init bundle puppet on the host for mysql\n...\n</code></pre>"},{"location":"OSP16.2Instructions/#ceph-specific-info","title":"Ceph Specific Info","text":"<pre><code>tail -f /var/lib/mistral/blm-ospinstall/ceph-ansible\n...\n2023-03-27 15:18:02,889 p=683304 u=root n=ansible | INSTALLER STATUS ***************************************************************\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Monitor           : Complete (0:01:10)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Manager           : Complete (0:01:03)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph OSD               : Complete (0:02:05)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph RGW               : Complete (0:00:33)\n2023-03-27 15:18:02,896 p=683304 u=root n=ansible | Install Ceph Client            : Complete (0:00:27)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Install Ceph Crash             : Complete (0:00:41)\n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | Monday 27 March 2023  15:18:02 -0400 (0:00:00.044)       0:07:56.045 ********** \n2023-03-27 15:18:02,897 p=683304 u=root n=ansible | =============================================================================== \n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-container-common : pulling blm-ospinstall-undercloud.ctlplane.hextupleo.lab:8787/rhceph/rhceph-4-rhel8:latest image -- 27.71s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mon : waiting for the monitor(s) to form the quorum... ------------ 17.18s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-mgr : create ceph mgr keyring(s) on a mon node -------------------- 12.77s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-handler : restart the ceph-crash service -------------------------- 12.47s\n2023-03-27 15:18:02,900 p=683304 u=root n=ansible | ceph-osd : wait for all osd to be up ----------------------------------- 12.34s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mon : fetch ceph initial keys ------------------------------------- 12.06s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : generate keys ------------------------------------------------ 8.93s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : use ceph-volume lvm batch to create bluestore osds ----------- 8.36s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-mgr : wait for all mgr to be up ------------------------------------ 7.46s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : assign application to pool(s) -------------------------------- 7.25s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : create openstack pool(s) ------------------------------------- 6.75s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | gather and delegate facts ----------------------------------------------- 5.94s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool crush_rule ------------------------------------ 5.88s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool min_size -------------------------------------- 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : customize pool size ------------------------------------------ 5.78s\n2023-03-27 15:18:02,901 p=683304 u=root n=ansible | ceph-osd : set pg_autoscale_mode value on pool(s) ----------------------- 5.69s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-crash : create client.crash keyring -------------------------------- 4.56s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.27s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.24s\n2023-03-27 15:18:02,902 p=683304 u=root n=ansible | ceph-config : create ceph initial directories --------------------------- 4.01s\n</code></pre>"}]}